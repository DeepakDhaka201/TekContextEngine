<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="AugmentWebviewStateStore">
    <option name="stateMap">
      <map>
        <entry key="CHAT_STATE" value="{&quot;currentConversationId&quot;:&quot;1c373740-dcd1-424b-a907-4c7659670822&quot;,&quot;conversations&quot;:{&quot;b99bb0eb-f9ac-4cc6-851a-47a973c25524&quot;:{&quot;id&quot;:&quot;b99bb0eb-f9ac-4cc6-851a-47a973c25524&quot;,&quot;createdAtIso&quot;:&quot;2025-07-12T23:14:39.018Z&quot;,&quot;lastInteractedAtIso&quot;:&quot;2025-07-12T23:14:39.018Z&quot;,&quot;chatHistory&quot;:[],&quot;feedbackStates&quot;:{},&quot;toolUseStates&quot;:{},&quot;draftExchange&quot;:{&quot;request_message&quot;:&quot;&quot;,&quot;rich_text_json_repr&quot;:{&quot;type&quot;:&quot;doc&quot;,&quot;content&quot;:[{&quot;type&quot;:&quot;paragraph&quot;}]},&quot;mentioned_items&quot;:[],&quot;status&quot;:&quot;draft&quot;},&quot;requestIds&quot;:[],&quot;isPinned&quot;:false,&quot;isShareable&quot;:false,&quot;extraData&quot;:{&quot;hasDirtyEdits&quot;:false},&quot;personaType&quot;:0},&quot;1c373740-dcd1-424b-a907-4c7659670822&quot;:{&quot;id&quot;:&quot;1c373740-dcd1-424b-a907-4c7659670822&quot;,&quot;createdAtIso&quot;:&quot;2025-07-12T23:14:39.028Z&quot;,&quot;lastInteractedAtIso&quot;:&quot;2025-07-12T23:16:38.599Z&quot;,&quot;chatHistory&quot;:[{&quot;request_id&quot;:&quot;b05348c1-60af-4a12-b30e-1bf18abea5c4&quot;,&quot;uuid&quot;:&quot;d2cf4a2f-8b39-4f36-8dae-e92975919c7f&quot;,&quot;chatItemType&quot;:&quot;agentic-checkpoint-delimiter&quot;,&quot;status&quot;:&quot;success&quot;,&quot;fromTimestamp&quot;:0,&quot;toTimestamp&quot;:1752362079028,&quot;seen_state&quot;:&quot;seen&quot;},{&quot;status&quot;:&quot;success&quot;,&quot;request_id&quot;:&quot;6d784a56-468e-4621-9c88-fe70417ee7eb&quot;,&quot;request_message&quot;:&quot;@Processor(JOB_TYPES.SYNC_CODEBASE)\n  async handleSyncCodebase(job: Job&lt;SyncJobData&gt;): Promise&lt;void&gt; {\n    const { codebaseId, type, metadata } = job.data;\n    const syncJobId = metadata?.syncJobId;\n\n    this.logger.log(`Starting parallel sync orchestration for codebase: ${codebaseId}`);\n\n    // Acquire distributed lock for this codebase\n    const lockResource = `sync-orchestration:${codebaseId}`;\n    const lockResult = await this.distributedLockService.acquireLock(lockResource, {\n      ttl: 3600000, // 1 hour\n      maxRetries: 3,\n    });\n\nhow to fix this error \n\nTS1241: Unable to resolve signature of method decorator when called as an expression.\nThe runtime will invoke the decorator with 3 arguments, but the decorator expects 1\nTS1270: Decorator function return type void | Function is not assignable to type\nvoid | TypedPropertyDescriptor&lt;(job: Job&lt;SyncJobData, any, string&gt;) =&gt; Promise&lt;void&gt;&gt;\nType Function is not assignable to type\nvoid | TypedPropertyDescriptor&lt;(job: Job&lt;SyncJobData, any, string&gt;) =&gt; Promise&lt;void&gt;&gt;\n&quot;,&quot;rich_text_json_repr&quot;:{&quot;type&quot;:&quot;doc&quot;,&quot;content&quot;:[{&quot;type&quot;:&quot;paragraph&quot;,&quot;content&quot;:[{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;@Processor(JOB_TYPES.SYNC_CODEBASE)&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;  async handleSyncCodebase(job: Job&lt;SyncJobData&gt;): Promise&lt;void&gt; {&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;    const { codebaseId, type, metadata } = job.data;&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;    const syncJobId = metadata?.syncJobId;&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;    this.logger.log(`Starting parallel sync orchestration for codebase: ${codebaseId}`);&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;    // Acquire distributed lock for this codebase&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;    const lockResource = `sync-orchestration:${codebaseId}`;&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;    const lockResult = await this.distributedLockService.acquireLock(lockResource, {&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;      ttl: 3600000, // 1 hour&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;      maxRetries: 3,&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;    });&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;how to fix this error &quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;TS1241: Unable to resolve signature of method decorator when called as an expression.&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;The runtime will invoke the decorator with 3 arguments, but the decorator expects 1&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;TS1270: Decorator function return type void | Function is not assignable to type&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;void | TypedPropertyDescriptor&lt;(job: Job&lt;SyncJobData, any, string&gt;) =&gt; Promise&lt;void&gt;&gt;&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;Type Function is not assignable to type&quot;},{&quot;type&quot;:&quot;hardBreak&quot;},{&quot;type&quot;:&quot;text&quot;,&quot;text&quot;:&quot;void | TypedPropertyDescriptor&lt;(job: Job&lt;SyncJobData, any, string&gt;) =&gt; Promise&lt;void&gt;&gt;&quot;},{&quot;type&quot;:&quot;hardBreak&quot;}]}]},&quot;mentioned_items&quot;:[],&quot;structured_output_nodes&quot;:[{&quot;id&quot;:1,&quot;type&quot;:5,&quot;content&quot;:&quot;&quot;,&quot;tool_use&quot;:{&quot;tool_use_id&quot;:&quot;toolu_vrtx_015Fnvf9T7CXENq536TagyuT&quot;,&quot;tool_name&quot;:&quot;view&quot;,&quot;input_json&quot;:&quot;{\&quot;path\&quot;: \&quot;src/modules/sync/sync.processor.ts\&quot;, \&quot;type\&quot;: \&quot;file\&quot;}&quot;,&quot;mcp_server_name&quot;:&quot;&quot;,&quot;mcp_tool_name&quot;:&quot;&quot;}},{&quot;id&quot;:2,&quot;type&quot;:0,&quot;content&quot;:&quot;I need to investigate the TypeScript decorator error you're encountering with the `@Processor` decorator. Let me first examine the current code and understand the decorator implementation.\n&quot;},{&quot;id&quot;:3,&quot;type&quot;:2,&quot;content&quot;:&quot;&quot;},{&quot;id&quot;:4,&quot;type&quot;:3,&quot;content&quot;:&quot;&quot;}],&quot;seen_state&quot;:&quot;seen&quot;,&quot;structured_request_nodes&quot;:[{&quot;id&quot;:0,&quot;type&quot;:0,&quot;text_node&quot;:{&quot;content&quot;:&quot;@Processor(JOB_TYPES.SYNC_CODEBASE)\n  async handleSyncCodebase(job: Job&lt;SyncJobData&gt;): Promise&lt;void&gt; {\n    const { codebaseId, type, metadata } = job.data;\n    const syncJobId = metadata?.syncJobId;\n\n    this.logger.log(`Starting parallel sync orchestration for codebase: ${codebaseId}`);\n\n    // Acquire distributed lock for this codebase\n    const lockResource = `sync-orchestration:${codebaseId}`;\n    const lockResult = await this.distributedLockService.acquireLock(lockResource, {\n      ttl: 3600000, // 1 hour\n      maxRetries: 3,\n    });\n\nhow to fix this error \n\nTS1241: Unable to resolve signature of method decorator when called as an expression.\nThe runtime will invoke the decorator with 3 arguments, but the decorator expects 1\nTS1270: Decorator function return type void | Function is not assignable to type\nvoid | TypedPropertyDescriptor&lt;(job: Job&lt;SyncJobData, any, string&gt;) =&gt; Promise&lt;void&gt;&gt;\nType Function is not assignable to type\nvoid | TypedPropertyDescriptor&lt;(job: Job&lt;SyncJobData, any, string&gt;) =&gt; Promise&lt;void&gt;&gt;\n&quot;}},{&quot;id&quot;:1,&quot;type&quot;:4,&quot;ide_state_node&quot;:{&quot;workspaceFolders&quot;:[{&quot;repositoryRoot&quot;:&quot;/Users/deepak/TekCode/TekAIContextEngine2&quot;,&quot;folderRoot&quot;:&quot;/Users/deepak/TekCode/TekAIContextEngine2&quot;}],&quot;workspaceFoldersUnchanged&quot;:false}}],&quot;timestamp&quot;:&quot;2025-07-12T23:15:15.859Z&quot;,&quot;response_text&quot;:&quot;I need to investigate the TypeScript decorator error you're encountering with the `@Processor` decorator. Let me first examine the current code and understand the decorator implementation.\n&quot;,&quot;workspace_file_chunks&quot;:[]},{&quot;status&quot;:&quot;success&quot;,&quot;request_id&quot;:&quot;11b9d3c7-18ce-4442-8c40-e1f391dd95ed&quot;,&quot;request_message&quot;:&quot;&quot;,&quot;structured_output_nodes&quot;:[{&quot;id&quot;:1,&quot;type&quot;:5,&quot;content&quot;:&quot;&quot;,&quot;tool_use&quot;:{&quot;tool_use_id&quot;:&quot;toolu_vrtx_01BqrGeCruHmJDsRArX6jTLi&quot;,&quot;tool_name&quot;:&quot;codebase-retrieval&quot;,&quot;input_json&quot;:&quot;{\&quot;information_request\&quot;: \&quot;Find the @Processor decorator implementation from @nestjs/bullmq and JOB_TYPES constant definition. Also look for how @Processor decorator should be used with method decorators in NestJS BullMQ.\&quot;}&quot;,&quot;mcp_server_name&quot;:&quot;&quot;,&quot;mcp_tool_name&quot;:&quot;&quot;}},{&quot;id&quot;:2,&quot;type&quot;:0,&quot;content&quot;:&quot;Now let me check the `@Processor` decorator implementation and the `JOB_TYPES` constant to understand the issue:\n&quot;},{&quot;id&quot;:3,&quot;type&quot;:2,&quot;content&quot;:&quot;&quot;},{&quot;id&quot;:4,&quot;type&quot;:3,&quot;content&quot;:&quot;&quot;}],&quot;seen_state&quot;:&quot;seen&quot;,&quot;structured_request_nodes&quot;:[{&quot;id&quot;:1,&quot;type&quot;:1,&quot;tool_result_node&quot;:{&quot;content&quot;:&quot;Here's the result of running `cat -n` on src/modules/sync/sync.processor.ts:\n     1\timport {Processor, WorkerHost} from '@nestjs/bullmq';\n     2\timport { Logger } from '@nestjs/common';\n     3\timport { Job } from 'bullmq';\n     4\timport { ConfigService } from '@nestjs/config';\n     5\timport { PrismaService } from '@/shared/database/prisma.service';\n     6\timport { GitlabService } from '../gitlab/gitlab.service';\n     7\timport { FileScannerService } from '../file/file-scanner.service';\n     8\timport { FileService } from '../file/file.service';\n     9\timport { StorageService } from '@/shared/storage/storage.service';\n    10\timport { WebSocketService } from '../websocket/websocket.service';\n    11\timport { CodegraphService } from '../codegraph/codegraph.service';\n    12\timport { DistributedLockService } from '../../shared/locking/distributed-lock.service';\n    13\timport { EnhancedQueueService } from '../../shared/queues/enhanced-queue.service';\n    14\timport { WorkerPoolService } from '../../shared/workers/worker-pool.service';\n    15\timport { SyncJobType, SyncJobStatus, CodebaseStatus, IndexTaskType } from '@prisma/client';\n    16\timport { SyncJobData } from './sync.service';\n    17\timport { QUEUE_NAMES, JOB_TYPES } from '../../shared/queues/queue.constants';\n    18\timport { ParallelProcessingConfig } from '../../config/parallel-processing.config';\n    19\t\n    20\t@Processor(QUEUE_NAMES.SYNC_QUEUE)\n    21\texport class SyncProcessor extends WorkerHost {\n    22\t  private readonly logger = new Logger(SyncProcessor.name);\n    23\t  private readonly config: ParallelProcessingConfig;\n    24\t\n    25\t  constructor(\n    26\t    private configService: ConfigService,\n    27\t    private prisma: PrismaService,\n    28\t    private gitlabService: GitlabService,\n    29\t    private fileScannerService: FileScannerService,\n    30\t    private fileService: FileService,\n    31\t    private storageService: StorageService,\n    32\t    private webSocketService: WebSocketService,\n    33\t    private codegraphService: CodegraphService,\n    34\t    private distributedLockService: DistributedLockService,\n    35\t    private enhancedQueueService: EnhancedQueueService,\n    36\t    private workerPoolService: WorkerPoolService,\n    37\t  ) {\n    38\t    super();\n    39\t    this.config = this.configService.get&lt;ParallelProcessingConfig&gt;('parallelProcessing');\n    40\t  }\n    41\t\n    42\t\n    43\t  async onActive(job: Job) {\n    44\t    this.logger.debug(`Processing sync job ${job.id} for codebase ${job.data.codebaseId}`);\n    45\t  }\n    46\t\n    47\t\n    48\t  async onCompleted(job: Job) {\n    49\t    this.logger.debug(`Completed sync job ${job.id} for codebase ${job.data.codebaseId}`);\n    50\t  }\n    51\t\n    52\t\n    53\t  async onFailed(job: Job, err: Error) {\n    54\t    this.logger.error(`Failed sync job ${job.id} for codebase ${job.data.codebaseId}:`, err);\n    55\t  }\n    56\t\n    57\t  /**\n    58\t   * Main orchestrator - Coordinates parallel pipelines (codebase-indexer approach)\n    59\t   */\n    60\t  @Processor(JOB_TYPES.SYNC_CODEBASE)\n    61\t  async handleSyncCodebase(job: Job&lt;SyncJobData&gt;): Promise&lt;void&gt; {\n    62\t    const { codebaseId, type, metadata } = job.data;\n    63\t    const syncJobId = metadata?.syncJobId;\n    64\t\n    65\t    this.logger.log(`Starting parallel sync orchestration for codebase: ${codebaseId}`);\n    66\t\n    67\t    // Acquire distributed lock for this codebase\n    68\t    const lockResource = `sync-orchestration:${codebaseId}`;\n    69\t    const lockResult = await this.distributedLockService.acquireLock(lockResource, {\n    70\t      ttl: 3600000, // 1 hour\n    71\t      maxRetries: 3,\n    72\t    });\n    73\t\n    74\t    if (!lockResult.acquired) {\n    75\t      throw new Error(`Failed to acquire sync lock for codebase ${codebaseId}: ${lockResult.error}`);\n    76\t    }\n    77\t\n    78\t    try {\n    79\t      // Update job status to running\n    80\t      await this.updateSyncJob(syncJobId, {\n    81\t        status: SyncJobStatus.RUNNING,\n    82\t        startedAt: new Date(),\n    83\t        progress: 0,\n    84\t      });\n    85\t\n    86\t      // Emit WebSocket update\n    87\t      await this.emitSyncUpdate(syncJobId, codebaseId, {\n    88\t        status: 'RUNNING',\n    89\t        progress: 0,\n    90\t        message: 'Starting parallel sync orchestration',\n    91\t      });\n    92\t\n    93\t      // Execute parallel sync orchestration\n    94\t      await this.orchestrateParallelSync(codebaseId, type, syncJobId, metadata);\n    95\t\n    96\t      // Mark sync as completed\n    97\t      await this.updateSyncJob(syncJobId, {\n    98\t        status: SyncJobStatus.COMPLETED,\n    99\t        completedAt: new Date(),\n   100\t        progress: 100,\n   101\t      });\n   102\t\n   103\t      await this.emitSyncUpdate(syncJobId, codebaseId, {\n   104\t        status: 'COMPLETED',\n   105\t        progress: 100,\n   106\t        message: 'Parallel sync orchestration completed',\n   107\t      });\n   108\t\n   109\t      this.logger.log(`Completed parallel sync orchestration for codebase: ${codebaseId}`);\n   110\t    } finally {\n   111\t      // Release distributed lock\n   112\t      await this.distributedLockService.releaseLock(lockResource, lockResult.lockId!);\n   113\t    }\n   114\t  }\n   115\t\n   116\t  /**\n   117\t   * Orchestrate parallel sync pipelines (following codebase-indexer pattern)\n   118\t   */\n   119\t  private async orchestrateParallelSync(\n   120\t    codebaseId: string,\n   121\t    type: SyncJobType,\n   122\t    syncJobId: string,\n   123\t    metadata: any,\n   124\t  ): Promise&lt;void&gt; {\n   125\t    this.logger.log(`Orchestrating parallel sync for codebase: ${codebaseId}, type: ${type}`);\n   126\t\n   127\t    try {\n   128\t      // Phase 1: Repository Sync &amp; File Discovery\n   129\t      await this.updateProgress(codebaseId, syncJobId, 'repository-sync', 10, 'Starting repository sync and file discovery');\n   130\t      await this.executeRepositorySync(codebaseId, syncJobId, metadata);\n   131\t\n   132\t      // Phase 2: Get discovered files\n   133\t      await this.updateProgress(codebaseId, syncJobId, 'file-discovery', 20, 'Discovering files in repository');\n   134\t      const files = await this.getDiscoveredFiles(codebaseId);\n   135\t\n   136\t      if (files.length === 0) {\n   137\t        this.logger.warn(`No files discovered for codebase ${codebaseId}`);\n   138\t        await this.updateProgress(codebaseId, syncJobId, 'completed', 100, 'No files found to process');\n   139\t        return;\n   140\t      }\n   141\t\n   142\t      // Phase 3: Execute parallel processing pipelines (codebase-indexer approach)\n   143\t      await this.updateProgress(codebaseId, syncJobId, 'parallel-processing', 30, `Processing ${files.length} files in parallel pipelines`);\n   144\t      await this.executeParallelProcessingPipelines(codebaseId, files, syncJobId, type);\n   145\t\n   146\t      // Phase 4: Finalize and update codebase status\n   147\t      await this.updateProgress(codebaseId, syncJobId, 'finalizing', 90, 'Finalizing sync process');\n   148\t      await this.finalizeSyncProcess(codebaseId, syncJobId);\n   149\t\n   150\t      await this.updateProgress(codebaseId, syncJobId, 'completed', 100, 'Sync completed successfully');\n   151\t    } catch (error) {\n   152\t      this.logger.error(`Error in parallel sync orchestration for codebase ${codebaseId}:`, error);\n   153\t      await this.updateProgress(codebaseId, syncJobId, 'failed', 0, `Sync failed: ${error.message}`);\n   154\t      throw error;\n   155\t    }\n   156\t  }\n   157\t\n   158\t  /**\n   159\t   * Execute repository sync and file discovery\n   160\t   */\n   161\t  private async executeRepositorySync(\n   162\t    codebaseId: string,\n   163\t    syncJobId: string,\n   164\t    metadata: any,\n   165\t  ): Promise&lt;void&gt; {\n   166\t    this.logger.log(`Executing repository sync for codebase: ${codebaseId}`);\n   167\t\n   168\t    try {\n   169\t      // Update progress\n   170\t      await this.updateSyncProgress(syncJobId, codebaseId, 10, 'Syncing repository...');\n   171\t\n   172\t      // Get codebase information\n   173\t      const codebase = await this.prisma.codebase.findUnique({\n   174\t        where: { id: codebaseId },\n   175\t      });\n   176\t\n   177\t      if (!codebase) {\n   178\t        throw new Error(`Codebase not found: ${codebaseId}`);\n   179\t      }\n   180\t\n   181\t      // Sync repository files\n   182\t      const syncResult = await this.gitlabService.syncRepository(\n   183\t        codebase.gitlabProjectId,\n   184\t        codebase.branch || 'main',\n   185\t      );\n   186\t\n   187\t      // Update progress\n   188\t      await this.updateSyncProgress(syncJobId, codebaseId, 30, 'Discovering files...');\n   189\t\n   190\t      // Scan and discover files\n   191\t      const discoveredFiles = await this.fileScannerService.scanRepository(\n   192\t        syncResult.repositoryPath,\n   193\t        {\n   194\t          includePatterns: metadata?.includePatterns,\n   195\t          excludePatterns: metadata?.excludePatterns,\n   196\t          maxFileSize: metadata?.maxFileSize,\n   197\t        },\n   198\t      );\n   199\t\n   200\t      // Store discovered files\n   201\t      await this.fileService.storeDiscoveredFiles(codebaseId, discoveredFiles);\n   202\t\n   203\t      // Update progress\n   204\t      await this.updateSyncProgress(syncJobId, codebaseId, 50, `Discovered ${discoveredFiles.length} files`);\n   205\t\n   206\t      this.logger.log(`Repository sync completed for codebase: ${codebaseId}, files: ${discoveredFiles.length}`);\n   207\t    } catch (error) {\n   208\t      this.logger.error(`Repository sync failed for codebase ${codebaseId}:`, error);\n   209\t      throw error;\n   210\t    }\n   211\t  }\n   212\t\n   213\t  /**\n   214\t   * Execute parallel processing pipelines (codebase-indexer approach)\n   215\t   */\n   216\t  private async executeParallelProcessingPipelines(\n   217\t    codebaseId: string,\n   218\t    files: any[],\n   219\t    syncJobId: string,\n   220\t    type: SyncJobType,\n   221\t  ): Promise&lt;void&gt; {\n   222\t    this.logger.log(`Executing parallel processing pipelines for ${files.length} files`);\n   223\t\n   224\t    try {\n   225\t      // Update progress\n   226\t      await this.updateSyncProgress(syncJobId, codebaseId, 60, 'Starting parallel processing...');\n   227\t\n   228\t      // Calculate batch configuration\n   229\t      const batchSize = this.config?.performance?.maxFilesPerBatch || 100;\n   230\t      const totalBatches = Math.ceil(files.length / batchSize);\n   231\t\n   232\t      // Parallel execution (like codebase-indexer's Go routines)\n   233\t      const pipelinePromises: Promise&lt;any&gt;[] = [];\n   234\t\n   235\t      // Pipeline 1: File Processing + Embedding Generation\n   236\t      if (type === SyncJobType.FULL || type === SyncJobType.EMBEDDING) {\n   237\t        pipelinePromises.push(\n   238\t          this.executeFileProcessingPipeline(codebaseId, files, totalBatches, syncJobId)\n   239\t        );\n   240\t      }\n   241\t\n   242\t      // Pipeline 2: Code Analysis (SCIP + Tree-sitter)\n   243\t      if (type === SyncJobType.FULL || type === SyncJobType.ANALYSIS) {\n   244\t        pipelinePromises.push(\n   245\t          this.executeCodeAnalysisPipeline(codebaseId, files, totalBatches, syncJobId)\n   246\t        );\n   247\t      }\n   248\t\n   249\t      // Wait for all pipelines to complete\n   250\t      const results = await Promise.allSettled(pipelinePromises);\n   251\t\n   252\t      // Check for failures\n   253\t      const failures = results.filter(result =&gt; result.status === 'rejected');\n   254\t      if (failures.length &gt; 0) {\n   255\t        this.logger.error(`${failures.length} pipelines failed:`, failures);\n   256\t        // Don't throw - allow partial success\n   257\t      }\n   258\t\n   259\t      // Update progress\n   260\t      await this.updateSyncProgress(syncJobId, codebaseId, 90, 'Parallel processing completed');\n   261\t\n   262\t      this.logger.log(`Parallel processing pipelines completed for codebase: ${codebaseId}`);\n   263\t    } catch (error) {\n   264\t      this.logger.error(`Parallel processing failed for codebase ${codebaseId}:`, error);\n   265\t      throw error;\n   266\t    }\n   267\t  }\n   268\t\n   269\t  /**\n   270\t   * Execute file processing pipeline\n   271\t   */\n   272\t  private async executeFileProcessingPipeline(\n   273\t    codebaseId: string,\n   274\t    files: any[],\n   275\t    totalBatches: number,\n   276\t    syncJobId: string,\n   277\t  ): Promise&lt;void&gt; {\n   278\t    this.logger.log(`Starting file processing pipeline for ${files.length} files`);\n   279\t\n   280\t    const batchSize = this.config?.performance?.maxFilesPerBatch || 100;\n   281\t    const fileBatches = this.createBatches(files, batchSize);\n   282\t\n   283\t    // Submit file processing jobs\n   284\t    const fileProcessingPromises = fileBatches.map((batch, index) =&gt;\n   285\t      this.enhancedQueueService.submitFileProcessingJob({\n   286\t        codebaseId,\n   287\t        files: batch.map(f =&gt; ({\n   288\t          path: f.path,\n   289\t          hash: f.hash || '',\n   290\t          size: f.size || 0,\n   291\t          language: f.language,\n   292\t          storageKey: f.storageKey,\n   293\t        })),\n   294\t        batchIndex: index,\n   295\t        totalBatches,\n   296\t        syncJobId,\n   297\t        options: {\n   298\t          forceReprocess: false,\n   299\t          validateContent: true,\n   300\t          extractMetadata: true,\n   301\t        },\n   302\t      })\n   303\t    );\n   304\t\n   305\t    // Wait for file processing to complete\n   306\t    await Promise.all(fileProcessingPromises);\n   307\t\n   308\t    // Submit embedding generation jobs\n   309\t    const embeddingPromises = fileBatches.map((batch, index) =&gt;\n   310\t      this.enhancedQueueService.submitEmbeddingJob({\n   311\t        codebaseId,\n   312\t        files: batch.map(f =&gt; ({\n   313\t          path: f.path,\n   314\t          language: f.language || 'text',\n   315\t          storageKey: f.storageKey || f.path,\n   316\t          size: f.size || 0,\n   317\t        })),\n   318\t        batchIndex: index,\n   319\t        totalBatches,\n   320\t        syncJobId,\n   321\t        options: {\n   322\t          chunkingStrategy: 'hybrid',\n   323\t          batchSize: this.config?.embeddingConfig?.batchSize || 20,\n   324\t          forceRegenerate: false,\n   325\t        },\n   326\t      })\n   327\t    );\n   328\t\n   329\t    // Wait for embedding generation to complete\n   330\t    await Promise.all(embeddingPromises);\n   331\t\n   332\t    this.logger.log(`File processing pipeline completed for ${files.length} files`);\n   333\t  }\n   334\t\n   335\t  /**\n   336\t   * Execute code analysis pipeline\n   337\t   */\n   338\t  private async executeCodeAnalysisPipeline(\n   339\t    codebaseId: string,\n   340\t    files: any[],\n   341\t    totalBatches: number,\n   342\t    syncJobId: string,\n   343\t  ): Promise&lt;void&gt; {\n   344\t    this.logger.log(`Starting code analysis pipeline for ${files.length} files`);\n   345\t\n   346\t    const batchSize = this.config?.performance?.maxFilesPerBatch || 100;\n   347\t    const fileBatches = this.createBatches(files, batchSize);\n   348\t\n   349\t    const codeAnalysisPromises = fileBatches.map((batch, index) =&gt;\n   350\t      this.enhancedQueueService.submitCodeAnalysisJob({\n   351\t        codebaseId,\n   352\t        files: batch.map(f =&gt; ({\n   353\t          path: f.path,\n   354\t          language: f.language || 'text',\n   355\t          storageKey: f.storageKey || f.path,\n   356\t        })),\n   357\t        analysisType: 'both', // SCIP + Tree-sitter\n   358\t        batchIndex: index,\n   359\t        totalBatches,\n   360\t        syncJobId,\n   361\t        options: {\n   362\t          extractSymbols: true,\n   363\t          buildRelationships: true,\n   364\t          generateIndex: true,\n   365\t        },\n   366\t      })\n   367\t    );\n   368\t\n   369\t    // Wait for code analysis to complete\n   370\t    await Promise.all(codeAnalysisPromises);\n   371\t\n   372\t    this.logger.log(`Code analysis pipeline completed for ${files.length} files`);\n   373\t  }\n   374\t\n   375\t  /**\n   376\t   * Finalize sync process\n   377\t   */\n   378\t  private async finalizeSyncProcess(codebaseId: string, syncJobId: string): Promise&lt;void&gt; {\n   379\t    this.logger.log(`Finalizing sync process for codebase: ${codebaseId}`);\n   380\t\n   381\t    try {\n   382\t      // Update codebase status\n   383\t      await this.prisma.codebase.update({\n   384\t        where: { id: codebaseId },\n   385\t        data: {\n   386\t          status: CodebaseStatus.ACTIVE,\n   387\t          lastSyncAt: new Date(),\n   388\t          updatedAt: new Date(),\n   389\t        },\n   390\t      });\n   391\t\n   392\t      // Update progress\n   393\t      await this.updateSyncProgress(syncJobId, codebaseId, 100, 'Sync completed successfully');\n   394\t\n   395\t      this.logger.log(`Sync process finalized for codebase: ${codebaseId}`);\n   396\t    } catch (error) {\n   397\t      this.logger.error(`Failed to finalize sync process for codebase ${codebaseId}:`, error);\n   398\t      throw error;\n   399\t    }\n   400\t  }\n   401\t\n   402\t  /**\n   403\t   * Get discovered files for codebase\n   404\t   */\n   405\t  private async getDiscoveredFiles(codebaseId: string): Promise&lt;any[]&gt; {\n   406\t    // TODO: Implement based on your actual file schema\n   407\t    // For now, return mock data\n   408\t    return [\n   409\t      {\n   410\t        path: 'src/main.ts',\n   411\t        hash: 'abc123',\n   412\t        size: 1024,\n   413\t        language: 'typescript',\n   414\t        storageKey: `${codebaseId}/src/main.ts`,\n   415\t      },\n   416\t      {\n   417\t        path: 'src/app.service.ts',\n   418\t        hash: 'def456',\n   419\t        size: 2048,\n   420\t        language: 'typescript',\n   421\t        storageKey: `${codebaseId}/src/app.service.ts`,\n   422\t      },\n   423\t    ];\n   424\t  }\n   425\t\n   426\t  /**\n   427\t   * Create batches from array\n   428\t   */\n   429\t  private createBatches&lt;T&gt;(items: T[], batchSize: number): T[][] {\n   430\t    const batches: T[][] = [];\n   431\t    for (let i = 0; i &lt; items.length; i += batchSize) {\n   432\t      batches.push(items.slice(i, i + batchSize));\n   433\t    }\n   434\t    return batches;\n   435\t  }\n   436\t\n   437\t  /**\n   438\t   * Update sync progress\n   439\t   */\n   440\t  private async updateSyncProgress(\n   441\t    syncJobId: string,\n   442\t    codebaseId: string,\n   443\t    progress: number,\n   444\t    message: string,\n   445\t  ): Promise&lt;void&gt; {\n   446\t    try {\n   447\t      await this.updateSyncJob(syncJobId, { progress });\n   448\t      await this.emitSyncUpdate(syncJobId, codebaseId, {\n   449\t        status: 'RUNNING',\n   450\t        progress,\n   451\t        message,\n   452\t      });\n   453\t    } catch (error) {\n   454\t      this.logger.warn(`Failed to update sync progress for ${syncJobId}:`, error);\n   455\t    }\n   456\t  }\n   457\t\n   458\t  /**\n   459\t   * Update progress and emit WebSocket updates\n   460\t   */\n   461\t  private async updateProgress(\n   462\t    codebaseId: string,\n   463\t    syncJobId: string,\n   464\t    stage: string,\n   465\t    progress: number,\n   466\t    message: string,\n   467\t    details?: any,\n   468\t  ): Promise&lt;void&gt; {\n   469\t    try {\n   470\t      // Submit progress update to queue for processing\n   471\t      await this.enhancedQueueService.submitProgressTrackingJob({\n   472\t        codebaseId,\n   473\t        syncJobId,\n   474\t        stage,\n   475\t        progress,\n   476\t        message,\n   477\t        details,\n   478\t      });\n   479\t    } catch (error) {\n   480\t      this.logger.error(`Failed to update progress for codebase ${codebaseId}:`, error);\n   481\t      // Don't throw - progress updates shouldn't break the main flow\n   482\t    }\n   483\t  }\n   484\t\n   485\t  /**\n   486\t   * Update sync job in database\n   487\t   */\n   488\t  private async updateSyncJob(\n   489\t    syncJobId: string,\n   490\t    updates: {\n   491\t      status?: SyncJobStatus;\n   492\t      progress?: number;\n   493\t      message?: string;\n   494\t      error?: string;\n   495\t      startedAt?: Date;\n   496\t      completedAt?: Date;\n   497\t    },\n   498\t  ): Promise&lt;void&gt; {\n   499\t    if (!syncJobId) return;\n   500\t\n   501\t    try {\n   502\t      // TODO: Update based on your actual schema\n   503\t      // For now, just log the update\n   504\t      this.logger.debug(`Sync job ${syncJobId} update:`, updates);\n   505\t\n   506\t      // If you have a syncJob table, uncomment this:\n   507\t      // await this.prisma.syncJob.update({\n   508\t      //   where: { id: syncJobId },\n   509\t      //   data: {\n   510\t      //     ...updates,\n   511\t      //     updatedAt: new Date(),\n   512\t      //   },\n   513\t      // });\n   514\t    } catch (error) {\n   515\t      this.logger.warn(`Failed to update sync job ${syncJobId}:`, error);\n   516\t    }\n   517\t  }\n   518\t\n   519\t  /**\n   520\t   * Emit sync status update via WebSocket\n   521\t   */\n   522\t  private async emitSyncUpdate(\n   523\t    syncJobId: string,\n   524\t    codebaseId: string,\n   525\t    update: {\n   526\t      status: string;\n   527\t      progress: number;\n   528\t      message?: string;\n   529\t      error?: string;\n   530\t    },\n   531\t  ): Promise&lt;void&gt; {\n   532\t    try {\n   533\t      this.webSocketService.emitToRoom(`codebase:${codebaseId}`, 'syncUpdate', {\n   534\t        syncJobId,\n   535\t        codebaseId,\n   536\t        ...update,\n   537\t        timestamp: new Date(),\n   538\t      });\n   539\t    } catch (error) {\n   540\t      this.logger.warn('Failed to emit sync update:', error);\n   541\t    }\n   542\t  }\n   543\t\n   544\t\n   545\t\n   546\t\n   547\t\n   548\t}\n   549\t\nTotal lines in file: 549\n&quot;,&quot;is_error&quot;:false,&quot;request_id&quot;:&quot;&quot;,&quot;tool_use_id&quot;:&quot;toolu_vrtx_015Fnvf9T7CXENq536TagyuT&quot;}},{&quot;id&quot;:2,&quot;type&quot;:4,&quot;ide_state_node&quot;:{&quot;workspaceFolders&quot;:[{&quot;repositoryRoot&quot;:&quot;/Users/deepak/TekCode/TekAIContextEngine2&quot;,&quot;folderRoot&quot;:&quot;/Users/deepak/TekCode/TekAIContextEngine2&quot;}],&quot;workspaceFoldersUnchanged&quot;:false}}],&quot;timestamp&quot;:&quot;2025-07-12T23:15:20.407Z&quot;,&quot;response_text&quot;:&quot;Now let me check the `@Processor` decorator implementation and the `JOB_TYPES` constant to understand the issue:\n&quot;,&quot;workspace_file_chunks&quot;:[]},{&quot;status&quot;:&quot;success&quot;,&quot;request_id&quot;:&quot;6210448c-bc52-4d99-97aa-662266bbbc49&quot;,&quot;request_message&quot;:&quot;&quot;,&quot;structured_output_nodes&quot;:[{&quot;id&quot;:1,&quot;type&quot;:5,&quot;content&quot;:&quot;&quot;,&quot;tool_use&quot;:{&quot;tool_use_id&quot;:&quot;toolu_vrtx_01A77Sqtssm6PMSNA9CURrig&quot;,&quot;tool_name&quot;:&quot;web-search&quot;,&quot;input_json&quot;:&quot;{\&quot;query\&quot;: \&quot;NestJS BullMQ @Processor decorator method decorator usage TypeScript error\&quot;}&quot;,&quot;mcp_server_name&quot;:&quot;&quot;,&quot;mcp_tool_name&quot;:&quot;&quot;}},{&quot;id&quot;:2,&quot;type&quot;:0,&quot;content&quot;:&quot;Now let me check the specific usage of `@Processor` decorator in NestJS BullMQ to understand the correct pattern:\n&quot;},{&quot;id&quot;:3,&quot;type&quot;:2,&quot;content&quot;:&quot;&quot;},{&quot;id&quot;:4,&quot;type&quot;:3,&quot;content&quot;:&quot;&quot;}],&quot;seen_state&quot;:&quot;seen&quot;,&quot;structured_request_nodes&quot;:[{&quot;id&quot;:1,&quot;type&quot;:1,&quot;tool_result_node&quot;:{&quot;content&quot;:&quot;The following code sections were retrieved:\nPath: src/modules/sync/processors/batch.processor.ts\n     1\timport { Processor, WorkerHost, OnQueueActive, OnQueueCompleted, OnQueueFailed } from '@nestjs/bullmq';\n     2\timport { Logger } from '@nestjs/common';\n     3\timport { Job } from 'bullmq';\n     4\timport { ConfigService } from '@nestjs/config';\n     5\timport { PrismaService } from '../../../shared/database/prisma.service';\n     6\timport { BatchOperationsService } from '../../../shared/database/batch-operations.service';\n     7\timport { StorageService } from '../../../shared/storage/storage.service';\n     8\timport { DistributedLockService } from '../../../shared/locking/distributed-lock.service';\n     9\timport { \n    10\t  QUEUE_NAMES, \n    11\t  JOB_TYPES, \n    12\t  BatchProcessingJobData,\n    13\t} from '../../../shared/queues/queue.constants';\n    14\timport { ParallelProcessingConfig } from '../../../config/parallel-processing.config';\n...\n    25\t\n    26\t/**\n    27\t * Batch Processor - Handles batch database and storage operations\n    28\t * Follows codebase-indexer's batch processing pattern for performance\n    29\t */\n    30\t@Processor(QUEUE_NAMES.BATCH_PROCESSING_QUEUE)\n    31\texport class BatchProcessor extends WorkerHost {\n    32\t  private readonly logger = new Logger(BatchProcessor.name);\n    33\t  private readonly config: ParallelProcessingConfig;\n    34\t\n    35\t  constructor(\n    36\t    private configService: ConfigService,\n    37\t    private prisma: PrismaService,\n    38\t    private batchOperations: BatchOperationsService,\n    39\t    private storageService: StorageService,\n    40\t    private distributedLockService: DistributedLockService,\n    41\t  ) {\n    42\t    super();\n    43\t    this.config = this.configService.get&lt;ParallelProcessingConfig&gt;('parallelProcessing');\n    44\t  }\n...\nPath: docs/DEVELOPMENT.md\n...\n   249\t\n   250\t3. **Implement service logic**:\n   251\t   ```typescript\n   252\t   async create(createDto: CreateItemDto) {\n   253\t     return this.prisma.item.create({\n   254\t       data: createDto,\n   255\t     });\n   256\t   }\n   257\t   ```\n   258\t\n   259\t4. **Add tests**:\n   260\t   ```typescript\n   261\t   describe('create', () =&gt; {\n   262\t     it('should create item', async () =&gt; {\n   263\t       // Test implementation\n   264\t     });\n   265\t   });\n   266\t   ```\n   267\t\n   268\t### 4. Background Jobs\n   269\t\n   270\t1. **Define job processor**:\n   271\t   ```typescript\n   272\t   @Process('job-name')\n   273\t   async handleJob(job: Job&lt;JobData&gt;) {\n   274\t     // Job implementation\n   275\t   }\n   276\t   ```\n   277\t\n   278\t2. **Queue job**:\n   279\t   ```typescript\n   280\t   await this.queue.add('job-name', jobData);\n   281\t   ```\n   282\t\n   283\t3. **Test job processing**:\n   284\t   ```typescript\n   285\t   // Test job execution\n   286\t   ```\n...\nPath: src/modules/codegraph/codegraph.processor.ts\n     1\timport { Processor, WorkerHost, OnQueueActive, OnQueueCompleted, OnQueueFailed } from '@nestjs/bullmq';\n     2\timport { Logger } from '@nestjs/common';\n     3\timport { Job } from 'bullmq';\n     4\timport { PrismaService } from '@/shared/database/prisma.service';\n     5\timport { StorageService } from '@/shared/storage/storage.service';\n     6\timport { WebSocketService } from '../websocket/websocket.service';\n     7\timport { DistributedLockService } from '../../shared/locking/distributed-lock.service';\n     8\timport { EnhancedQueueService } from '../../shared/queues/enhanced-queue.service';\n     9\timport { WorkerPoolService } from '../../shared/workers/worker-pool.service';\n    10\timport { ScipService } from './services/scip.service';\n    11\timport { TreeSitterService } from './services/tree-sitter.service';\n    12\timport { EmbeddingService } from './services/embedding.service';\n    13\timport { BadgerDbService } from './services/badger-db.service';\n    14\timport { CodegraphConfigService } from './services/codegraph-config.service';\n    15\timport { IndexTaskType, IndexTaskStatus } from '@prisma/client';\n    16\timport { CodegraphJobData } from './codegraph.service';\n    17\timport { QUEUE_NAMES, JOB_TYPES } from '../../shared/queues/queue.constants';\n    18\timport { ParallelProcessingConfig } from '../../config/parallel-processing.config';\n    19\timport { ConfigService } from '@nestjs/config';\n    20\t\n    21\t@Processor(QUEUE_NAMES.CODEGRAPH_QUEUE)\n    22\texport class CodegraphProcessor extends WorkerHost {\n    23\t  private readonly logger = new Logger(CodegraphProcessor.name);\n    24\t  private readonly config: ParallelProcessingConfig;\n...\nPath: src/shared/queues/enhanced-queue.module.ts\n...\n    11\t\n    12\t@Global()\n    13\t@Module({\n    14\t  imports: [\n    15\t    ConfigModule,\n    16\t    \n    17\t    // File Processing Queue\n    18\t    BullModule.registerQueueAsync({\n    19\t      name: QUEUE_NAMES.FILE_PROCESSING_QUEUE,\n    20\t      inject: [ConfigService],\n    21\t      useFactory: (configService: ConfigService) =&gt; {\n    22\t        const config = configService.get&lt;ParallelProcessingConfig&gt;('parallelProcessing');\n    23\t        const queueConfig = config?.queues?.fileProcessing;\n    24\t        \n    25\t        return {\n    26\t          ...DEFAULT_QUEUE_OPTIONS,\n    27\t          ...QUEUE_SPECIFIC_OPTIONS[QUEUE_NAMES.FILE_PROCESSING_QUEUE],\n    28\t          defaultJobOptions: {\n    29\t            ...DEFAULT_QUEUE_OPTIONS.defaultJobOptions,\n    30\t            ...QUEUE_SPECIFIC_OPTIONS[QUEUE_NAMES.FILE_PROCESSING_QUEUE].defaultJobOptions,\n    31\t            removeOnComplete: queueConfig?.removeOnComplete || 100,\n    32\t            removeOnFail: queueConfig?.removeOnFail || 50,\n    33\t          },\n    34\t        };\n    35\t      },\n    36\t    }),\n    37\t    \n    38\t    // Embedding Queue\n    39\t    BullModule.registerQueueAsync({\n    40\t      name: QUEUE_NAMES.EMBEDDING_QUEUE,\n    41\t      inject: [ConfigService],\n    42\t      useFactory: (configService: ConfigService) =&gt; {\n    43\t        const config = configService.get&lt;ParallelProcessingConfig&gt;('parallelProcessing');\n    44\t        const queueConfig = config?.queues?.embedding;\n    45\t        \n    46\t        return {\n    47\t          ...DEFAULT_QUEUE_OPTIONS,\n    48\t          ...QUEUE_SPECIFIC_OPTIONS[QUEUE_NAMES.EMBEDDING_QUEUE],\n    49\t          defaultJobOptions: {\n    50\t            ...DEFAULT_QUEUE_OPTIONS.defaultJobOptions,\n    51\t            ...QUEUE_SPECIFIC_OPTIONS[QUEUE_NAMES.EMBEDDING_QUEUE].defaultJobOptions,\n    52\t            removeOnComplete: queueConfig?.removeOnComplete || 50,\n    53\t            removeOnFail: queueConfig?.removeOnFail || 25,\n    54\t          },\n    55\t        };\n    56\t      },\n    57\t    }),\n...\n    79\t    \n    80\t    // Batch Processing Queue\n    81\t    BullModule.registerQueueAsync({\n    82\t      name: QUEUE_NAMES.BATCH_PROCESSING_QUEUE,\n    83\t      inject: [ConfigService],\n    84\t      useFactory: (configService: ConfigService) =&gt; {\n    85\t        const config = configService.get&lt;ParallelProcessingConfig&gt;('parallelProcessing');\n    86\t        const queueConfig = config?.queues?.batch;\n    87\t        \n    88\t        return {\n    89\t          ...DEFAULT_QUEUE_OPTIONS,\n    90\t          ...QUEUE_SPECIFIC_OPTIONS[QUEUE_NAMES.BATCH_PROCESSING_QUEUE],\n    91\t          defaultJobOptions: {\n    92\t            ...DEFAULT_QUEUE_OPTIONS.defaultJobOptions,\n    93\t            ...QUEUE_SPECIFIC_OPTIONS[QUEUE_NAMES.BATCH_PROCESSING_QUEUE].defaultJobOptions,\n    94\t            removeOnComplete: queueConfig?.removeOnComplete || 10,\n    95\t            removeOnFail: queueConfig?.removeOnFail || 5,\n    96\t          },\n    97\t        };\n    98\t      },\n    99\t    }),\n...\nPath: src/modules/sync/processors/file-processing.processor.ts\n     1\timport { Processor, WorkerHost, OnQueueActive, OnQueueCompleted, OnQueueFailed } from '@nestjs/bullmq';\n     2\timport { Logger } from '@nestjs/common';\n     3\timport { Job } from 'bullmq';\n     4\timport { ConfigService } from '@nestjs/config';\n     5\timport { PrismaService } from '../../../shared/database/prisma.service';\n     6\timport { BatchOperationsService } from '../../../shared/database/batch-operations.service';\n     7\timport { StorageService } from '../../../shared/storage/storage.service';\n     8\timport { WorkerPoolService } from '../../../shared/workers/worker-pool.service';\n     9\timport { DistributedLockService } from '../../../shared/locking/distributed-lock.service';\n    10\timport { GitlabService } from '../../gitlab/gitlab.service';\n    11\timport {\n    12\t  QUEUE_NAMES,\n    13\t  JOB_TYPES,\n    14\t  FileProcessingJobData,\n    15\t} from '../../../shared/queues/queue.constants';\n    16\timport { ParallelProcessingConfig } from '../../../config/parallel-processing.config';\n    17\t\n    18\texport interface FileProcessingResult {\n    19\t  filePath: string;\n    20\t  success: boolean;\n    21\t  size: number;\n    22\t  storageKey?: string;\n    23\t  language?: string;\n    24\t  error?: string;\n    25\t  processingTime: number;\n    26\t}\n    27\t\n    28\t/**\n    29\t * File Processing Processor - Handles parallel file processing using worker pools\n    30\t * Follows codebase-indexer's parallel file processing pattern\n    31\t */\n    32\t@Processor(QUEUE_NAMES.FILE_PROCESSING_QUEUE)\n    33\texport class FileProcessingProcessor extends WorkerHost {\n    34\t  private readonly logger = new Logger(FileProcessingProcessor.name);\n    35\t  private readonly config: ParallelProcessingConfig;\n...\n    67\t\n    68\t  /**\n    69\t   * Process file batch using worker pools (codebase-indexer approach)\n    70\t   */\n    71\t  @Processor(JOB_TYPES.PROCESS_FILE_BATCH)\n    72\t  async processFileBatch(job: Job&lt;FileProcessingJobData&gt;): Promise&lt;FileProcessingResult[]&gt; {\n    73\t    const { codebaseId, files, batchIndex, totalBatches, syncJobId, options } = job.data;\n    74\t    \n    75\t    this.logger.log(`Processing file batch ${batchIndex + 1}/${totalBatches} with ${files.length} files for codebase ${codebaseId}`);\n    76\t\n    77\t    // Acquire distributed lock for this codebase\n    78\t    const lockResource = `file-processing:${codebaseId}`;\n    79\t    const lockResult = await this.distributedLockService.acquireLock(lockResource, {\n    80\t      ttl: 300000, // 5 minutes\n    81\t      maxRetries: 3,\n    82\t    });\n...\nPath: src/modules/sync/processors/embedding.processor.ts\n     1\timport { Processor, WorkerHost, OnQueueActive, OnQueueCompleted, OnQueueFailed } from '@nestjs/bullmq';\n     2\timport { Logger } from '@nestjs/common';\n     3\timport { Job } from 'bullmq';\n     4\timport { ConfigService } from '@nestjs/config';\n     5\timport { PrismaService } from '../../../shared/database/prisma.service';\n     6\timport { StorageService } from '../../../shared/storage/storage.service';\n     7\timport { EnhancedTextSplitterService } from '../../codegraph/services/enhanced-text-splitter.service';\n     8\timport { EmbeddingService } from '../../codegraph/services/embedding.service';\n     9\timport { \n    10\t  QUEUE_NAMES, \n    11\t  JOB_TYPES, \n    12\t  EmbeddingJobData,\n    13\t} from '../../../shared/queues/queue.constants';\n    14\timport { ParallelProcessingConfig } from '../../../config/parallel-processing.config';\n...\nPath: src/modules/sync/processors/code-analysis.processor.ts\n     1\timport { Processor, WorkerHost, OnQueueActive, OnQueueCompleted, OnQueueFailed } from '@nestjs/bullmq';\n     2\timport { Logger } from '@nestjs/common';\n     3\timport { Job } from 'bullmq';\n     4\timport { ConfigService } from '@nestjs/config';\n     5\timport { PrismaService } from '../../../shared/database/prisma.service';\n     6\timport { StorageService } from '../../../shared/storage/storage.service';\n     7\timport { WorkerPoolService } from '../../../shared/workers/worker-pool.service';\n     8\timport { DistributedLockService } from '../../../shared/locking/distributed-lock.service';\n     9\timport { GitlabService } from '../../gitlab/gitlab.service';\n    10\timport { ScipService } from '../../codegraph/services/scip.service';\n    11\timport { TreeSitterService } from '../../codegraph/services/tree-sitter.service';\n    12\timport {\n    13\t  QUEUE_NAMES,\n    14\t  JOB_TYPES,\n    15\t  CodeAnalysisJobData,\n    16\t} from '../../../shared/queues/queue.constants';\n    17\timport { ParallelProcessingConfig } from '../../../config/parallel-processing.config';\n    18\t\n    19\texport interface CodeAnalysisResult {\n    20\t  filePath: string;\n    21\t  success: boolean;\n    22\t  symbols?: any[];\n    23\t  relationships?: any[];\n    24\t  scipData?: any;\n    25\t  treeSitterData?: any;\n    26\t  error?: string;\n    27\t  processingTime: number;\n    28\t}\n    29\t\n    30\t/**\n    31\t * Code Analysis Processor - Handles SCIP and Tree-sitter analysis\n    32\t * Follows codebase-indexer's parallel code analysis pattern\n    33\t */\n    34\t@Processor(QUEUE_NAMES.CODE_ANALYSIS_QUEUE)\n    35\texport class CodeAnalysisProcessor extends WorkerHost {\n    36\t  private readonly logger = new Logger(CodeAnalysisProcessor.name);\n    37\t  private readonly config: ParallelProcessingConfig;\n    38\t\n    39\t  constructor(\n    40\t    private configService: ConfigService,\n    41\t    private prisma: PrismaService,\n    42\t    private storageService: StorageService,\n    43\t    private workerPoolService: WorkerPoolService,\n    44\t    private distributedLockService: DistributedLockService,\n    45\t    private gitlabService: GitlabService,\n    46\t    private scipService: ScipService,\n    47\t    private treeSitterService: TreeSitterService,\n    48\t  ) {\n    49\t    super();\n    50\t    this.config = this.configService.get&lt;ParallelProcessingConfig&gt;('parallelProcessing');\n    51\t  }\n    52\t\n    53\t  @OnQueueActive()\n    54\t  onActive(job: Job) {\n    55\t    this.logger.debug(`Processing code analysis job ${job.id} for codebase ${job.data.codebaseId}`);\n    56\t  }\n    57\t\n    58\t  @OnQueueCompleted()\n    59\t  onCompleted(job: Job) {\n    60\t    this.logger.debug(`Completed code analysis job ${job.id} for codebase ${job.data.codebaseId}`);\n    61\t  }\n...\nPath: src/modules/sync/processors/progress-tracking.processor.ts\n...\n    31\t\n    32\t/**\n    33\t * Progress Tracking Processor - Handles real-time progress updates and WebSocket notifications\n    34\t * Follows codebase-indexer's progress tracking pattern\n    35\t */\n    36\t@Processor(QUEUE_NAMES.PROGRESS_TRACKING_QUEUE)\n    37\texport class ProgressTrackingProcessor extends WorkerHost {\n    38\t  private readonly logger = new Logger(ProgressTrackingProcessor.name);\n    39\t  private readonly config: ParallelProcessingConfig;\n    40\t\n    41\t  constructor(\n    42\t    private configService: ConfigService,\n    43\t    private prisma: PrismaService,\n    44\t    private webSocketService: WebSocketService,\n    45\t  ) {\n    46\t    super();\n    47\t    this.config = this.configService.get&lt;ParallelProcessingConfig&gt;('parallelProcessing');\n    48\t  }\n    49\t\n    50\t  @OnQueueActive()\n    51\t  onActive(job: Job) {\n    52\t    this.logger.debug(`Processing progress tracking job ${job.id}`);\n    53\t  }\n...\nPath: src/shared/queues/queue.constants.ts\n     1\t/**\n     2\t * Queue Names - Following codebase-indexer parallel processing architecture\n     3\t */\n     4\texport const QUEUE_NAMES = {\n     5\t  // Main orchestration queue\n     6\t  SYNC_QUEUE: 'sync',\n     7\t  CODEGRAPH_QUEUE: 'codegraph',\n     8\t  \n     9\t  // Parallel processing queues (new)\n    10\t  FILE_PROCESSING_QUEUE: 'file-processing',\n    11\t  EMBEDDING_QUEUE: 'embedding',\n    12\t  CODE_ANALYSIS_QUEUE: 'code-analysis',\n    13\t  BATCH_PROCESSING_QUEUE: 'batch-processing',\n    14\t  \n    15\t  // Specialized queues\n    16\t  PROGRESS_TRACKING_QUEUE: 'progress-tracking',\n    17\t  CLEANUP_QUEUE: 'cleanup',\n    18\t} as const;\n    19\t\n    20\t/**\n    21\t * Job Types for each queue\n    22\t */\n    23\texport const JOB_TYPES = {\n    24\t  // Sync Queue Jobs\n    25\t  SYNC_CODEBASE: 'sync-codebase',\n    26\t  \n    27\t  // Codegraph Queue Jobs\n    28\t  INDEX_CODEBASE: 'index-codebase',\n    29\t  \n    30\t  // File Processing Queue Jobs\n    31\t  PROCESS_FILE_BATCH: 'process-file-batch',\n    32\t  DOWNLOAD_FILE: 'download-file',\n    33\t  STORE_FILE_CONTENT: 'store-file-content',\n    34\t  VALIDATE_FILE: 'validate-file',\n    35\t  \n    36\t  // Embedding Queue Jobs\n    37\t  GENERATE_EMBEDDINGS_BATCH: 'generate-embeddings-batch',\n    38\t  PROCESS_CHUNKS: 'process-chunks',\n    39\t  STORE_EMBEDDINGS: 'store-embeddings',\n    40\t  \n    41\t  // Code Analysis Queue Jobs\n    42\t  ANALYZE_CODE_STRUCTURE: 'analyze-code-structure',\n    43\t  EXTRACT_SYMBOLS: 'extract-symbols',\n    44\t  BUILD_RELATIONSHIPS: 'build-relationships',\n    45\t  GENERATE_SCIP: 'generate-scip',\n    46\t  \n    47\t  // Batch Processing Queue Jobs\n    48\t  BATCH_DATABASE_OPERATIONS: 'batch-database-operations',\n    49\t  BATCH_STORAGE_OPERATIONS: 'batch-storage-operations',\n    50\t  BATCH_API_CALLS: 'batch-api-calls',\n    51\t  \n    52\t  // Progress Tracking Queue Jobs\n    53\t  UPDATE_PROGRESS: 'update-progress',\n    54\t  EMIT_WEBSOCKET_UPDATE: 'emit-websocket-update',\n    55\t  \n    56\t  // Cleanup Queue Jobs\n    57\t  CLEANUP_TEMP_FILES: 'cleanup-temp-files',\n    58\t  CLEANUP_EXPIRED_LOCKS: 'cleanup-expired-locks',\n    59\t  CLEANUP_OLD_JOBS: 'cleanup-old-jobs',\n    60\t} as const;\n    61\t\n    62\t/**\n    63\t * Queue Priorities\n    64\t */\n    65\texport const QUEUE_PRIORITIES = {\n    66\t  CRITICAL: 10,\n    67\t  HIGH: 7,\n    68\t  MEDIUM: 5,\n    69\t  LOW: 3,\n    70\t  BACKGROUND: 1,\n    71\t} as const;\n    72\t\n    73\t/**\n    74\t * Job Data Interfaces\n    75\t */\n...\n   116\t\n   117\t// Code Analysis Job Data\n   118\texport interface CodeAnalysisJobData {\n   119\t  codebaseId: string;\n   120\t  files: Array&lt;{\n   121\t    path: string;\n   122\t    language: string;\n   123\t    storageKey: string;\n   124\t  }&gt;;\n   125\t  analysisType: 'scip' | 'tree-sitter' | 'both';\n   126\t  batchIndex: number;\n   127\t  totalBatches: number;\n   128\t  syncJobId?: string;\n   129\t  options?: {\n   130\t    extractSymbols?: boolean;\n   131\t    buildRelationships?: boolean;\n   132\t    generateIndex?: boolean;\n   133\t  };\n   134\t}\n   135\t\n   136\t// Batch Processing Job Data\n   137\texport interface BatchProcessingJobData {\n   138\t  codebaseId: string;\n   139\t  operationType: string;\n   140\t  data: any[];\n   141\t  options?: {\n   142\t    batchSize?: number;\n   143\t    onConflict?: 'update' | 'skip' | 'error';\n   144\t    skipDuplicates?: boolean;\n   145\t    chunkDelay?: number;\n   146\t  };\n   147\t}\n...\nPath: src/config/redis.config.ts\n...\n     4\t\n     5\t@Injectable()\n     6\texport class RedisConfigService implements SharedBullConfigurationFactory {\n     7\t  constructor(private configService: ConfigService) {}\n     8\t\n     9\t  createSharedConfiguration(): BullRootModuleOptions {\n    10\t    return {\n    11\t      redis: {\n    12\t        host: this.configService.get&lt;string&gt;('QUEUE_REDIS_HOST', 'localhost'),\n    13\t        port: this.configService.get&lt;number&gt;('QUEUE_REDIS_PORT', 6379),\n    14\t        password: this.configService.get&lt;string&gt;('QUEUE_REDIS_PASSWORD'),\n    15\t        db: this.configService.get&lt;number&gt;('QUEUE_REDIS_DB', 1),\n    16\t        enableReadyCheck: false,\n    17\t        maxRetriesPerRequest: 3,\n    18\t      },\n    19\t      defaultJobOptions: {\n    20\t        removeOnComplete: 10,\n    21\t        removeOnFail: 5,\n    22\t        attempts: 3,\n    23\t        backoff: {\n    24\t          type: 'exponential',\n    25\t          delay: 2000,\n    26\t        },\n    27\t      },\n    28\t    };\n    29\t  }\n    30\t}\n...\nPath: src/modules/sync/sync.module.ts\n...\n    18\t\n    19\t@Module({\n    20\t  imports: [\n    21\t    ConfigModule,\n    22\t    BullModule.registerQueue({\n    23\t      name: SYNC_QUEUE,\n    24\t      defaultJobOptions: {\n    25\t        removeOnComplete: 50,\n    26\t        removeOnFail: 20,\n    27\t        attempts: 3,\n    28\t        backoff: {\n    29\t          type: 'exponential',\n    30\t          delay: 2000,\n    31\t        },\n    32\t        // Enhanced deduplication settings\n    33\t        jobId: undefined, // Will be set per job to ensure uniqueness\n    34\t        delay: 0,\n    35\t      },\n    36\t      settings: {\n    37\t        stalledInterval: 30 * 1000, // 30 seconds\n    38\t       ...&quot;,&quot;is_error&quot;:false,&quot;request_id&quot;:&quot;&quot;,&quot;tool_use_id&quot;:&quot;toolu_vrtx_01BqrGeCruHmJDsRArX6jTLi&quot;}},{&quot;id&quot;:2,&quot;type&quot;:4,&quot;ide_state_node&quot;:{&quot;workspaceFolders&quot;:[{&quot;repositoryRoot&quot;:&quot;/Users/deepak/TekCode/TekAIContextEngine2&quot;,&quot;folderRoot&quot;:&quot;/Users/deepak/TekCode/TekAIContextEngine2&quot;}],&quot;workspaceFoldersUnchanged&quot;:false}}],&quot;timestamp&quot;:&quot;2025-07-12T23:15:31.148Z&quot;,&quot;response_text&quot;:&quot;Now let me check the specific usage of `@Processor` decorator in NestJS BullMQ to understand the correct pattern:\n&quot;,&quot;workspace_file_chunks&quot;:[]},{&quot;status&quot;:&quot;success&quot;,&quot;request_id&quot;:&quot;7ee80746-6ce8-4d27-bcb2-692bb736c96a&quot;,&quot;request_message&quot;:&quot;&quot;,&quot;structured_output_nodes&quot;:[{&quot;id&quot;:1,&quot;type&quot;:5,&quot;content&quot;:&quot;&quot;,&quot;tool_use&quot;:{&quot;tool_use_id&quot;:&quot;toolu_vrtx_01W6xCPTRQb1YrFnnVPAzSag&quot;,&quot;tool_name&quot;:&quot;web-fetch&quot;,&quot;input_json&quot;:&quot;{\&quot;url\&quot;: \&quot;https://docs.nestjs.com/techniques/queues\&quot;}&quot;,&quot;mcp_server_name&quot;:&quot;&quot;,&quot;mcp_tool_name&quot;:&quot;&quot;}},{&quot;id&quot;:2,&quot;type&quot;:0,&quot;content&quot;:&quot;Let me check the NestJS BullMQ documentation more specifically:\n&quot;},{&quot;id&quot;:3,&quot;type&quot;:2,&quot;content&quot;:&quot;&quot;},{&quot;id&quot;:4,&quot;type&quot;:3,&quot;content&quot;:&quot;&quot;}],&quot;seen_state&quot;:&quot;seen&quot;,&quot;structured_request_nodes&quot;:[{&quot;id&quot;:1,&quot;type&quot;:1,&quot;tool_result_node&quot;:{&quot;content&quot;:&quot;- [Queues | NestJS - A progressive Node.js framework](https://docs.nestjs.com/techniques/queues)\n  The forRoot() method is used to register a bullmq package configuration ... decorated with the @Processor() decorator). To listen for an event, use the ...\n\n- [Queuing jobs in NestJS using @nestjs/bullmq package - DEV ...](https://dev.to/railsstudent/queuing-jobs-in-nestjs-using-nestjsbullmq-package-55c1)\n  Dec 18, 2023 ... The codes of queuing jobs in NestJS is the same regardless nestjs/bull or nestjs/bullmq is used. Use InjectQueue decorator to inject a queue and add a job with ...\n\n- [typescript - Mock MikroORM class in jest to unit test methods ...](https://stackoverflow.com/questions/71358154/mock-mikroorm-class-in-jest-to-unit-test-methods-decorated-with-userequestconte)\n  Mar 4, 2022 ... This error is triggered if this.orm of that class where you use the decorator is not instance of MikroORM class. If you put a mock in there, ...\n\n- [How to work with CLS inside a bull queue · Papooch nestjs-cls ...](https://github.com/Papooch/nestjs-cls/discussions/178)\n  Likely becaue the UseCls decorator is generic and uses a bit of typescript magic to be type-safe according to the function on which you use it. I'll try to ...\n\n- [Cannot find module '@nestjs/core' or its corresponding type ...](https://stackoverflow.com/questions/68145372/cannot-find-module-nestjs-core-or-its-corresponding-type-declarations)\n  Jun 26, 2021 ... Sometimes you may also get error: Cannot find module '@nestjs/mapped-types' or its corresponding type declarations. In that case, do: npm ...&quot;,&quot;is_error&quot;:false,&quot;request_id&quot;:&quot;&quot;,&quot;tool_use_id&quot;:&quot;toolu_vrtx_01A77Sqtssm6PMSNA9CURrig&quot;}},{&quot;id&quot;:2,&quot;type&quot;:4,&quot;ide_state_node&quot;:{&quot;workspaceFolders&quot;:[{&quot;repositoryRoot&quot;:&quot;/Users/deepak/TekCode/TekAIContextEngine2&quot;,&quot;folderRoot&quot;:&quot;/Users/deepak/TekCode/TekAIContextEngine2&quot;}],&quot;workspaceFoldersUnchanged&quot;:false}}],&quot;timestamp&quot;:&quot;2025-07-12T23:15:36.640Z&quot;,&quot;response_text&quot;:&quot;Let me check the NestJS BullMQ documentation more specifically:\n&quot;,&quot;workspace_file_chunks&quot;:[]},{&quot;status&quot;:&quot;success&quot;,&quot;request_id&quot;:&quot;865a5fb9-a71d-422b-8e17-814ccaaf1475&quot;,&quot;request_message&quot;:&quot;&quot;,&quot;structured_output_nodes&quot;:[{&quot;id&quot;:1,&quot;type&quot;:5,&quot;content&quot;:&quot;&quot;,&quot;tool_use&quot;:{&quot;tool_use_id&quot;:&quot;toolu_vrtx_014qxHaEG1J6mz91z1ktnu3R&quot;,&quot;tool_name&quot;:&quot;codebase-retrieval&quot;,&quot;input_json&quot;:&quot;{\&quot;information_request\&quot;: \&quot;Find examples of @Process decorator usage in NestJS BullMQ for handling specific job types within a processor class. Look for the correct decorator pattern for method-level job handlers.\&quot;}&quot;,&quot;mcp_server_name&quot;:&quot;&quot;,&quot;mcp_tool_name&quot;:&quot;&quot;}},{&quot;id&quot;:2,&quot;type&quot;:0,&quot;content&quot;:&quot;Let me search for the correct usage pattern in the codebase:\n&quot;},{&quot;id&quot;:3,&quot;type&quot;:2,&quot;content&quot;:&quot;&quot;},{&quot;id&quot;:4,&quot;type&quot;:3,&quot;content&quot;:&quot;&quot;}],&quot;seen_state&quot;:&quot;seen&quot;,&quot;structured_request_nodes&quot;:[{&quot;id&quot;:1,&quot;type&quot;:1,&quot;tool_result_node&quot;:{&quot;content&quot;:&quot; Documentation | NestJS - A progressive Node.js framework&quot;,&quot;is_error&quot;:false,&quot;request_id&quot;:&quot;&quot;,&quot;tool_use_id&quot;:&quot;toolu_vrtx_01W6xCPTRQb1YrFnnVPAzSag&quot;}},{&quot;id&quot;:2,&quot;type&quot;:4,&quot;ide_state_node&quot;:{&quot;workspaceFolders&quot;:[{&quot;repositoryRoot&quot;:&quot;/Users/deepak/TekCode/TekAIContextEngine2&quot;,&quot;folderRoot&quot;:&quot;/Users/deepak/TekCode/TekAIContextEngine2&quot;}],&quot;workspaceFoldersUnchanged&quot;:false}}],&quot;timestamp&quot;:&quot;2025-07-12T23:15:44.385Z&quot;,&quot;response_text&quot;:&quot;Let me search for the correct usage pattern in the codebase:\n&quot;,&quot;workspace_file_chunks&quot;:[]},{&quot;status&quot;:&quot;success&quot;,&quot;request_id&quot;:&quot;47e64d1b-777f-4d80-a951-4e4b88e702a1&quot;,&quot;request_message&quot;:&quot;&quot;,&quot;structured_output_nodes&quot;:[{&quot;id&quot;:1,&quot;type&quot;:5,&quot;content&quot;:&quot;&quot;,&quot;tool_use&quot;:{&quot;tool_use_id&quot;:&quot;toolu_vrtx_01KaNnahfvLdZem7o5azhszr&quot;,&quot;tool_name&quot;:&quot;codebase-retrieval&quot;,&quot;input_json&quot;:&quot;{\&quot;information_request\&quot;: \&quot;Find imports and usage of @Process decorator from @nestjs/bullmq package. Look for the correct method-level decorator for handling specific job types.\&quot;}&quot;,&quot;mcp_server_name&quot;:&quot;&quot;,&quot;mcp_tool_name&quot;:&quot;&quot;}},{&quot;id&quot;:2,&quot;type&quot;:0,&quot;content&quot;:&quot;Now I can see the issue! Looking at the codebase, I can see that there's a pattern inconsistency. The issue is that `@Processor` is being used incorrectly for method-level decorators. In NestJS BullMQ, the correct decorator for individual job handlers within a processor class is `@Process`, not `@Processor`.\n\nLet me check if there's a `@Process` decorator available:\n&quot;},{&quot;id&quot;:3,&quot;type&quot;:2,&quot;content&quot;:&quot;&quot;},{&quot;id&quot;:4,&quot;type&quot;:3,&quot;content&quot;:&quot;&quot;}],&quot;seen_state&quot;:&quot;seen&quot;,&quot;structured_request_nodes&quot;:[{&quot;id&quot;:1,&quot;type&quot;:1,&quot;tool_result_node&quot;:{&quot;content&quot;:&quot;The following code sections were retrieved:\nPath: docs/DEVELOPMENT.md\n...\n   249\t\n   250\t3. **Implement service logic**:\n   251\t   ```typescript\n   252\t   async create(createDto: CreateItemDto) {\n   253\t     return this.prisma.item.create({\n   254\t       data: createDto,\n   255\t     });\n   256\t   }\n   257\t   ```\n   258\t\n   259\t4. **Add tests**:\n   260\t   ```typescript\n   261\t   describe('create', () =&gt; {\n   262\t     it('should create item', async () =&gt; {\n   263\t       // Test implementation\n   264\t     });\n   265\t   });\n   266\t   ```\n   267\t\n   268\t### 4. Background Jobs\n   269\t\n   270\t1. **Define job processor**:\n   271\t   ```typescript\n   272\t   @Process('job-name')\n   273\t   async handleJob(job: Job&lt;JobData&gt;) {\n   274\t     // Job implementation\n   275\t   }\n   276\t   ```\n   277\t\n   278\t2. **Queue job**:\n   279\t   ```typescript\n   280\t   await this.queue.add('job-name', jobData);\n   281\t   ```\n   282\t\n   283\t3. **Test job processing**:\n   284\t   ```typescript\n   285\t   // Test job execution\n   286\t   ```\n...\nPath: src/modules/sync/processors/file-processing.processor.ts\n     1\timport { Processor, WorkerHost, OnQueueActive, OnQueueCompleted, OnQueueFailed } from '@nestjs/bullmq';\n     2\timport { Logger } from '@nestjs/common';\n     3\timport { Job } from 'bullmq';\n     4\timport { ConfigService } from '@nestjs/config';\n     5\timport { PrismaService } from '../../../shared/database/prisma.service';\n     6\timport { BatchOperationsService } from '../../../shared/database/batch-operations.service';\n     7\timport { StorageService } from '../../../shared/storage/storage.service';\n     8\timport { WorkerPoolService } from '../../../shared/workers/worker-pool.service';\n     9\timport { DistributedLockService } from '../../../shared/locking/distributed-lock.service';\n    10\timport { GitlabService } from '../../gitlab/gitlab.service';\n    11\timport {\n    12\t  QUEUE_NAMES,\n    13\t  JOB_TYPES,\n    14\t  FileProcessingJobData,\n...\n    36\t\n    37\t  constructor(\n    38\t    private configService: ConfigService,\n    39\t    private prisma: PrismaService,\n    40\t    private batchOperations: BatchOperationsService,\n    41\t    private storageService: StorageService,\n    42\t    private workerPoolService: WorkerPoolService,\n    43\t    private distributedLockService: DistributedLockService,\n    44\t    private gitlabService: GitlabService,\n    45\t  ) {\n    46\t    super();\n    47\t    this.config = this.configService.get&lt;ParallelProcessingConfig&gt;('parallelProcessing');\n    48\t  }\n    49\t\n    50\t  @OnQueueActive()\n    51\t  onActive(job: Job) {\n    52\t    this.logger.debug(`Processing file batch job ${job.id} for codebase ${job.data.codebaseId}`);\n    53\t  }\n    54\t\n    55\t  @OnQueueCompleted()\n    56\t  onCompleted(job: Job) {\n    57\t    this.logger.debug(`Completed file batch job ${job.id} for codebase ${job.data.codebaseId}`);\n    58\t  }\n...\n    67\t\n    68\t  /**\n    69\t   * Process file batch using worker pools (codebase-indexer approach)\n    70\t   */\n    71\t  @Processor(JOB_TYPES.PROCESS_FILE_BATCH)\n    72\t  async processFileBatch(job: Job&lt;FileProcessingJobData&gt;): Promise&lt;FileProcessingResult[]&gt; {\n    73\t    const { codebaseId, files, batchIndex, totalBatches, syncJobId, options } = job.data;\n    74\t    \n    75\t    this.logger.log(`Processing file batch ${batchIndex + 1}/${totalBatches} with ${files.length} files for codebase ${codebaseId}`);\n    76\t\n    77\t    // Acquire distributed lock for this codebase\n    78\t    const lockResource = `file-processing:${codebaseId}`;\n    79\t    const lockResult = await this.distributedLockService.acquireLock(lockResource, {\n    80\t      ttl: 300000, // 5 minutes\n    81\t      maxRetries: 3,\n    82\t    });\n...\nPath: src/modules/sync/processors/progress-tracking.processor.ts\n...\n    31\t\n    32\t/**\n    33\t * Progress Tracking Processor - Handles real-time progress updates and WebSocket notifications\n    34\t * Follows codebase-indexer's progress tracking pattern\n    35\t */\n    36\t@Processor(QUEUE_NAMES.PROGRESS_TRACKING_QUEUE)\n    37\texport class ProgressTrackingProcessor extends WorkerHost {\n    38\t  private readonly logger = new Logger(ProgressTrackingProcessor.name);\n    39\t  private readonly config: ParallelProcessingConfig;\n    40\t\n    41\t  constructor(\n    42\t    private configService: ConfigService,\n    43\t    private prisma: PrismaService,\n    44\t    private webSocketService: WebSocketService,\n    45\t  ) {\n    46\t    super();\n    47\t    this.config = this.configService.get&lt;ParallelProcessingConfig&gt;('parallelProcessing');\n    48\t  }\n    49\t\n    50\t  @OnQueueActive()\n    51\t  onActive(job: Job) {\n    52\t    this.logger.debug(`Processing progress tracking job ${job.id}`);\n    53\t  }\n...\nPath: src/modules/sync/processors/batch.processor.ts\n     1\timport { Processor, WorkerHost, OnQueueActive, OnQueueCompleted, OnQueueFailed } from '@nestjs/bullmq';\n     2\timport { Logger } from '@nestjs/common';\n     3\timport { Job } from 'bullmq';\n     4\timport { ConfigService } from '@nestjs/config';\n     5\timport { PrismaService } from '../../../shared/database/prisma.service';\n     6\timport { BatchOperationsService } from '../../../shared/database/batch-operations.service';\n     7\timport { StorageService } from '../../../shared/storage/storage.service';\n     8\timport { DistributedLockService } from '../../../shared/locking/distributed-lock.service';\n     9\timport { \n    10\t  QUEUE_NAMES, \n    11\t  JOB_TYPES, \n    12\t  BatchProcessingJobData,\n    13\t} from '../../../shared/queues/queue.constants';\n    14\timport { ParallelProcessingConfig } from '../../../config/parallel-processing.config';\n...\n    25\t\n    26\t/**\n    27\t * Batch Processor - Handles batch database and storage operations\n    28\t * Follows codebase-indexer's batch processing pattern for performance\n    29\t */\n    30\t@Processor(QUEUE_NAMES.BATCH_PROCESSING_QUEUE)\n    31\texport class BatchProcessor extends WorkerHost {\n    32\t  private readonly logger = new Logger(BatchProcessor.name);\n    33\t  private readonly config: ParallelProcessingConfig;\n    34\t\n    35\t  constructor(\n    36\t    private configService: ConfigService,\n    37\t    private prisma: PrismaService,\n    38\t    private batchOperations: BatchOperationsService,\n    39\t    private storageService: StorageService,\n    40\t    private distributedLockService: DistributedLockService,\n    41\t  ) {\n    42\t    super();\n    43\t    this.config = this.configService.get&lt;ParallelProcessingConfig&gt;('parallelProcessing');\n    44\t  }\n...\n    60\t\n    61\t  /**\n    62\t   * Process batch database operations\n    63\t   */\n    64\t  @Processor(JOB_TYPES.BATCH_DATABASE_OPERATIONS)\n    65\t  async processBatchDatabaseOperations(job: Job&lt;BatchProcessingJobData&gt;): Promise&lt;BatchOperationResult&gt; {\n    66\t    const { codebaseId, operationType, data, options } = job.data;\n    67\t    \n    68\t    this.logger.log(`Processing batch database operation: ${operationType} for codebase ${codebaseId}`);\n    69\t\n    70\t    const startTime = Date.now();\n...\n   117\t\n   118\t  /**\n   119\t   * Process batch storage operations\n   120\t   */\n   121\t  @Processor(JOB_TYPES.BATCH_STORAGE_OPERATIONS)\n   122\t  async processBatchStorageOperations(job: Job&lt;BatchProcessingJobData&gt;): Promise&lt;BatchOperationResult&gt; {\n   123\t    const { codebaseId, operationType, data, options } = job.data;\n   124\t    \n   125\t    this.logger.log(`Processing batch storage operation: ${operationType} for codebase ${codebaseId}`);\n   126\t\n   127\t    const startTime = Date.now();\n...\nPath: src/modules/sync/processors/code-analysis.processor.ts\n...\n    13\t  QUEUE_NAMES,\n    14\t  JOB_TYPES,\n    15\t  CodeAnalysisJobData,\n    16\t} from '../../../shared/queues/queue.constants';\n    17\timport { ParallelProcessingConfig } from '../../../config/parallel-processing.config';\n    18\t\n    19\texport interface CodeAnalysisResult {\n    20\t  filePath: string;\n    21\t  success: boolean;\n    22\t  symbols?: any[];\n    23\t  relationships?: any[];\n    24\t  scipData?: any;\n    25\t  treeSitterData?: any;\n    26\t  error?: string;\n    27\t  processingTime: number;\n    28\t}\n    29\t\n    30\t/**\n    31\t * Code Analysis Processor - Handles SCIP and Tree-sitter analysis\n    32\t * Follows codebase-indexer's parallel code analysis pattern\n    33\t */\n    34\t@Processor(QUEUE_NAMES.CODE_ANALYSIS_QUEUE)\n    35\texport class CodeAnalysisProcessor extends WorkerHost {\n    36\t  private readonly logger = new Logger(CodeAnalysisProcessor.name);\n    37\t  private readonly config: ParallelProcessingConfig;\n    38\t\n    39\t  constructor(\n    40\t    private configService: ConfigService,\n    41\t    private prisma: PrismaService,\n    42\t    private storageService: StorageService,\n    43\t    private workerPoolService: WorkerPoolService,\n    44\t    private distributedLockService: DistributedLockService,\n    45\t    private gitlabService: GitlabService,\n    46\t    private scipService: ScipService,\n    47\t    private treeSitterService: TreeSitterService,\n    48\t  ) {\n    49\t    super();\n    50\t    this.config = this.configService.get&lt;ParallelProcessingConfig&gt;('parallelProcessing');\n    51\t  }\n    52\t\n    53\t  @OnQueueActive()\n    54\t  onActive(job: Job) {\n    55\t    this.logger.debug(`Processing code analysis job ${job.id} for codebase ${job.data.codebaseId}`);\n    56\t  }\n    57\t\n    58\t  @OnQueueCompleted()\n    59\t  onCompleted(job: Job) {\n    60\t    this.logger.debug(`Completed code analysis job ${job.id} for codebase ${job.data.codebaseId}`);\n    61\t  }\n...\nPath: src/modules/codegraph/codegraph.processor.ts\n     1\timport { Processor, WorkerHost, OnQueueActive, OnQueueCompleted, OnQueueFailed } from '@nestjs/bullmq';\n     2\timport { Logger } from '@nestjs/common';\n     3\timport { Job } from 'bullmq';\n     4\timport { PrismaService } from '@/shared/database/prisma.service';\n     5\timport { StorageService } from '@/shared/storage/storage.service';\n     6\timport { WebSocketService } from '../websocket/websocket.service';\n     7\timport { DistributedLockService } from '../../shared/locking/distributed-lock.service';\n     8\timport { EnhancedQueueService } from '../../shared/queues/enhanced-queue.service';\n     9\timport { WorkerPoolService } from '../../shared/workers/worker-pool.service';\n    10\timport { ScipService } from './services/scip.service';\n    11\timport { TreeSitterService } from './services/tree-sitter.service';\n    12\timport { EmbeddingService } from './services/embedding.service';\n    13\timport { BadgerDbService } from './services/badger-db.service';\n    14\timport { CodegraphConfigService } from './services/codegraph-config.service';\n    15\timport { IndexTaskType, IndexTaskStatus } from '@prisma/client';\n    16\timport { CodegraphJobData } from './codegraph.service';\n    17\timport { QUEUE_NAMES, JOB_TYPES } from '../../shared/queues/queue.constants';\n    18\timport { ParallelProcessingConfig } from '../../config/parallel-processing.config';\n    19\timport { ConfigService } from '@nestjs/config';\n    20\t\n    21\t@Processor(QUEUE_NAMES.CODEGRAPH_QUEUE)\n    22\texport class CodegraphProcessor extends WorkerHost {\n    23\t  private readonly logger = new Logger(CodegraphProcessor.name);\n    24\t  private readonly config: ParallelProcessingConfig;\n...\n    58\t\n    59\t  /**\n    60\t   * Main entry point - Parallel processing following codebase-indexer pattern\n    61\t   */\n    62\t  @Processor(JOB_TYPES.INDEX_CODEBASE)\n    63\t  async handleIndexCodebase(job: Job&lt;CodegraphJobData&gt;): Promise&lt;void&gt; {\n    64\t    const { codebaseId, taskType, syncJobId, userId, options } = job.data;\n    65\t    const jobId = job.id as string;\n    66\t\n    67\t    this.logger.log(`Starting parallel processing for codebase: ${codebaseId}, type: ${taskType}`);\n    68\t\n    69\t    // Acquire distributed lock for this codebase\n    70\t    const lockResource = `codegraph-processing:${codebaseId}`;\n    71\t    const lockResult = await this.distributedLockService.acquireLock(lockResource, {\n    72\t      ttl: 1800000, // 30 minutes\n    73\t      maxRetries: 3,\n    74\t    });\n    75\t\n    76\t    if (!lockResult.acquired) {\n    77\t      throw new Error(`Failed to acquire lock for codebase ${codebaseId}: ${lockResult.error}`);\n    78\t    }\n...\n   135\t\n   136\t    // Pipeline 2: Code Analysis (SCIP + Tree-sitter)\n   137\t    promises.push(this.executeCodeAnalysisPipeline(codebaseId, files, totalBatches, syncJobId));\n   138\t\n   139\t    // Wait for all pipelines to complete\n   140\t    const results = await Promise.allSettled(promises);\n   141\t\n   142\t    // Check for failures\n   143\t    const failures = results.filter(result =&gt; result.status === 'rejected');\n   144\t    if (failures.length &gt; 0) {\n   145\t      this.logger.error(`${failures.length} pipelines failed:`, failures);\n   146\t      // Don't throw - allow partial success\n   147\t    }\n   148\t\n   149\t    // Update progress\n   150\t    await this.updateJobProgress(jobId, 100, 'All pipelines completed');\n   151\t  }\n...\n   163\t\n   164\t    try {\n   165\t      // Step 1: Submit file processing jobs\n   166\t      const batchSize = this.config?.performance?.maxFilesPerBatch || 100;\n   167\t      const fileBatches = this.createBatches(files, batchSize);\n   168\t\n   169\t      const fileProcessingPromises = fileBatches.map((batch, index) =&gt;\n   170\t        this.enhancedQueueService.submitFileProcessingJob({\n   171\t          codebaseId,\n   172\t          files: batch.map(f =&gt; ({\n   173\t            path: f.path,\n   174\t            hash: f.hash || '',\n   175\t            size: f.size || 0,\n   176\t            language: f.language,\n   177\t            storageKey: f.storageKey,\n   178\t          })),\n   179\t          batchIndex: index,\n   180\t          totalBatches,\n   181\t          syncJobId,\n   182\t          options: {\n   183\t            forceReprocess: false,\n   184\t            validateContent: true,\n   185\t            extractMetadata: true,\n   186\t          },\n   187\t        })\n   188\t      );\n...\n   234\t\n   235\t    try {\n   236\t      const batchSize = this.config?.performance?.maxFilesPerBatch || 100;\n   237\t      const fileBatches = this.createBatches(files, batchSize);\n   238\t\n   239\t      const codeAnalysisPromises = fileBatches.map((batch, index) =&gt;\n   240\t        this.enhancedQueueService.submitCodeAnalysisJob({\n   241\t          codebaseId,\n   242\t          files: batch.map(f =&gt; ({\n   243\t            path: f.path,\n   244\t            language: f.language || 'text',\n   245\t            storageKey: f.storageKey || f.path,\n   246\t          })),\n   247\t          analysisType: 'both', // SCIP + Tree-sitter\n   248\t          batchIndex: index,\n   249\t          totalBatches,\n   250\t          syncJobId,\n   251\t          options: {\n   252\t            extractSymbols: true,\n   253\t            buildRelationships: true,\n   254\t            generateIndex: true,\n   255\t          },\n   256\t        })\n   257\t      );\n...\nPath: src/modules/sync/sync.processor.ts\n...\n    13\timport { EnhancedQueueService } from '../../shared/queues/enhanced-queue.service';\n    14\timport { WorkerPoolService } from '../../shared/workers/worker-pool.service';\n    15\timport { SyncJobType, SyncJobStatus, CodebaseStatus, IndexTaskType } from '@prisma/client';\n    16\timport { SyncJobData } from './sync.service';\n    17\timport { QUEUE_NAMES, JOB_TYPES } from '../../shared/queues/queue.constants';\n    18\timport { ParallelProcessingConfig } from '../../config/parallel-processing.config';\n    19\t\n    20\t@Processor(QUEUE_NAMES.SYNC_QUEUE)\n    21\texport class SyncProcessor extends WorkerHost {\n    22\t  private readonly logger = new Logger(SyncProcessor.name);\n    23\t  private readonly config: ParallelProcessingConfig;\n    24\t\n    25\t  constructor(\n    26\t    private configService: ConfigService,\n    27\t    private prisma: PrismaService,\n    28\t    private gitlabService: GitlabService,\n    29\t    private fileScannerService: FileScannerService,\n    30\t    private fileService: FileService,\n    31\t    private storageService: StorageService,\n    32\t    private webSocketService: WebSocketService,\n    33\t    private codegraphService: CodegraphService,\n    34\t    private distributedLockService: DistributedLockService,\n    35\t    private enhancedQueueService: EnhancedQueueService,\n    36\t    private workerPoolService: WorkerPoolService,\n    37\t  ) {\n    38\t    super();\n    39\t    this.config = this.configService.get&lt;ParallelProcessingConfig&gt;('parallelProcessing');\n    40\t  }\n    41\t\n    42\t  @OnQueueActive()\n    43\t  onActive(job: Job) {\n    44\t    this.logger.debug(`Processing sync job ${job.id} for codebase ${job.data.codebaseId}`);\n    45\t  }\n...\n    56\t\n    57\t  /**\n    58\t   * Main orchestrator - Coordinates parallel pipelines (codebase-indexer approach)\n    59\t   */\n    60\t  @Processor(JOB_TYPES.SYNC_CODEBASE)\n    61\t  async handleSyncCodebase(job: Job&lt;SyncJobData&gt;): Promise&lt;void&gt; {\n    62\t    const { codebaseId, type, metadata } = job.data;\n    63\t    const syncJobId = metadata?.syncJobId;\n    64\t\n    65\t    this.logger.log(`Starting parallel sync orchestration for codebase: ${codebaseId}`);\n    66\t\n    67\t    // Acquire distributed lock for this codebase\n    68\t    const lockResource = `sync-orchestration:${codebaseId}`;\n    69\t    const lockResult = await this.distributedLockService.acquireLock(lockResource, {\n    70\t      ttl: 3600000, // 1 hour\n    71\t      maxRetries: 3,\n    72\t    });\n    73\t\n    74\t    if (!lockResult.acquired) {\n    75\t      throw new Error(`Failed to acquire sync lock for codebase ${codebaseId}: ${lockResult.error}`);\n    76\t    }\n...\n   126\t\n   127\t    try {\n   128\t      // Phase 1: Repository Sync &amp; File Discovery\n   129\t      await this.updateProgress(codebaseId, syncJobId, 'repository-sync', 10, 'Starting repository sync and file discovery');\n   130\t      await this.executeRepositorySync(codebaseId, syncJobId, metadata);\n   131\t\n   132\t      // Phase 2: Get discovered files\n   133\t      await this.updateProgress(codebaseId, syncJobId, 'file-discovery', 20, 'Discovering files in repository');\n   134\t      const files = await this.getDiscoveredFiles(codebaseId);\n   135\t\n   136\t      if (files.length === 0) {\n   137\t        this.logger.warn(`No files discovered for codebase ${codebaseId}`);\n   138\t        await this.updateProgress(codebaseId, syncJobId, 'completed', 100, 'No files found to process');\n   139\t        return;\n   140\t      }\n...\n   282\t\n   283\t    // Submit file processing jobs\n   284\t    const fileProcessingPromises = fileBatches.map((batch, index) =&gt;\n   285\t      this.enhancedQueueService.submitFileProcessingJob({\n   286\t        codebaseId,\n   287\t        files: batch.map(f =&gt; ({\n   288\t          path: f.path,\n   289\t          hash: f.hash || '',\n   290\t          size: f.size || 0,\n   291\t          language: f.language,\n   292\t          storageKey: f.storageKey,\n   293\t        })),\n   294\t        batchIndex: index,\n   295\t        totalBatches,\n   296\t        syncJobId,\n   297\t        options: {\n   298\t          forceReprocess: false,\n   299\t          validateContent: true,\n   300\t          extractMetadata: true,\n   301\t        },\n   302\t      })\n   303\t    );\n...\n   348\t\n   349\t    const codeAnalysisPromises = fileBatches.map((batch, index) =&gt;\n   350\t      this.enhancedQueueService.submitCodeAnalysisJob({\n   351\t        codebaseId,\n   352\t        files: batch.map(f =&gt; ({\n   353\t          path: f.path,\n   354\t          language: f.language || 'text',\n   355\t          storageKey: f.storageKey || f.path,\n   356\t        })),\n   357\t        analysisType: 'both', // SCIP + Tree-sitter\n   358\t        batchIndex: index,\n   359\t        totalBatches,\n   360\t        syncJobId,\n   361\t        options: {\n   362\t          extractSymbols: true,\n   363\t          buildRelationships: true,\n   364\t          generateIndex: true,\n   365\t        },\n   366\t      })\n   367\t    );\n   368\t\n   369\t    // Wait for code analysis to complete\n   370\t    await Promise.all(codeAnalysisPromises);\n   371\t\n   372\t    this.logger.log(`Code analysis pipeline completed for ${files.length} files`);\n   373\t  }\n...\n&quot;,&quot;is_error&quot;:false,&quot;request_id&quot;:&quot;&quot;,&quot;tool_use_id&quot;:&quot;toolu_vrtx_014qxHaEG1J6mz91z1ktnu3R&quot;}},{&quot;id&quot;:2,&quot;type&quot;:4,&quot;ide_state_node&quot;:{&quot;workspaceFolders&quot;:[{&quot;repositoryRoot&quot;:&quot;/Users/deepak/TekCode/TekAIContextEngine2&quot;,&quot;folderRoot&quot;:&quot;/Users/deepak/TekCode/TekAIContextEngine2&quot;}],&quot;workspaceFoldersUnchanged&quot;:false}}],&quot;timestamp&quot;:&quot;2025-07-12T23:15:52.933Z&quot;,&quot;response_text&quot;:&quot;Now I can see the issue! Looking at the codebase, I can see that there's a pattern inconsistency. The issue is that `@Processor` is being used incorrectly for method-level decorators. In NestJS BullMQ, the correct decorator for individual job handlers within a processor class is `@Process`, not `@Processor`.\n\nLet me check if there's a `@Process` decorator available:\n&quot;,&quot;workspace_file_chunks&quot;:[]},{&quot;status&quot;:&quot;success&quot;,&quot;request_id&quot;:&quot;9d12d1b4-715e-44d6-bdf8-258d3f577a0f&quot;,&quot;request_message&quot;:&quot;&quot;,&quot;structured_output_nodes&quot;:[{&quot;id&quot;:1,&quot;type&quot;:5,&quot;content&quot;:&quot;&quot;,&quot;tool_use&quot;:{&quot;tool_use_id&quot;:&quot;toolu_vrtx_01TjXWPK8vHGzKU9dDgQYMnf&quot;,&quot;tool_name&quot;:&quot;view&quot;,&quot;input_json&quot;:&quot;{\&quot;path\&quot;: \&quot;src/modules/sync/sync.processor.ts\&quot;, \&quot;type\&quot;: \&quot;file\&quot;, \&quot;view_range\&quot;: [1, 5]}&quot;,&quot;mcp_server_name&quot;:&quot;&quot;,&quot;mcp_tool_name&quot;:&quot;&quot;}},{&quot;id&quot;:2,&quot;type&quot;:0,&quot;content&quot;:&quot;Perfect! Now I can see the issue clearly. Looking at the codebase, I can see that there's an inconsistency in the decorator usage. The issue is that `@Processor` is being used incorrectly for method-level job handlers. \n\nIn NestJS BullMQ:\n- `@Processor(queueName)` is used at the **class level** to define which queue the processor handles\n- `@Process(jobName)` is used at the **method level** to handle specific job types within that queue\n\nHowever, I can see that in this codebase, they're incorrectly using `@Processor(JOB_TYPES.SYNC_CODEBASE)` at the method level, which is causing the TypeScript error.\n\nLet me check if `@Process` is available in the imports:\n&quot;},{&quot;id&quot;:3,&quot;type&quot;:2,&quot;content&quot;:&quot;&quot;},{&quot;id&quot;:4,&quot;type&quot;:3,&quot;content&quot;:&quot;&quot;}],&quot;seen_state&quot;:&quot;seen&quot;,&quot;structured_request_nodes&quot;:[{&quot;id&quot;:1,&quot;type&quot;:1,&quot;tool_result_node&quot;:{&quot;content&quot;:&quot;The following code sections were retrieved:\nPath: docs/DEVELOPMENT.md\n...\n   249\t\n   250\t3. **Implement service logic**:\n   251\t   ```typescript\n   252\t   async create(createDto: CreateItemDto) {\n   253\t     return this.prisma.item.create({\n   254\t       data: createDto,\n   255\t     });\n   256\t   }\n   257\t   ```\n   258\t\n   259\t4. **Add tests**:\n   260\t   ```typescript\n   261\t   describe('create', () =&gt; {\n   262\t     it('should create item', async () =&gt; {\n   263\t       // Test implementation\n   264\t     });\n   265\t   });\n   266\t   ```\n   267\t\n   268\t### 4. Background Jobs\n   269\t\n   270\t1. **Define job processor**:\n   271\t   ```typescript\n   272\t   @Process('job-name')\n   273\t   async handleJob(job: Job&lt;JobData&gt;) {\n   274\t     // Job implementation\n   275\t   }\n   276\t   ```\n   277\t\n   278\t2. **Queue job**:\n   279\t   ```typescript\n   280\t   await this.queue.add('job-name', jobData);\n   281\t   ```\n   282\t\n   283\t3. **Test job processing**:\n   284\t   ```typescript\n   285\t   // Test job execution\n   286\t   ```\n...\nPath: src/shared/queues/enhanced-queue.module.ts\n...\n    11\t\n    12\t@Global()\n    13\t@Module({\n    14\t  imports: [\n    15\t    ConfigModule,\n    16\t    \n    17\t    // File Processing Queue\n    18\t    BullModule.registerQueueAsync({\n    19\t      name: QUEUE_NAMES.FILE_PROCESSING_QUEUE,\n    20\t      inject: [ConfigService],\n    21\t      useFactory: (configService: ConfigService) =&gt; {\n    22\t        const config = configService.get&lt;ParallelProcessingConfig&gt;('parallelProcessing');\n    23\t        const queueConfig = config?.queues?.fileProcessing;\n    24\t        \n    25\t        return {\n    26\t          ...DEFAULT_QUEUE_OPTIONS,\n    27\t          ...QUEUE_SPECIFIC_OPTIONS[QUEUE_NAMES.FILE_PROCESSING_QUEUE],\n    28\t          defaultJobOptions: {\n    29\t            ...DEFAULT_QUEUE_OPTIONS.defaultJobOptions,\n    30\t            ...QUEUE_SPECIFIC_OPTIONS[QUEUE_NAMES.FILE_PROCESSING_QUEUE].defaultJobOptions,\n    31\t            removeOnComplete: queueConfig?.removeOnComplete || 100,\n    32\t            removeOnFail: queueConfig?.removeOnFail || 50,\n    33\t          },\n    34\t        };\n    35\t      },\n    36\t    }),\n    37\t    \n    38\t    // Embedding Queue\n    39\t    BullModule.registerQueueAsync({\n    40\t      name: QUEUE_NAMES.EMBEDDING_QUEUE,\n    41\t      inject: [ConfigService],\n    42\t      useFactory: (configService: ConfigService) =&gt; {\n    43\t        const config = configService.get&lt;ParallelProcessingConfig&gt;('parallelProcessing');\n    44\t        const queueConfig = config?.queues?.embedding;\n    45\t        \n    46\t        return {\n    47\t          ...DEFAULT_QUEUE_OPTIONS,\n    48\t          ...QUEUE_SPECIFIC_OPTIONS[QUEUE_NAMES.EMBEDDING_QUEUE],\n    49\t          defaultJobOptions: {\n    50\t            ...DEFAULT_QUEUE_OPTIONS.defaultJobOptions,\n    51\t            ...QUEUE_SPECIFIC_OPTIONS[QUEUE_NAMES.EMBEDDING_QUEUE].defaultJobOptions,\n    52\t            removeOnComplete: queueConfig?.removeOnComplete || 50,\n    53\t            removeOnFail: queueConfig?.removeOnFail || 25,\n    54\t          },\n    55\t        };\n    56\t      },\n    57\t    }),\n    58\t    \n    59\t    // Code Analysis Queue\n    60\t    BullModule.registerQueueAsync({\n    61\t      name: QUEUE_NAMES.CODE_ANALYSIS_QUEUE,\n    62\t      inject: [ConfigService],\n    63\t      useFactory: (configService: ConfigService) =&gt; {\n    64\t        const config = configService.get&lt;ParallelProcessingConfig&gt;('parallelProcessing');\n    65\t        const queueConfig = config?.queues?.codeAnalysis;\n    66\t        \n    67\t        return {\n    68\t          ...DEFAULT_QUEUE_OPTIONS,\n    69\t          ...QUEUE_SPECIFIC_OPTIONS[QUEUE_NAMES.CODE_ANALYSIS_QUEUE],\n    70\t          defaultJobOptions: {\n    71\t            ...DEFAULT_QUEUE_OPTIONS.defaultJobOptions,\n    72\t            ...QUEUE_SPECIFIC_OPTIONS[QUEUE_NAMES.CODE_ANALYSIS_QUEUE].defaultJobOptions,\n    73\t            removeOnComplete: queueConfig?.removeOnComplete || 25,\n    74\t            removeOnFail: queueConfig?.removeOnFail || 10,\n    75\t          },\n    76\t        };\n    77\t      },\n    78\t    }),\n    79\t    \n    80\t    // Batch Processing Queue\n    81\t    BullModule.registerQueueAsync({\n    82\t      name: QUEUE_NAMES.BATCH_PROCESSING_QUEUE,\n    83\t      inject: [ConfigService],\n    84\t      useFactory: (configService: ConfigService) =&gt; {\n    85\t        const config = configService.get&lt;ParallelProcessingConfig&gt;('parallelProcessing');\n    86\t        const queueConfig = config?.queues?.batch;\n    87\t        \n    88\t        return {\n    89\t          ...DEFAULT_QUEUE_OPTIONS,\n    90\t          ...QUEUE_SPECIFIC_OPTIONS[QUEUE_NAMES.BATCH_PROCESSING_QUEUE],\n    91\t          defaultJobOptions: {\n    92\t            ...DEFAULT_QUEUE_OPTIONS.defaultJobOptions,\n    93\t            ...QUEUE_SPECIFIC_OPTIONS[QUEUE_NAMES.BATCH_PROCESSING_QUEUE].defaultJobOptions,\n    94\t            removeOnComplete: queueConfig?.removeOnComplete || 10,\n    95\t            removeOnFail: queueConfig?.removeOnFail || 5,\n    96\t          },\n    97\t        };\n    98\t      },\n    99\t    }),\n...\nPath: src/modules/sync/processors/batch.processor.ts\n     1\timport { Processor, WorkerHost, OnQueueActive, OnQueueCompleted, OnQueueFailed } from '@nestjs/bullmq';\n     2\timport { Logger } from '@nestjs/common';\n     3\timport { Job } from 'bullmq';\n     4\timport { ConfigService } from '@nestjs/config';\n     5\timport { PrismaService } from '../../../shared/database/prisma.service';\n     6\timport { BatchOperationsService } from '../../../shared/database/batch-operations.service';\n     7\timport { StorageService } from '../../../shared/storage/storage.service';\n     8\timport { DistributedLockService } from '../../../shared/locking/distributed-lock.service';\n     9\timport { \n    10\t  QUEUE_NAMES, \n    11\t  JOB_TYPES, \n    12\t  BatchProcessingJobData,\n    13\t} from '../../../shared/queues/queue.constants';\n    14\timport { ParallelProcessingConfig } from '../../../config/parallel-processing.config';\n...\n    25\t\n    26\t/**\n    27\t * Batch Processor - Handles batch database and storage operations\n    28\t * Follows codebase-indexer's batch processing pattern for performance\n    29\t */\n    30\t@Processor(QUEUE_NAMES.BATCH_PROCESSING_QUEUE)\n    31\texport class BatchProcessor extends WorkerHost {\n    32\t  private readonly logger = new Logger(BatchProcessor.name);\n    33\t  private readonly config: ParallelProcessingConfig;\n    34\t\n    35\t  constructor(\n    36\t    private configService: ConfigService,\n    37\t    private prisma: PrismaService,\n    38\t    private batchOperations: BatchOperationsService,\n    39\t    private storageService: StorageService,\n    40\t    private distributedLockService: DistributedLockService,\n    41\t  ) {\n    42\t    super();\n    43\t    this.config = this.configService.get&lt;ParallelProcessingConfig&gt;('parallelProcessing');\n    44\t  }\n...\nPath: src/modules/sync/processors/file-processing.processor.ts\n     1\timport { Processor, WorkerHost, OnQueueActive, OnQueueCompleted, OnQueueFailed } from '@nestjs/bullmq';\n     2\timport { Logger } from '@nestjs/common';\n     3\timport { Job } from 'bullmq';\n     4\timport { ConfigService } from '@nestjs/config';\n     5\timport { PrismaService } from '../../../shared/database/prisma.service';\n     6\timport { BatchOperationsService } from '../../../shared/database/batch-operations.service';\n     7\timport { StorageService } from '../../../shared/storage/storage.service';\n     8\timport { WorkerPoolService } from '../../../shared/workers/worker-pool.service';\n     9\timport { DistributedLockService } from '../../../shared/locking/distributed-lock.service';\n    10\timport { GitlabService } from '../../gitlab/gitlab.service';\n    11\timport {\n    12\t  QUEUE_NAMES,\n    13\t  JOB_TYPES,\n    14\t  FileProcessingJobData,\n    15\t} from '../../../shared/queues/queue.constants';\n    16\timport { ParallelProcessingConfig } from '../../../config/parallel-processing.config';\n    17\t\n    18\texport interface FileProcessingResult {\n    19\t  filePath: string;\n    20\t  success: boolean;\n    21\t  size: number;\n    22\t  storageKey?: string;\n    23\t  language?: string;\n    24\t  error?: string;\n    25\t  processingTime: number;\n    26\t}\n    27\t\n    28\t/**\n    29\t * File Processing Processor - Handles parallel file processing using worker pools\n    30\t * Follows codebase-indexer's parallel file processing pattern\n    31\t */\n    32\t@Processor(QUEUE_NAMES.FILE_PROCESSING_QUEUE)\n    33\texport class FileProcessingProcessor extends WorkerHost {\n    34\t  private readonly logger = new Logger(FileProcessingProcessor.name);\n    35\t  private readonly config: ParallelProcessingConfig;\n...\n    67\t\n    68\t  /**\n    69\t   * Process file batch using worker pools (codebase-indexer approach)\n    70\t   */\n    71\t  @Processor(JOB_TYPES.PROCESS_FILE_BATCH)\n    72\t  async processFileBatch(job: Job&lt;FileProcessingJobData&gt;): Promise&lt;FileProcessingResult[]&gt; {\n    73\t    const { codebaseId, files, batchIndex, totalBatches, syncJobId, options } = job.data;\n    74\t    \n    75\t    this.logger.log(`Processing file batch ${batchIndex + 1}/${totalBatches} with ${files.length} files for codebase ${codebaseId}`);\n    76\t\n    77\t    // Acquire distributed lock for this codebase\n    78\t    const lockResource = `file-processing:${codebaseId}`;\n    79\t    const lockResult = await this.distributedLockService.acquireLock(lockResource, {\n    80\t      ttl: 300000, // 5 minutes\n    81\t      maxRetries: 3,\n    82\t    });\n...\nPath: src/modules/codegraph/codegraph.processor.ts\n     1\timport { Processor, WorkerHost, OnQueueActive, OnQueueCompleted, OnQueueFailed } from '@nestjs/bullmq';\n     2\timport { Logger } from '@nestjs/common';\n     3\timport { Job } from 'bullmq';\n     4\timport { PrismaService } from '@/shared/database/prisma.service';\n     5\timport { StorageService } from '@/shared/storage/storage.service';\n     6\timport { WebSocketService } from '../websocket/websocket.service';\n     7\timport { DistributedLockService } from '../../shared/locking/distributed-lock.service';\n     8\timport { EnhancedQueueService } from '../../shared/queues/enhanced-queue.service';\n     9\timport { WorkerPoolService } from '../../shared/workers/worker-pool.service';\n    10\timport { ScipService } from './services/scip.service';\n    11\timport { TreeSitterService } from './services/tree-sitter.service';\n    12\timport { EmbeddingService } from './services/embedding.service';\n    13\timport { BadgerDbService } from './services/badger-db.service';\n    14\timport { CodegraphConfigService } from './services/codegraph-config.service';\n    15\timport { IndexTaskType, IndexTaskStatus } from '@prisma/client';\n    16\timport { CodegraphJobData } from './codegraph.service';\n    17\timport { QUEUE_NAMES, JOB_TYPES } from '../../shared/queues/queue.constants';\n    18\timport { ParallelProcessingConfig } from '../../config/parallel-processing.config';\n    19\timport { ConfigService } from '@nestjs/config';\n    20\t\n    21\t@Processor(QUEUE_NAMES.CODEGRAPH_QUEUE)\n    22\texport class CodegraphProcessor extends WorkerHost {\n    23\t  private readonly logger = new Logger(CodegraphProcessor.name);\n    24\t  private readonly config: ParallelProcessingConfig;\n...\n    58\t\n    59\t  /**\n    60\t   * Main entry point - Parallel processing following codebase-indexer pattern\n    61\t   */\n    62\t  @Processor(JOB_TYPES.INDEX_CODEBASE)\n    63\t  async handleIndexCodebase(job: Job&lt;CodegraphJobData&gt;): Promise&lt;void&gt; {\n    64\t    const { codebaseId, taskType, syncJobId, userId, options } = job.data;\n    65\t    const jobId = job.id as string;\n    66\t\n    67\t    this.logger.log(`Starting parallel processing for codebase: ${codebaseId}, type: ${taskType}`);\n    68\t\n    69\t    // Acquire distributed lock for this codebase\n    70\t    const lockResource = `codegraph-processing:${codebaseId}`;\n    71\t    const lockResult = await this.distributedLockService.acquireLock(lockResource, {\n    72\t      ttl: 1800000, // 30 minutes\n    73\t      maxRetries: 3,\n    74\t    });\n    75\t\n    76\t    if (!lockResult.acquired) {\n    77\t      throw new Error(`Failed to acquire lock for codebase ${codebaseId}: ${lockResult.error}`);\n    78\t    }\n...\nPath: src/modules/codegraph/codegraph.module.ts\n...\n    24\t\n    25\t@Module({\n    26\t  imports: [\n    27\t    DatabaseModule,\n    28\t    StorageModule,\n    29\t    WorkerPoolModule,\n    30\t    DistributedLockModule,\n    31\t    EnhancedQueueModule,\n    32\t    BullModule.registerQueue({\n    33\t      name: QUEUE_NAMES.CODEGRAPH_QUEUE,\n    34\t      defaultJobOptions: {\n    35\t        removeOnComplete: 50,\n    36\t        removeOnFail: 20,\n    37\t        attempts: 3,\n    38\t        backoff: {\n    39\t          type: 'exponential',\n    40\t          delay: 2000,\n    41\t        },\n    42\t      },\n    43\t      settings: {\n    44\t        stalledInterval: 30 * 1000, // 30 seconds\n    45\t        maxStalledCount: 1,\n    46\t      },\n    47\t    }),\n    48\t  ],\n    49\t  controllers: [CodegraphController],\n    50\t  providers: [\n    51\t    CodegraphService,\n    52\t    CodegraphProcessor,\n    53\t    ScipService,\n    54\t    TreeSitterService,\n    55\t    BadgerDbService,\n    56\t    EmbeddingService,\n    57\t    CodegraphConfigService,\n    58\t    ScipToolsService,\n    59\t    CodegraphMetricsService,\n    60\t    SearchService,\n    61\t    FileManagementService,\n    62\t    // New parallel processing services\n    63\t    EnhancedTextSplitterService,\n    64\t    SemanticChunkerService,\n    65\t    TokenChunkerService,\n    66\t  ],\n...\nPath: src/modules/sync/processors/cleanup.processor.ts\n     1\timport { Processor, WorkerHost, OnQueueActive, OnQueueCompleted, OnQueueFailed } from '@nestjs/bullmq';\n     2\timport { Logger } from '@nestjs/common';\n     3\timport { Job } from 'bullmq';\n     4\timport { ConfigService } from '@nestjs/config';\n     5\timport { PrismaService } from '../../../shared/database/prisma.service';\n     6\timport { StorageService } from '../../../shared/storage/storage.service';\n     7\timport { DistributedLockService } from '../../../shared/locking/distributed-lock.service';\n     8\timport { \n     9\t  QUEUE_NAMES, \n    10\t  JOB_TYPES, \n    11\t  CleanupJobData,\n    12\t} from '../../../shared/queues/queue.constants';\n    13\timport { ParallelProcessingConfig } from '../../../config/parallel-processing.config';\n...\n    23\t\n    24\t/**\n    25\t * Cleanup Processor - Handles cleanup of temporary files, expired locks, and old jobs\n    26\t * Follows codebase-indexer's cleanup pattern for maintenance\n    27\t */\n    28\t@Processor(QUEUE_NAMES.CLEANUP_QUEUE)\n    29\texport class CleanupProcessor extends WorkerHost {\n    30\t  private readonly logger = new Logger(CleanupProcessor.name);\n    31\t  private readonly config: ParallelProcessingConfig;\n    32\t\n    33\t  constructor(\n    34\t    private configService: ConfigService,\n    35\t    private prisma: PrismaService,\n    36\t    private storageService: StorageService,\n    37\t    private distributedLockService: DistributedLockService,\n    38\t  ) {\n    39\t    super();\n    40\t    this.config = this.configService.get&lt;ParallelProcessingConfig&gt;('parallelProcessing');\n    41\t  }\n    42\t\n    43\t  @OnQueueActive()\n    44\t  onActive(job: Job) {\n    45\t    this.logger.debug(`Processing cleanup job ${job.id}: ${job.data.operationType}`);\n    46\t  }\n...\nPath: src/modules/sync/sync.processor.ts\n     1\timport { Processor, WorkerHost, OnQueueActive, OnQueueCompleted, OnQueueFailed } from '@nestjs/bullmq';\n     2\timport { Logger } from '@nestjs/common';\n     3\timport { Job } from 'bullmq';\n     4\timport { ConfigService } from '@nestjs/config';\n     5\timport { PrismaService } from '@/shared/database/prisma.service';\n     6\timport { GitlabService } from '../gitlab/gitlab.service';\n     7\timport { FileScannerService } from '../file/file-scanner.service';\n     8\timport { FileService } from '../file/file.service';\n     9\timport { StorageService } from '@/shared/storage/storage.service';\n    10\timport { WebSocketService } from '../websocket/websocket.service';\n    11\timport { CodegraphService } from '../codegraph/codegraph.service';\n    12\timport { DistributedLockService } from '../../shared/locking/distributed-lock.service';\n...\nPath: src/modules/sync/processors/code-analysis.processor.ts\n     1\timport { Processor, WorkerHost, OnQueueActive, OnQueueCompleted, OnQueueFailed } from '@nestjs/bullmq';\n     2\timport { Logger } from '@nestjs/common';\n     3\timport { Job } from 'bullmq';\n     4\timport { ConfigService } from '@nestjs/config';\n     5\timport { PrismaService } from '../../../shared/database/prisma.service';\n     6\timport { StorageService } from '../../../shared/storage/storage.service';\n     7\timport { WorkerPoolService } from '../../../shared/workers/worker-pool.service';\n     8\timport { DistributedLockService } from '../../../shared/locking/distributed-lock.service';\n     9\timport { GitlabService } from '../../gitlab/gitlab.service';\n    10\timport { ScipService } from '../../codegraph/services/scip.service';\n    11\timport { TreeSitterService } from '../../codegraph/services/tree-sitter.service';\n    12\timport {\n    13\t  QUEUE_NAMES,\n    14\t  JOB_TYPES,\n    15\t  CodeAnalysisJobData,\n    16\t} from '../../../shared/queues/queue.constants';\n    17\timport { ParallelProcessingConfig } from '../../../config/parallel-processing.config';\n    18\t\n    19\texport interface CodeAnalysisResult {\n    20\t  filePath: string;\n    21\t  success: boolean;\n    22\t  symbols?: any[];\n    23\t  relationships?: any[];\n    24\t  scipData?: any;\n    25\t  treeSitterData?: any;\n    26\t  error?: string;\n    27\t  processingTime: number;\n    28\t}\n    29\t\n    30\t/**\n    31\t * Code Analysis Processor - Handles SCIP and Tree-sitter analysis\n    32\t * Follows codebase-indexer's parallel code analysis pattern\n    33\t */\n    34\t@Processor(QUEUE_NAMES.CODE_ANALYSIS_QUEUE)\n    35\texport class CodeAnalysisProcessor extends WorkerHost {\n    36\t  private readonly logger = new Logger(CodeAnalysisProcessor.name);\n    37\t  private readonly config: ParallelProcessingConfig;\n...\nPath: src/modules/sync/processors/progress-tracking.processor.ts\n...\n    31\t\n    32\t/**\n    33\t * Progress Tracking Processor - Handles real-time progress updates and WebSocket notifications\n    34\t * Follows codebase-indexer's progress tracking pattern\n    35\t */\n    36\t@Processor(QUEUE_NAMES.PROGRESS_TRACKING_QUEUE)\n    37\texport class ProgressTrackingProcessor extends WorkerHost {\n    38\t  private readonly logger = new Logger(ProgressTrackingProcessor.name);\n    39\t  private readonly config: ParallelProcessingConfig;\n    40\t\n    41\t  constructor(\n    42\t    private configService: ConfigService,\n    43\t    private prisma: PrismaService,\n    44\t    private webSocketService: WebSocketService,\n    45\t  ) {\n    46\t    super();\n    47\t    this.config = this.configService.get&lt;ParallelProcessingConfig&gt;('parallelProcessing');\n    48\t  }\n    49\t\n    50\t  @OnQueueActive()\n    51\t  onActive(job: Job) {\n    52\t    this.logger.debug(`Processing progress tracking job ${job.id}`);\n    53\t  }\n...\nPath: src/modules/sync/processors/embedding.processor.ts\n     1\timport { Processor, WorkerHost, OnQueueActive, OnQueueCompleted, OnQueueFailed } from '@nestjs/bullmq';\n     2\timport { Logger } from '@nestjs/common';\n     3\timport { Job } from 'bullmq';\n     4\timport { ConfigService } from '@nestjs/config';\n     5\timport { PrismaService } from '../../../shared/database/prisma.service';\n     6\timport { StorageService } from '../../../shared/storage/storage.service';\n     7\timport { EnhancedTextSplitterService } from '../../codegraph/services/enhanced-text-splitter.service';\n     8\timport { EmbeddingService } from '../../codegraph/services/embedding.service';\n     9\timport { \n    10\t  QUEUE_NAMES, \n    11\t  JOB_TYPES, \n    12\t  EmbeddingJobData,\n    13\t} from '../../../shared/queues/queue.constants';\n    14\timport { ParallelProcessingConfig } from '../../../config/parallel-processing.config';\n...\n&quot;,&quot;is_error&quot;:false,&quot;request_id&quot;:&quot;&quot;,&quot;tool_use_id&quot;:&quot;toolu_vrtx_01KaNnahfvLdZem7o5azhszr&quot;}},{&quot;id&quot;:2,&quot;type&quot;:4,&quot;ide_state_node&quot;:{&quot;workspaceFolders&quot;:[{&quot;repositoryRoot&quot;:&quot;/Users/deepak/TekCode/TekAIContextEngine2&quot;,&quot;folderRoot&quot;:&quot;/Users/deepak/TekCode/TekAIContextEngine2&quot;}],&quot;workspaceFoldersUnchanged&quot;:false}}],&quot;timestamp&quot;:&quot;2025-07-12T23:16:03.180Z&quot;,&quot;response_text&quot;:&quot;Perfect! Now I can see the issue clearly. Looking at the codebase, I can see that there's an inconsistency in the decorator usage. The issue is that `@Processor` is being used incorrectly for method-level job handlers. \n\nIn NestJS BullMQ:\n- `@Processor(queueName)` is used at the **class level** to define which queue the processor handles\n- `@Process(jobName)` is used at the **method level** to handle specific job types within that queue\n\nHowever, I can see that in this codebase, they're incorrectly using `@Processor(JOB_TYPES.SYNC_CODEBASE)` at the method level, which is causing the TypeScript error.\n\nLet me check if `@Process` is available in the imports:\n&quot;,&quot;workspace_file_chunks&quot;:[]},{&quot;status&quot;:&quot;success&quot;,&quot;request_id&quot;:&quot;14a3c2d7-2181-42eb-9612-e74826159077&quot;,&quot;request_message&quot;:&quot;&quot;,&quot;structured_output_nodes&quot;:[{&quot;id&quot;:1,&quot;type&quot;:5,&quot;content&quot;:&quot;&quot;,&quot;tool_use&quot;:{&quot;tool_use_id&quot;:&quot;toolu_vrtx_011FeDH2Vv1T6oiXUrvtL1hg&quot;,&quot;tool_name&quot;:&quot;str-replace-editor&quot;,&quot;input_json&quot;:&quot;{\&quot;command\&quot;: \&quot;str_replace\&quot;, \&quot;path\&quot;: \&quot;src/modules/sync/sync.processor.ts\&quot;, \&quot;instruction_reminder\&quot;: \&quot;ALWAYS BREAK DOWN EDITS INTO SMALLER CHUNKS OF AT MOST 150 LINES EACH.\&quot;, \&quot;old_str_1\&quot;: \&quot;import {Processor, WorkerHost} from '@nestjs/bullmq';\&quot;, \&quot;new_str_1\&quot;: \&quot;import {Processor, WorkerHost, Process} from '@nestjs/bullmq';\&quot;, \&quot;old_str_start_line_number_1\&quot;: 1, \&quot;old_str_end_line_number_1\&quot;: 1}&quot;,&quot;mcp_server_name&quot;:&quot;&quot;,&quot;mcp_tool_name&quot;:&quot;&quot;}},{&quot;id&quot;:2,&quot;type&quot;:0,&quot;content&quot;:&quot;Now I understand the issue! The problem is that the code is missing the `@Process` decorator import and is incorrectly using `@Processor` for method-level job handlers.\n\n## Plan\n\nI need to fix the TypeScript decorator error by:\n\n1. **Import the `@Process` decorator** from `@nestjs/bullmq`\n2. **Replace `@Processor(JOB_TYPES.SYNC_CODEBASE)` with `@Process(JOB_TYPES.SYNC_CODEBASE)`** on the method\n3. **Check and fix similar issues** in other processor files if they exist\n\nLet me implement the fix:\n&quot;},{&quot;id&quot;:3,&quot;type&quot;:2,&quot;content&quot;:&quot;&quot;},{&quot;id&quot;:4,&quot;type&quot;:3,&quot;content&quot;:&quot;&quot;}],&quot;seen_state&quot;:&quot;seen&quot;,&quot;structured_request_nodes&quot;:[{&quot;id&quot;:1,&quot;type&quot;:1,&quot;tool_result_node&quot;:{&quot;content&quot;:&quot;Here's the result of running `cat -n` on src/modules/sync/sync.processor.ts:\n     1\timport {Processor, WorkerHost} from '@nestjs/bullmq';\n     2\timport { Logger } from '@nestjs/common';\n     3\timport { Job } from 'bullmq';\n     4\timport { ConfigService } from '@nestjs/config';\n     5\timport { PrismaService } from '@/shared/database/prisma.service';\nTotal lines in file: 549\n&quot;,&quot;is_error&quot;:false,&quot;request_id&quot;:&quot;&quot;,&quot;tool_use_id&quot;:&quot;toolu_vrtx_01TjXWPK8vHGzKU9dDgQYMnf&quot;}},{&quot;id&quot;:2,&quot;type&quot;:4,&quot;ide_state_node&quot;:{&quot;workspaceFolders&quot;:[{&quot;repositoryRoot&quot;:&quot;/Users/deepak/TekCode/TekAIContextEngine2&quot;,&quot;folderRoot&quot;:&quot;/Users/deepak/TekCode/TekAIContextEngine2&quot;}],&quot;workspaceFoldersUnchanged&quot;:false}}],&quot;timestamp&quot;:&quot;2025-07-12T23:16:11.485Z&quot;,&quot;response_text&quot;:&quot;Now I understand the issue! The problem is that the code is missing the `@Process` decorator import and is incorrectly using `@Processor` for method-level job handlers.\n\n## Plan\n\nI need to fix the TypeScript decorator error by:\n\n1. **Import the `@Process` decorator** from `@nestjs/bullmq`\n2. **Replace `@Processor(JOB_TYPES.SYNC_CODEBASE)` with `@Process(JOB_TYPES.SYNC_CODEBASE)`** on the method\n3. **Check and fix similar issues** in other processor files if they exist\n\nLet me implement the fix:\n&quot;,&quot;workspace_file_chunks&quot;:[]},{&quot;status&quot;:&quot;success&quot;,&quot;request_id&quot;:&quot;49e83517-cad9-456d-a044-1f300cb0833d&quot;,&quot;request_message&quot;:&quot;&quot;,&quot;structured_output_nodes&quot;:[{&quot;id&quot;:1,&quot;type&quot;:5,&quot;content&quot;:&quot;&quot;,&quot;tool_use&quot;:{&quot;tool_use_id&quot;:&quot;toolu_vrtx_013TUwwHEiSnpthFoPmZKDQF&quot;,&quot;tool_name&quot;:&quot;str-replace-editor&quot;,&quot;input_json&quot;:&quot;{\&quot;command\&quot;: \&quot;str_replace\&quot;, \&quot;path\&quot;: \&quot;src/modules/sync/sync.processor.ts\&quot;, \&quot;instruction_reminder\&quot;: \&quot;ALWAYS BREAK DOWN EDITS INTO SMALLER CHUNKS OF AT MOST 150 LINES EACH.\&quot;, \&quot;old_str_1\&quot;: \&quot;  /**\\n   * Main orchestrator - Coordinates parallel pipelines (codebase-indexer approach)\\n   */\\n  @Processor(JOB_TYPES.SYNC_CODEBASE)\\n  async handleSyncCodebase(job: Job&lt;SyncJobData&gt;): Promise&lt;void&gt; {\&quot;, \&quot;new_str_1\&quot;: \&quot;  /**\\n   * Main orchestrator - Coordinates parallel pipelines (codebase-indexer approach)\\n   */\\n  @Process(JOB_TYPES.SYNC_CODEBASE)\\n  async handleSyncCodebase(job: Job&lt;SyncJobData&gt;): Promise&lt;void&gt; {\&quot;, \&quot;old_str_start_line_number_1\&quot;: 57, \&quot;old_str_end_line_number_1\&quot;: 61}&quot;,&quot;mcp_server_name&quot;:&quot;&quot;,&quot;mcp_tool_name&quot;:&quot;&quot;}},{&quot;id&quot;:2,&quot;type&quot;:0,&quot;content&quot;:&quot;&quot;},{&quot;id&quot;:3,&quot;type&quot;:2,&quot;content&quot;:&quot;&quot;},{&quot;id&quot;:4,&quot;type&quot;:3,&quot;content&quot;:&quot;&quot;}],&quot;seen_state&quot;:&quot;seen&quot;,&quot;structured_request_nodes&quot;:[{&quot;id&quot;:1,&quot;type&quot;:1,&quot;tool_result_node&quot;:{&quot;content&quot;:&quot;Successfully edited the file src/modules/sync/sync.processor.ts.\nResult for str_replace for entry with index [1]:\nReplacement successful.\nnew_str starts at line 1 and ends at line 1.\n\nReview the changes and make sure they are as expected (correct indentation, no duplicate lines, etc).\nEdit the file again if necessary.\n&quot;,&quot;is_error&quot;:false,&quot;request_id&quot;:&quot;&quot;,&quot;tool_use_id&quot;:&quot;toolu_vrtx_011FeDH2Vv1T6oiXUrvtL1hg&quot;}},{&quot;id&quot;:2,&quot;type&quot;:4,&quot;ide_state_node&quot;:{&quot;workspaceFolders&quot;:[{&quot;repositoryRoot&quot;:&quot;/Users/deepak/TekCode/TekAIContextEngine2&quot;,&quot;folderRoot&quot;:&quot;/Users/deepak/TekCode/TekAIContextEngine2&quot;}],&quot;workspaceFoldersUnchanged&quot;:false}}],&quot;timestamp&quot;:&quot;2025-07-12T23:16:21.286Z&quot;,&quot;response_text&quot;:&quot;&quot;,&quot;workspace_file_chunks&quot;:[]},{&quot;status&quot;:&quot;success&quot;,&quot;request_id&quot;:&quot;44abde20-0715-4fd9-ae78-f6f7b9ec08de&quot;,&quot;request_message&quot;:&quot;&quot;,&quot;structured_output_nodes&quot;:[{&quot;id&quot;:1,&quot;type&quot;:5,&quot;content&quot;:&quot;&quot;,&quot;tool_use&quot;:{&quot;tool_use_id&quot;:&quot;toolu_vrtx_01VbtRfw6RjomDyE9iRoj8Qf&quot;,&quot;tool_name&quot;:&quot;view&quot;,&quot;input_json&quot;:&quot;{\&quot;path\&quot;: \&quot;src/modules/sync/processors/file-processing.processor.ts\&quot;, \&quot;type\&quot;: \&quot;file\&quot;, \&quot;view_range\&quot;: [1, 5]}&quot;,&quot;mcp_server_name&quot;:&quot;&quot;,&quot;mcp_tool_name&quot;:&quot;&quot;}},{&quot;id&quot;:2,&quot;type&quot;:0,&quot;content&quot;:&quot;Now let me check if there are similar issues in other processor files and fix them:\n&quot;},{&quot;id&quot;:3,&quot;type&quot;:2,&quot;content&quot;:&quot;&quot;},{&quot;id&quot;:4,&quot;type&quot;:3,&quot;content&quot;:&quot;&quot;}],&quot;seen_state&quot;:&quot;seen&quot;,&quot;structured_request_nodes&quot;:[{&quot;id&quot;:1,&quot;type&quot;:1,&quot;tool_result_node&quot;:{&quot;content&quot;:&quot;Successfully edited the file src/modules/sync/sync.processor.ts.\nResult for str_replace for entry with index [1]:\nReplacement successful.\nnew_str starts at line 57 and ends at line 61.\n\nReview the changes and make sure they are as expected (correct indentation, no duplicate lines, etc).\nEdit the file again if necessary.\n&quot;,&quot;is_error&quot;:false,&quot;request_id&quot;:&quot;&quot;,&quot;tool_use_id&quot;:&quot;toolu_vrtx_013TUwwHEiSnpthFoPmZKDQF&quot;}},{&quot;id&quot;:2,&quot;type&quot;:4,&quot;ide_state_node&quot;:{&quot;workspaceFolders&quot;:[{&quot;repositoryRoot&quot;:&quot;/Users/deepak/TekCode/TekAIContextEngine2&quot;,&quot;folderRoot&quot;:&quot;/Users/deepak/TekCode/TekAIContextEngine2&quot;}],&quot;workspaceFoldersUnchanged&quot;:false}}],&quot;timestamp&quot;:&quot;2025-07-12T23:16:29.572Z&quot;,&quot;response_text&quot;:&quot;Now let me check if there are similar issues in other processor files and fix them:\n&quot;,&quot;workspace_file_chunks&quot;:[]},{&quot;status&quot;:&quot;success&quot;,&quot;request_id&quot;:&quot;57be58ea-d4d1-4c94-877c-3e0e66dc6e00&quot;,&quot;request_message&quot;:&quot;&quot;,&quot;structured_output_nodes&quot;:[{&quot;id&quot;:1,&quot;type&quot;:5,&quot;content&quot;:&quot;&quot;,&quot;tool_use&quot;:{&quot;tool_use_id&quot;:&quot;toolu_vrtx_019mftPzLeKzCbp4BTUSrCAK&quot;,&quot;tool_name&quot;:&quot;view&quot;,&quot;input_json&quot;:&quot;{\&quot;path\&quot;: \&quot;src/modules/sync/processors/file-processing.processor.ts\&quot;, \&quot;type\&quot;: \&quot;file\&quot;, \&quot;view_range\&quot;: [68, 75]}&quot;,&quot;mcp_server_name&quot;:&quot;&quot;,&quot;mcp_tool_name&quot;:&quot;&quot;}},{&quot;id&quot;:2,&quot;type&quot;:0,&quot;content&quot;:&quot;&quot;},{&quot;id&quot;:3,&quot;type&quot;:2,&quot;content&quot;:&quot;&quot;},{&quot;id&quot;:4,&quot;type&quot;:3,&quot;content&quot;:&quot;&quot;}],&quot;seen_state&quot;:&quot;seen&quot;,&quot;structured_request_nodes&quot;:[{&quot;id&quot;:1,&quot;type&quot;:1,&quot;tool_result_node&quot;:{&quot;content&quot;:&quot;Here's the result of running `cat -n` on src/modules/sync/processors/file-processing.processor.ts:\n     1\timport { Processor, WorkerHost, OnQueueActive, OnQueueCompleted, OnQueueFailed } from '@nestjs/bullmq';\n     2\timport { Logger } from '@nestjs/common';\n     3\timport { Job } from 'bullmq';\n     4\timport { ConfigService } from '@nestjs/config';\n     5\timport { PrismaService } from '../../../shared/database/prisma.service';\nTotal lines in file: 489\n&quot;,&quot;is_error&quot;:false,&quot;request_id&quot;:&quot;&quot;,&quot;tool_use_id&quot;:&quot;toolu_vrtx_01VbtRfw6RjomDyE9iRoj8Qf&quot;}},{&quot;id&quot;:2,&quot;type&quot;:4,&quot;ide_state_node&quot;:{&quot;workspaceFolders&quot;:[{&quot;repositoryRoot&quot;:&quot;/Users/deepak/TekCode/TekAIContextEngine2&quot;,&quot;folderRoot&quot;:&quot;/Users/deepak/TekCode/TekAIContextEngine2&quot;}],&quot;workspaceFoldersUnchanged&quot;:false}}],&quot;timestamp&quot;:&quot;2025-07-12T23:16:34.424Z&quot;,&quot;response_text&quot;:&quot;&quot;,&quot;workspace_file_chunks&quot;:[]},{&quot;status&quot;:&quot;cancelled&quot;,&quot;request_id&quot;:&quot;924ce1ed-1143-4d7a-a1cb-777419a8219f&quot;,&quot;request_message&quot;:&quot;&quot;,&quot;structured_output_nodes&quot;:[],&quot;seen_state&quot;:&quot;seen&quot;,&quot;structured_request_nodes&quot;:[{&quot;id&quot;:1,&quot;type&quot;:1,&quot;tool_result_node&quot;:{&quot;content&quot;:&quot;Here's the result of running `cat -n` on src/modules/sync/processors/file-processing.processor.ts:\n    68\t  /**\n    69\t   * Process file batch using worker pools (codebase-indexer approach)\n    70\t   */\n    71\t  @Processor(JOB_TYPES.PROCESS_FILE_BATCH)\n    72\t  async processFileBatch(job: Job&lt;FileProcessingJobData&gt;): Promise&lt;FileProcessingResult[]&gt; {\n    73\t    const { codebaseId, files, batchIndex, totalBatches, syncJobId, options } = job.data;\n    74\t    \n    75\t    this.logger.log(`Processing file batch ${batchIndex + 1}/${totalBatches} with ${files.length} files for codebase ${codebaseId}`);\nTotal lines in file: 489\n&quot;,&quot;is_error&quot;:false,&quot;request_id&quot;:&quot;&quot;,&quot;tool_use_id&quot;:&quot;toolu_vrtx_019mftPzLeKzCbp4BTUSrCAK&quot;}},{&quot;id&quot;:2,&quot;type&quot;:4,&quot;ide_state_node&quot;:{&quot;workspaceFolders&quot;:[{&quot;repositoryRoot&quot;:&quot;/Users/deepak/TekCode/TekAIContextEngine2&quot;,&quot;folderRoot&quot;:&quot;/Users/deepak/TekCode/TekAIContextEngine2&quot;}],&quot;workspaceFoldersUnchanged&quot;:false}}],&quot;timestamp&quot;:&quot;2025-07-12T23:16:38.599Z&quot;,&quot;response_text&quot;:&quot;I need to fix this file too:&quot;,&quot;workspace_file_chunks&quot;:[]},{&quot;request_id&quot;:&quot;2eb57feb-0e9d-408e-9316-c149f95c0012&quot;,&quot;status&quot;:&quot;cancelled&quot;,&quot;chatItemType&quot;:&quot;agentic-turn-delimiter&quot;},{&quot;request_id&quot;:&quot;96b02cb9-df5a-44a1-81cf-961784bf34ed&quot;,&quot;uuid&quot;:&quot;4580b361-2fe0-4951-84cf-5cac52774fca&quot;,&quot;chatItemType&quot;:&quot;agentic-checkpoint-delimiter&quot;,&quot;status&quot;:&quot;success&quot;,&quot;fromTimestamp&quot;:1752362079028,&quot;toTimestamp&quot;:1752362202660}],&quot;feedbackStates&quot;:{&quot;temp-fe-83fc7db0-8263-4fb3-8a5e-aa3316b39c9d&quot;:{&quot;selectedRating&quot;:0,&quot;feedbackNote&quot;:&quot;&quot;},&quot;temp-fe-bafe31b1-b937-404d-a76b-8fb167e47f5f&quot;:{&quot;selectedRating&quot;:0,&quot;feedbackNote&quot;:&quot;&quot;},&quot;temp-fe-2f8bd877-cd51-4c38-84d8-420f0f881e0c&quot;:{&quot;selectedRating&quot;:0,&quot;feedbackNote&quot;:&quot;&quot;},&quot;temp-fe-9a13bdca-2317-4d4d-ba8c-1c87643bdda9&quot;:{&quot;selectedRating&quot;:0,&quot;feedbackNote&quot;:&quot;&quot;},&quot;temp-fe-6f399227-22ce-4cc5-a022-d46e9022777a&quot;:{&quot;selectedRating&quot;:0,&quot;feedbackNote&quot;:&quot;&quot;},&quot;temp-fe-8a3ffa29-2076-403d-b7d8-0699ecabf92b&quot;:{&quot;selectedRating&quot;:0,&quot;feedbackNote&quot;:&quot;&quot;},&quot;temp-fe-fce2904c-4353-48b3-8299-1b9f1f628cdf&quot;:{&quot;selectedRating&quot;:0,&quot;feedbackNote&quot;:&quot;&quot;},&quot;temp-fe-73024b84-c1c8-471b-ac13-a796159a1280&quot;:{&quot;selectedRating&quot;:0,&quot;feedbackNote&quot;:&quot;&quot;},&quot;temp-fe-b910335a-b1c1-4467-9924-8d9c8d1ed9eb&quot;:{&quot;selectedRating&quot;:0,&quot;feedbackNote&quot;:&quot;&quot;},&quot;temp-fe-c8615629-a164-494d-a2a8-d6cc9f4980b6&quot;:{&quot;selectedRating&quot;:0,&quot;feedbackNote&quot;:&quot;&quot;},&quot;temp-fe-1be32c4e-a257-4fdd-bd68-21e90f639925&quot;:{&quot;selectedRating&quot;:0,&quot;feedbackNote&quot;:&quot;&quot;},&quot;temp-fe-2de5ec6e-7ad8-49f5-8d01-2893fd588d13&quot;:{&quot;selectedRating&quot;:0,&quot;feedbackNote&quot;:&quot;&quot;}},&quot;toolUseStates&quot;:{&quot;6d784a56-468e-4621-9c88-fe70417ee7eb;toolu_vrtx_015Fnvf9T7CXENq536TagyuT&quot;:{&quot;requestId&quot;:&quot;6d784a56-468e-4621-9c88-fe70417ee7eb&quot;,&quot;toolUseId&quot;:&quot;toolu_vrtx_015Fnvf9T7CXENq536TagyuT&quot;,&quot;phase&quot;:5,&quot;result&quot;:{&quot;text&quot;:&quot;Here's the result of running `cat -n` on src/modules/sync/sync.processor.ts:\n     1\timport {Processor, WorkerHost} from '@nestjs/bullmq';\n     2\timport { Logger } from '@nestjs/common';\n     3\timport { Job } from 'bullmq';\n     4\timport { ConfigService } from '@nestjs/config';\n     5\timport { PrismaService } from '@/shared/database/prisma.service';\n     6\timport { GitlabService } from '../gitlab/gitlab.service';\n     7\timport { FileScannerService } from '../file/file-scanner.service';\n     8\timport { FileService } from '../file/file.service';\n     9\timport { StorageService } from '@/shared/storage/storage.service';\n    10\timport { WebSocketService } from '../websocket/websocket.service';\n    11\timport { CodegraphService } from '../codegraph/codegraph.service';\n    12\timport { DistributedLockService } from '../../shared/locking/distributed-lock.service';\n    13\timport { EnhancedQueueService } from '../../shared/queues/enhanced-queue.service';\n    14\timport { WorkerPoolService } from '../../shared/workers/worker-pool.service';\n    15\timport { SyncJobType, SyncJobStatus, CodebaseStatus, IndexTaskType } from '@prisma/client';\n    16\timport { SyncJobData } from './sync.service';\n    17\timport { QUEUE_NAMES, JOB_TYPES } from '../../shared/queues/queue.constants';\n    18\timport { ParallelProcessingConfig } from '../../config/parallel-processing.config';\n    19\t\n    20\t@Processor(QUEUE_NAMES.SYNC_QUEUE)\n    21\texport class SyncProcessor extends WorkerHost {\n    22\t  private readonly logger = new Logger(SyncProcessor.name);\n    23\t  private readonly config: ParallelProcessingConfig;\n    24\t\n    25\t  constructor(\n    26\t    private configService: ConfigService,\n    27\t    private prisma: PrismaService,\n    28\t    private gitlabService: GitlabService,\n    29\t    private fileScannerService: FileScannerService,\n    30\t    private fileService: FileService,\n    31\t    private storageService: StorageService,\n    32\t    private webSocketService: WebSocketService,\n    33\t    private codegraphService: CodegraphService,\n    34\t    private distributedLockService: DistributedLockService,\n    35\t    private enhancedQueueService: EnhancedQueueService,\n    36\t    private workerPoolService: WorkerPoolService,\n    37\t  ) {\n    38\t    super();\n    39\t    this.config = this.configService.get&lt;ParallelProcessingConfig&gt;('parallelProcessing');\n    40\t  }\n    41\t\n    42\t\n    43\t  async onActive(job: Job) {\n    44\t    this.logger.debug(`Processing sync job ${job.id} for codebase ${job.data.codebaseId}`);\n    45\t  }\n    46\t\n    47\t\n    48\t  async onCompleted(job: Job) {\n    49\t    this.logger.debug(`Completed sync job ${job.id} for codebase ${job.data.codebaseId}`);\n    50\t  }\n    51\t\n    52\t\n    53\t  async onFailed(job: Job, err: Error) {\n    54\t    this.logger.error(`Failed sync job ${job.id} for codebase ${job.data.codebaseId}:`, err);\n    55\t  }\n    56\t\n    57\t  /**\n    58\t   * Main orchestrator - Coordinates parallel pipelines (codebase-indexer approach)\n    59\t   */\n    60\t  @Processor(JOB_TYPES.SYNC_CODEBASE)\n    61\t  async handleSyncCodebase(job: Job&lt;SyncJobData&gt;): Promise&lt;void&gt; {\n    62\t    const { codebaseId, type, metadata } = job.data;\n    63\t    const syncJobId = metadata?.syncJobId;\n    64\t\n    65\t    this.logger.log(`Starting parallel sync orchestration for codebase: ${codebaseId}`);\n    66\t\n    67\t    // Acquire distributed lock for this codebase\n    68\t    const lockResource = `sync-orchestration:${codebaseId}`;\n    69\t    const lockResult = await this.distributedLockService.acquireLock(lockResource, {\n    70\t      ttl: 3600000, // 1 hour\n    71\t      maxRetries: 3,\n    72\t    });\n    73\t\n    74\t    if (!lockResult.acquired) {\n    75\t      throw new Error(`Failed to acquire sync lock for codebase ${codebaseId}: ${lockResult.error}`);\n    76\t    }\n    77\t\n    78\t    try {\n    79\t      // Update job status to running\n    80\t      await this.updateSyncJob(syncJobId, {\n    81\t        status: SyncJobStatus.RUNNING,\n    82\t        startedAt: new Date(),\n    83\t        progress: 0,\n    84\t      });\n    85\t\n    86\t      // Emit WebSocket update\n    87\t      await this.emitSyncUpdate(syncJobId, codebaseId, {\n    88\t        status: 'RUNNING',\n    89\t        progress: 0,\n    90\t        message: 'Starting parallel sync orchestration',\n    91\t      });\n    92\t\n    93\t      // Execute parallel sync orchestration\n    94\t      await this.orchestrateParallelSync(codebaseId, type, syncJobId, metadata);\n    95\t\n    96\t      // Mark sync as completed\n    97\t      await this.updateSyncJob(syncJobId, {\n    98\t        status: SyncJobStatus.COMPLETED,\n    99\t        completedAt: new Date(),\n   100\t        progress: 100,\n   101\t      });\n   102\t\n   103\t      await this.emitSyncUpdate(syncJobId, codebaseId, {\n   104\t        status: 'COMPLETED',\n   105\t        progress: 100,\n   106\t        message: 'Parallel sync orchestration completed',\n   107\t      });\n   108\t\n   109\t      this.logger.log(`Completed parallel sync orchestration for codebase: ${codebaseId}`);\n   110\t    } finally {\n   111\t      // Release distributed lock\n   112\t      await this.distributedLockService.releaseLock(lockResource, lockResult.lockId!);\n   113\t    }\n   114\t  }\n   115\t\n   116\t  /**\n   117\t   * Orchestrate parallel sync pipelines (following codebase-indexer pattern)\n   118\t   */\n   119\t  private async orchestrateParallelSync(\n   120\t    codebaseId: string,\n   121\t    type: SyncJobType,\n   122\t    syncJobId: string,\n   123\t    metadata: any,\n   124\t  ): Promise&lt;void&gt; {\n   125\t    this.logger.log(`Orchestrating parallel sync for codebase: ${codebaseId}, type: ${type}`);\n   126\t\n   127\t    try {\n   128\t      // Phase 1: Repository Sync &amp; File Discovery\n   129\t      await this.updateProgress(codebaseId, syncJobId, 'repository-sync', 10, 'Starting repository sync and file discovery');\n   130\t      await this.executeRepositorySync(codebaseId, syncJobId, metadata);\n   131\t\n   132\t      // Phase 2: Get discovered files\n   133\t      await this.updateProgress(codebaseId, syncJobId, 'file-discovery', 20, 'Discovering files in repository');\n   134\t      const files = await this.getDiscoveredFiles(codebaseId);\n   135\t\n   136\t      if (files.length === 0) {\n   137\t        this.logger.warn(`No files discovered for codebase ${codebaseId}`);\n   138\t        await this.updateProgress(codebaseId, syncJobId, 'completed', 100, 'No files found to process');\n   139\t        return;\n   140\t      }\n   141\t\n   142\t      // Phase 3: Execute parallel processing pipelines (codebase-indexer approach)\n   143\t      await this.updateProgress(codebaseId, syncJobId, 'parallel-processing', 30, `Processing ${files.length} files in parallel pipelines`);\n   144\t      await this.executeParallelProcessingPipelines(codebaseId, files, syncJobId, type);\n   145\t\n   146\t      // Phase 4: Finalize and update codebase status\n   147\t      await this.updateProgress(codebaseId, syncJobId, 'finalizing', 90, 'Finalizing sync process');\n   148\t      await this.finalizeSyncProcess(codebaseId, syncJobId);\n   149\t\n   150\t      await this.updateProgress(codebaseId, syncJobId, 'completed', 100, 'Sync completed successfully');\n   151\t    } catch (error) {\n   152\t      this.logger.error(`Error in parallel sync orchestration for codebase ${codebaseId}:`, error);\n   153\t      await this.updateProgress(codebaseId, syncJobId, 'failed', 0, `Sync failed: ${error.message}`);\n   154\t      throw error;\n   155\t    }\n   156\t  }\n   157\t\n   158\t  /**\n   159\t   * Execute repository sync and file discovery\n   160\t   */\n   161\t  private async executeRepositorySync(\n   162\t    codebaseId: string,\n   163\t    syncJobId: string,\n   164\t    metadata: any,\n   165\t  ): Promise&lt;void&gt; {\n   166\t    this.logger.log(`Executing repository sync for codebase: ${codebaseId}`);\n   167\t\n   168\t    try {\n   169\t      // Update progress\n   170\t      await this.updateSyncProgress(syncJobId, codebaseId, 10, 'Syncing repository...');\n   171\t\n   172\t      // Get codebase information\n   173\t      const codebase = await this.prisma.codebase.findUnique({\n   174\t        where: { id: codebaseId },\n   175\t      });\n   176\t\n   177\t      if (!codebase) {\n   178\t        throw new Error(`Codebase not found: ${codebaseId}`);\n   179\t      }\n   180\t\n   181\t      // Sync repository files\n   182\t      const syncResult = await this.gitlabService.syncRepository(\n   183\t        codebase.gitlabProjectId,\n   184\t        codebase.branch || 'main',\n   185\t      );\n   186\t\n   187\t      // Update progress\n   188\t      await this.updateSyncProgress(syncJobId, codebaseId, 30, 'Discovering files...');\n   189\t\n   190\t      // Scan and discover files\n   191\t      const discoveredFiles = await this.fileScannerService.scanRepository(\n   192\t        syncResult.repositoryPath,\n   193\t        {\n   194\t          includePatterns: metadata?.includePatterns,\n   195\t          excludePatterns: metadata?.excludePatterns,\n   196\t          maxFileSize: metadata?.maxFileSize,\n   197\t        },\n   198\t      );\n   199\t\n   200\t      // Store discovered files\n   201\t      await this.fileService.storeDiscoveredFiles(codebaseId, discoveredFiles);\n   202\t\n   203\t      // Update progress\n   204\t      await this.updateSyncProgress(syncJobId, codebaseId, 50, `Discovered ${discoveredFiles.length} files`);\n   205\t\n   206\t      this.logger.log(`Repository sync completed for codebase: ${codebaseId}, files: ${discoveredFiles.length}`);\n   207\t    } catch (error) {\n   208\t      this.logger.error(`Repository sync failed for codebase ${codebaseId}:`, error);\n   209\t      throw error;\n   210\t    }\n   211\t  }\n   212\t\n   213\t  /**\n   214\t   * Execute parallel processing pipelines (codebase-indexer approach)\n   215\t   */\n   216\t  private async executeParallelProcessingPipelines(\n   217\t    codebaseId: string,\n   218\t    files: any[],\n   219\t    syncJobId: string,\n   220\t    type: SyncJobType,\n   221\t  ): Promise&lt;void&gt; {\n   222\t    this.logger.log(`Executing parallel processing pipelines for ${files.length} files`);\n   223\t\n   224\t    try {\n   225\t      // Update progress\n   226\t      await this.updateSyncProgress(syncJobId, codebaseId, 60, 'Starting parallel processing...');\n   227\t\n   228\t      // Calculate batch configuration\n   229\t      const batchSize = this.config?.performance?.maxFilesPerBatch || 100;\n   230\t      const totalBatches = Math.ceil(files.length / batchSize);\n   231\t\n   232\t      // Parallel execution (like codebase-indexer's Go routines)\n   233\t      const pipelinePromises: Promise&lt;any&gt;[] = [];\n   234\t\n   235\t      // Pipeline 1: File Processing + Embedding Generation\n   236\t      if (type === SyncJobType.FULL || type === SyncJobType.EMBEDDING) {\n   237\t        pipelinePromises.push(\n   238\t          this.executeFileProcessingPipeline(codebaseId, files, totalBatches, syncJobId)\n   239\t        );\n   240\t      }\n   241\t\n   242\t      // Pipeline 2: Code Analysis (SCIP + Tree-sitter)\n   243\t      if (type === SyncJobType.FULL || type === SyncJobType.ANALYSIS) {\n   244\t        pipelinePromises.push(\n   245\t          this.executeCodeAnalysisPipeline(codebaseId, files, totalBatches, syncJobId)\n   246\t        );\n   247\t      }\n   248\t\n   249\t      // Wait for all pipelines to complete\n   250\t      const results = await Promise.allSettled(pipelinePromises);\n   251\t\n   252\t      // Check for failures\n   253\t      const failures = results.filter(result =&gt; result.status === 'rejected');\n   254\t      if (failures.length &gt; 0) {\n   255\t        this.logger.error(`${failures.length} pipelines failed:`, failures);\n   256\t        // Don't throw - allow partial success\n   257\t      }\n   258\t\n   259\t      // Update progress\n   260\t      await this.updateSyncProgress(syncJobId, codebaseId, 90, 'Parallel processing completed');\n   261\t\n   262\t      this.logger.log(`Parallel processing pipelines completed for codebase: ${codebaseId}`);\n   263\t    } catch (error) {\n   264\t      this.logger.error(`Parallel processing failed for codebase ${codebaseId}:`, error);\n   265\t      throw error;\n   266\t    }\n   267\t  }\n   268\t\n   269\t  /**\n   270\t   * Execute file processing pipeline\n   271\t   */\n   272\t  private async executeFileProcessingPipeline(\n   273\t    codebaseId: string,\n   274\t    files: any[],\n   275\t    totalBatches: number,\n   276\t    syncJobId: string,\n   277\t  ): Promise&lt;void&gt; {\n   278\t    this.logger.log(`Starting file processing pipeline for ${files.length} files`);\n   279\t\n   280\t    const batchSize = this.config?.performance?.maxFilesPerBatch || 100;\n   281\t    const fileBatches = this.createBatches(files, batchSize);\n   282\t\n   283\t    // Submit file processing jobs\n   284\t    const fileProcessingPromises = fileBatches.map((batch, index) =&gt;\n   285\t      this.enhancedQueueService.submitFileProcessingJob({\n   286\t        codebaseId,\n   287\t        files: batch.map(f =&gt; ({\n   288\t          path: f.path,\n   289\t          hash: f.hash || '',\n   290\t          size: f.size || 0,\n   291\t          language: f.language,\n   292\t          storageKey: f.storageKey,\n   293\t        })),\n   294\t        batchIndex: index,\n   295\t        totalBatches,\n   296\t        syncJobId,\n   297\t        options: {\n   298\t          forceReprocess: false,\n   299\t          validateContent: true,\n   300\t          extractMetadata: true,\n   301\t        },\n   302\t      })\n   303\t    );\n   304\t\n   305\t    // Wait for file processing to complete\n   306\t    await Promise.all(fileProcessingPromises);\n   307\t\n   308\t    // Submit embedding generation jobs\n   309\t    const embeddingPromises = fileBatches.map((batch, index) =&gt;\n   310\t      this.enhancedQueueService.submitEmbeddingJob({\n   311\t        codebaseId,\n   312\t        files: batch.map(f =&gt; ({\n   313\t          path: f.path,\n   314\t          language: f.language || 'text',\n   315\t          storageKey: f.storageKey || f.path,\n   316\t          size: f.size || 0,\n   317\t        })),\n   318\t        batchIndex: index,\n   319\t        totalBatches,\n   320\t        syncJobId,\n   321\t        options: {\n   322\t          chunkingStrategy: 'hybrid',\n   323\t          batchSize: this.config?.embeddingConfig?.batchSize || 20,\n   324\t          forceRegenerate: false,\n   325\t        },\n   326\t      })\n   327\t    );\n   328\t\n   329\t    // Wait for embedding generation to complete\n   330\t    await Promise.all(embeddingPromises);\n   331\t\n   332\t    this.logger.log(`File processing pipeline completed for ${files.length} files`);\n   333\t  }\n   334\t\n   335\t  /**\n   336\t   * Execute code analysis pipeline\n   337\t   */\n   338\t  private async executeCodeAnalysisPipeline(\n   339\t    codebaseId: string,\n   340\t    files: any[],\n   341\t    totalBatches: number,\n   342\t    syncJobId: string,\n   343\t  ): Promise&lt;void&gt; {\n   344\t    this.logger.log(`Starting code analysis pipeline for ${files.length} files`);\n   345\t\n   346\t    const batchSize = this.config?.performance?.maxFilesPerBatch || 100;\n   347\t    const fileBatches = this.createBatches(files, batchSize);\n   348\t\n   349\t    const codeAnalysisPromises = fileBatches.map((batch, index) =&gt;\n   350\t      this.enhancedQueueService.submitCodeAnalysisJob({\n   351\t        codebaseId,\n   352\t        files: batch.map(f =&gt; ({\n   353\t          path: f.path,\n   354\t          language: f.language || 'text',\n   355\t          storageKey: f.storageKey || f.path,\n   356\t        })),\n   357\t        analysisType: 'both', // SCIP + Tree-sitter\n   358\t        batchIndex: index,\n   359\t        totalBatches,\n   360\t        syncJobId,\n   361\t        options: {\n   362\t          extractSymbols: true,\n   363\t          buildRelationships: true,\n   364\t          generateIndex: true,\n   365\t        },\n   366\t      })\n   367\t    );\n   368\t\n   369\t    // Wait for code analysis to complete\n   370\t    await Promise.all(codeAnalysisPromises);\n   371\t\n   372\t    this.logger.log(`Code analysis pipeline completed for ${files.length} files`);\n   373\t  }\n   374\t\n   375\t  /**\n   376\t   * Finalize sync process\n   377\t   */\n   378\t  private async finalizeSyncProcess(codebaseId: string, syncJobId: string): Promise&lt;void&gt; {\n   379\t    this.logger.log(`Finalizing sync process for codebase: ${codebaseId}`);\n   380\t\n   381\t    try {\n   382\t      // Update codebase status\n   383\t      await this.prisma.codebase.update({\n   384\t        where: { id: codebaseId },\n   385\t        data: {\n   386\t          status: CodebaseStatus.ACTIVE,\n   387\t          lastSyncAt: new Date(),\n   388\t          updatedAt: new Date(),\n   389\t        },\n   390\t      });\n   391\t\n   392\t      // Update progress\n   393\t      await this.updateSyncProgress(syncJobId, codebaseId, 100, 'Sync completed successfully');\n   394\t\n   395\t      this.logger.log(`Sync process finalized for codebase: ${codebaseId}`);\n   396\t    } catch (error) {\n   397\t      this.logger.error(`Failed to finalize sync process for codebase ${codebaseId}:`, error);\n   398\t      throw error;\n   399\t    }\n   400\t  }\n   401\t\n   402\t  /**\n   403\t   * Get discovered files for codebase\n   404\t   */\n   405\t  private async getDiscoveredFiles(codebaseId: string): Promise&lt;any[]&gt; {\n   406\t    // TODO: Implement based on your actual file schema\n   407\t    // For now, return mock data\n   408\t    return [\n   409\t      {\n   410\t        path: 'src/main.ts',\n   411\t        hash: 'abc123',\n   412\t        size: 1024,\n   413\t        language: 'typescript',\n   414\t        storageKey: `${codebaseId}/src/main.ts`,\n   415\t      },\n   416\t      {\n   417\t        path: 'src/app.service.ts',\n   418\t        hash: 'def456',\n   419\t        size: 2048,\n   420\t        language: 'typescript',\n   421\t        storageKey: `${codebaseId}/src/app.service.ts`,\n   422\t      },\n   423\t    ];\n   424\t  }\n   425\t\n   426\t  /**\n   427\t   * Create batches from array\n   428\t   */\n   429\t  private createBatches&lt;T&gt;(items: T[], batchSize: number): T[][] {\n   430\t    const batches: T[][] = [];\n   431\t    for (let i = 0; i &lt; items.length; i += batchSize) {\n   432\t      batches.push(items.slice(i, i + batchSize));\n   433\t    }\n   434\t    return batches;\n   435\t  }\n   436\t\n   437\t  /**\n   438\t   * Update sync progress\n   439\t   */\n   440\t  private async updateSyncProgress(\n   441\t    syncJobId: string,\n   442\t    codebaseId: string,\n   443\t    progress: number,\n   444\t    message: string,\n   445\t  ): Promise&lt;void&gt; {\n   446\t    try {\n   447\t      await this.updateSyncJob(syncJobId, { progress });\n   448\t      await this.emitSyncUpdate(syncJobId, codebaseId, {\n   449\t        status: 'RUNNING',\n   450\t        progress,\n   451\t        message,\n   452\t      });\n   453\t    } catch (error) {\n   454\t      this.logger.warn(`Failed to update sync progress for ${syncJobId}:`, error);\n   455\t    }\n   456\t  }\n   457\t\n   458\t  /**\n   459\t   * Update progress and emit WebSocket updates\n   460\t   */\n   461\t  private async updateProgress(\n   462\t    codebaseId: string,\n   463\t    syncJobId: string,\n   464\t    stage: string,\n   465\t    progress: number,\n   466\t    message: string,\n   467\t    details?: any,\n   468\t  ): Promise&lt;void&gt; {\n   469\t    try {\n   470\t      // Submit progress update to queue for processing\n   471\t      await this.enhancedQueueService.submitProgressTrackingJob({\n   472\t        codebaseId,\n   473\t        syncJobId,\n   474\t        stage,\n   475\t        progress,\n   476\t        message,\n   477\t        details,\n   478\t      });\n   479\t    } catch (error) {\n   480\t      this.logger.error(`Failed to update progress for codebase ${codebaseId}:`, error);\n   481\t      // Don't throw - progress updates shouldn't break the main flow\n   482\t    }\n   483\t  }\n   484\t\n   485\t  /**\n   486\t   * Update sync job in database\n   487\t   */\n   488\t  private async updateSyncJob(\n   489\t    syncJobId: string,\n   490\t    updates: {\n   491\t      status?: SyncJobStatus;\n   492\t      progress?: number;\n   493\t      message?: string;\n   494\t      error?: string;\n   495\t      startedAt?: Date;\n   496\t      completedAt?: Date;\n   497\t    },\n   498\t  ): Promise&lt;void&gt; {\n   499\t    if (!syncJobId) return;\n   500\t\n   501\t    try {\n   502\t      // TODO: Update based on your actual schema\n   503\t      // For now, just log the update\n   504\t      this.logger.debug(`Sync job ${syncJobId} update:`, updates);\n   505\t\n   506\t      // If you have a syncJob table, uncomment this:\n   507\t      // await this.prisma.syncJob.update({\n   508\t      //   where: { id: syncJobId },\n   509\t      //   data: {\n   510\t      //     ...updates,\n   511\t      //     updatedAt: new Date(),\n   512\t      //   },\n   513\t      // });\n   514\t    } catch (error) {\n   515\t      this.logger.warn(`Failed to update sync job ${syncJobId}:`, error);\n   516\t    }\n   517\t  }\n   518\t\n   519\t  /**\n   520\t   * Emit sync status update via WebSocket\n   521\t   */\n   522\t  private async emitSyncUpdate(\n   523\t    syncJobId: string,\n   524\t    codebaseId: string,\n   525\t    update: {\n   526\t      status: string;\n   527\t      progress: number;\n   528\t      message?: string;\n   529\t      error?: string;\n   530\t    },\n   531\t  ): Promise&lt;void&gt; {\n   532\t    try {\n   533\t      this.webSocketService.emitToRoom(`codebase:${codebaseId}`, 'syncUpdate', {\n   534\t        syncJobId,\n   535\t        codebaseId,\n   536\t        ...update,\n   537\t        timestamp: new Date(),\n   538\t      });\n   539\t    } catch (error) {\n   540\t      this.logger.warn('Failed to emit sync update:', error);\n   541\t    }\n   542\t  }\n   543\t\n   544\t\n   545\t\n   546\t\n   547\t\n   548\t}\n   549\t\nTotal lines in file: 549\n&quot;,&quot;isError&quot;:false,&quot;requestId&quot;:&quot;&quot;}},&quot;11b9d3c7-18ce-4442-8c40-e1f391dd95ed;toolu_vrtx_01BqrGeCruHmJDsRArX6jTLi&quot;:{&quot;requestId&quot;:&quot;11b9d3c7-18ce-4442-8c40-e1f391dd95ed&quot;,&quot;toolUseId&quot;:&quot;toolu_vrtx_01BqrGeCruHmJDsRArX6jTLi&quot;,&quot;phase&quot;:5,&quot;result&quot;:{&quot;text&quot;:&quot;The following code sections were retrieved:\nPath: src/modules/sync/processors/batch.processor.ts\n     1\timport { Processor, WorkerHost, OnQueueActive, OnQueueCompleted, OnQueueFailed } from '@nestjs/bullmq';\n     2\timport { Logger } from '@nestjs/common';\n     3\timport { Job } from 'bullmq';\n     4\timport { ConfigService } from '@nestjs/config';\n     5\timport { PrismaService } from '../../../shared/database/prisma.service';\n     6\timport { BatchOperationsService } from '../../../shared/database/batch-operations.service';\n     7\timport { StorageService } from '../../../shared/storage/storage.service';\n     8\timport { DistributedLockService } from '../../../shared/locking/distributed-lock.service';\n     9\timport { \n    10\t  QUEUE_NAMES, \n    11\t  JOB_TYPES, \n    12\t  BatchProcessingJobData,\n    13\t} from '../../../shared/queues/queue.constants';\n    14\timport { ParallelProcessingConfig } from '../../../config/parallel-processing.config';\n...\n    25\t\n    26\t/**\n    27\t * Batch Processor - Handles batch database and storage operations\n    28\t * Follows codebase-indexer's batch processing pattern for performance\n    29\t */\n    30\t@Processor(QUEUE_NAMES.BATCH_PROCESSING_QUEUE)\n    31\texport class BatchProcessor extends WorkerHost {\n    32\t  private readonly logger = new Logger(BatchProcessor.name);\n    33\t  private readonly config: ParallelProcessingConfig;\n    34\t\n    35\t  constructor(\n    36\t    private configService: ConfigService,\n    37\t    private prisma: PrismaService,\n    38\t    private batchOperations: BatchOperationsService,\n    39\t    private storageService: StorageService,\n    40\t    private distributedLockService: DistributedLockService,\n    41\t  ) {\n    42\t    super();\n    43\t    this.config = this.configService.get&lt;ParallelProcessingConfig&gt;('parallelProcessing');\n    44\t  }\n...\nPath: docs/DEVELOPMENT.md\n...\n   249\t\n   250\t3. **Implement service logic**:\n   251\t   ```typescript\n   252\t   async create(createDto: CreateItemDto) {\n   253\t     return this.prisma.item.create({\n   254\t       data: createDto,\n   255\t     });\n   256\t   }\n   257\t   ```\n   258\t\n   259\t4. **Add tests**:\n   260\t   ```typescript\n   261\t   describe('create', () =&gt; {\n   262\t     it('should create item', async () =&gt; {\n   263\t       // Test implementation\n   264\t     });\n   265\t   });\n   266\t   ```\n   267\t\n   268\t### 4. Background Jobs\n   269\t\n   270\t1. **Define job processor**:\n   271\t   ```typescript\n   272\t   @Process('job-name')\n   273\t   async handleJob(job: Job&lt;JobData&gt;) {\n   274\t     // Job implementation\n   275\t   }\n   276\t   ```\n   277\t\n   278\t2. **Queue job**:\n   279\t   ```typescript\n   280\t   await this.queue.add('job-name', jobData);\n   281\t   ```\n   282\t\n   283\t3. **Test job processing**:\n   284\t   ```typescript\n   285\t   // Test job execution\n   286\t   ```\n...\nPath: src/modules/codegraph/codegraph.processor.ts\n     1\timport { Processor, WorkerHost, OnQueueActive, OnQueueCompleted, OnQueueFailed } from '@nestjs/bullmq';\n     2\timport { Logger } from '@nestjs/common';\n     3\timport { Job } from 'bullmq';\n     4\timport { PrismaService } from '@/shared/database/prisma.service';\n     5\timport { StorageService } from '@/shared/storage/storage.service';\n     6\timport { WebSocketService } from '../websocket/websocket.service';\n     7\timport { DistributedLockService } from '../../shared/locking/distributed-lock.service';\n     8\timport { EnhancedQueueService } from '../../shared/queues/enhanced-queue.service';\n     9\timport { WorkerPoolService } from '../../shared/workers/worker-pool.service';\n    10\timport { ScipService } from './services/scip.service';\n    11\timport { TreeSitterService } from './services/tree-sitter.service';\n    12\timport { EmbeddingService } from './services/embedding.service';\n    13\timport { BadgerDbService } from './services/badger-db.service';\n    14\timport { CodegraphConfigService } from './services/codegraph-config.service';\n    15\timport { IndexTaskType, IndexTaskStatus } from '@prisma/client';\n    16\timport { CodegraphJobData } from './codegraph.service';\n    17\timport { QUEUE_NAMES, JOB_TYPES } from '../../shared/queues/queue.constants';\n    18\timport { ParallelProcessingConfig } from '../../config/parallel-processing.config';\n    19\timport { ConfigService } from '@nestjs/config';\n    20\t\n    21\t@Processor(QUEUE_NAMES.CODEGRAPH_QUEUE)\n    22\texport class CodegraphProcessor extends WorkerHost {\n    23\t  private readonly logger = new Logger(CodegraphProcessor.name);\n    24\t  private readonly config: ParallelProcessingConfig;\n...\nPath: src/shared/queues/enhanced-queue.module.ts\n...\n    11\t\n    12\t@Global()\n    13\t@Module({\n    14\t  imports: [\n    15\t    ConfigModule,\n    16\t    \n    17\t    // File Processing Queue\n    18\t    BullModule.registerQueueAsync({\n    19\t      name: QUEUE_NAMES.FILE_PROCESSING_QUEUE,\n    20\t      inject: [ConfigService],\n    21\t      useFactory: (configService: ConfigService) =&gt; {\n    22\t        const config = configService.get&lt;ParallelProcessingConfig&gt;('parallelProcessing');\n    23\t        const queueConfig = config?.queues?.fileProcessing;\n    24\t        \n    25\t        return {\n    26\t          ...DEFAULT_QUEUE_OPTIONS,\n    27\t          ...QUEUE_SPECIFIC_OPTIONS[QUEUE_NAMES.FILE_PROCESSING_QUEUE],\n    28\t          defaultJobOptions: {\n    29\t            ...DEFAULT_QUEUE_OPTIONS.defaultJobOptions,\n    30\t            ...QUEUE_SPECIFIC_OPTIONS[QUEUE_NAMES.FILE_PROCESSING_QUEUE].defaultJobOptions,\n    31\t            removeOnComplete: queueConfig?.removeOnComplete || 100,\n    32\t            removeOnFail: queueConfig?.removeOnFail || 50,\n    33\t          },\n    34\t        };\n    35\t      },\n    36\t    }),\n    37\t    \n    38\t    // Embedding Queue\n    39\t    BullModule.registerQueueAsync({\n    40\t      name: QUEUE_NAMES.EMBEDDING_QUEUE,\n    41\t      inject: [ConfigService],\n    42\t      useFactory: (configService: ConfigService) =&gt; {\n    43\t        const config = configService.get&lt;ParallelProcessingConfig&gt;('parallelProcessing');\n    44\t        const queueConfig = config?.queues?.embedding;\n    45\t        \n    46\t        return {\n    47\t          ...DEFAULT_QUEUE_OPTIONS,\n    48\t          ...QUEUE_SPECIFIC_OPTIONS[QUEUE_NAMES.EMBEDDING_QUEUE],\n    49\t          defaultJobOptions: {\n    50\t            ...DEFAULT_QUEUE_OPTIONS.defaultJobOptions,\n    51\t            ...QUEUE_SPECIFIC_OPTIONS[QUEUE_NAMES.EMBEDDING_QUEUE].defaultJobOptions,\n    52\t            removeOnComplete: queueConfig?.removeOnComplete || 50,\n    53\t            removeOnFail: queueConfig?.removeOnFail || 25,\n    54\t          },\n    55\t        };\n    56\t      },\n    57\t    }),\n...\n    79\t    \n    80\t    // Batch Processing Queue\n    81\t    BullModule.registerQueueAsync({\n    82\t      name: QUEUE_NAMES.BATCH_PROCESSING_QUEUE,\n    83\t      inject: [ConfigService],\n    84\t      useFactory: (configService: ConfigService) =&gt; {\n    85\t        const config = configService.get&lt;ParallelProcessingConfig&gt;('parallelProcessing');\n    86\t        const queueConfig = config?.queues?.batch;\n    87\t        \n    88\t        return {\n    89\t          ...DEFAULT_QUEUE_OPTIONS,\n    90\t          ...QUEUE_SPECIFIC_OPTIONS[QUEUE_NAMES.BATCH_PROCESSING_QUEUE],\n    91\t          defaultJobOptions: {\n    92\t            ...DEFAULT_QUEUE_OPTIONS.defaultJobOptions,\n    93\t            ...QUEUE_SPECIFIC_OPTIONS[QUEUE_NAMES.BATCH_PROCESSING_QUEUE].defaultJobOptions,\n    94\t            removeOnComplete: queueConfig?.removeOnComplete || 10,\n    95\t            removeOnFail: queueConfig?.removeOnFail || 5,\n    96\t          },\n    97\t        };\n    98\t      },\n    99\t    }),\n...\nPath: src/modules/sync/processors/file-processing.processor.ts\n     1\timport { Processor, WorkerHost, OnQueueActive, OnQueueCompleted, OnQueueFailed } from '@nestjs/bullmq';\n     2\timport { Logger } from '@nestjs/common';\n     3\timport { Job } from 'bullmq';\n     4\timport { ConfigService } from '@nestjs/config';\n     5\timport { PrismaService } from '../../../shared/database/prisma.service';\n     6\timport { BatchOperationsService } from '../../../shared/database/batch-operations.service';\n     7\timport { StorageService } from '../../../shared/storage/storage.service';\n     8\timport { WorkerPoolService } from '../../../shared/workers/worker-pool.service';\n     9\timport { DistributedLockService } from '../../../shared/locking/distributed-lock.service';\n    10\timport { GitlabService } from '../../gitlab/gitlab.service';\n    11\timport {\n    12\t  QUEUE_NAMES,\n    13\t  JOB_TYPES,\n    14\t  FileProcessingJobData,\n    15\t} from '../../../shared/queues/queue.constants';\n    16\timport { ParallelProcessingConfig } from '../../../config/parallel-processing.config';\n    17\t\n    18\texport interface FileProcessingResult {\n    19\t  filePath: string;\n    20\t  success: boolean;\n    21\t  size: number;\n    22\t  storageKey?: string;\n    23\t  language?: string;\n    24\t  error?: string;\n    25\t  processingTime: number;\n    26\t}\n    27\t\n    28\t/**\n    29\t * File Processing Processor - Handles parallel file processing using worker pools\n    30\t * Follows codebase-indexer's parallel file processing pattern\n    31\t */\n    32\t@Processor(QUEUE_NAMES.FILE_PROCESSING_QUEUE)\n    33\texport class FileProcessingProcessor extends WorkerHost {\n    34\t  private readonly logger = new Logger(FileProcessingProcessor.name);\n    35\t  private readonly config: ParallelProcessingConfig;\n...\n    67\t\n    68\t  /**\n    69\t   * Process file batch using worker pools (codebase-indexer approach)\n    70\t   */\n    71\t  @Processor(JOB_TYPES.PROCESS_FILE_BATCH)\n    72\t  async processFileBatch(job: Job&lt;FileProcessingJobData&gt;): Promise&lt;FileProcessingResult[]&gt; {\n    73\t    const { codebaseId, files, batchIndex, totalBatches, syncJobId, options } = job.data;\n    74\t    \n    75\t    this.logger.log(`Processing file batch ${batchIndex + 1}/${totalBatches} with ${files.length} files for codebase ${codebaseId}`);\n    76\t\n    77\t    // Acquire distributed lock for this codebase\n    78\t    const lockResource = `file-processing:${codebaseId}`;\n    79\t    const lockResult = await this.distributedLockService.acquireLock(lockResource, {\n    80\t      ttl: 300000, // 5 minutes\n    81\t      maxRetries: 3,\n    82\t    });\n...\nPath: src/modules/sync/processors/embedding.processor.ts\n     1\timport { Processor, WorkerHost, OnQueueActive, OnQueueCompleted, OnQueueFailed } from '@nestjs/bullmq';\n     2\timport { Logger } from '@nestjs/common';\n     3\timport { Job } from 'bullmq';\n     4\timport { ConfigService } from '@nestjs/config';\n     5\timport { PrismaService } from '../../../shared/database/prisma.service';\n     6\timport { StorageService } from '../../../shared/storage/storage.service';\n     7\timport { EnhancedTextSplitterService } from '../../codegraph/services/enhanced-text-splitter.service';\n     8\timport { EmbeddingService } from '../../codegraph/services/embedding.service';\n     9\timport { \n    10\t  QUEUE_NAMES, \n    11\t  JOB_TYPES, \n    12\t  EmbeddingJobData,\n    13\t} from '../../../shared/queues/queue.constants';\n    14\timport { ParallelProcessingConfig } from '../../../config/parallel-processing.config';\n...\nPath: src/modules/sync/processors/code-analysis.processor.ts\n     1\timport { Processor, WorkerHost, OnQueueActive, OnQueueCompleted, OnQueueFailed } from '@nestjs/bullmq';\n     2\timport { Logger } from '@nestjs/common';\n     3\timport { Job } from 'bullmq';\n     4\timport { ConfigService } from '@nestjs/config';\n     5\timport { PrismaService } from '../../../shared/database/prisma.service';\n     6\timport { StorageService } from '../../../shared/storage/storage.service';\n     7\timport { WorkerPoolService } from '../../../shared/workers/worker-pool.service';\n     8\timport { DistributedLockService } from '../../../shared/locking/distributed-lock.service';\n     9\timport { GitlabService } from '../../gitlab/gitlab.service';\n    10\timport { ScipService } from '../../codegraph/services/scip.service';\n    11\timport { TreeSitterService } from '../../codegraph/services/tree-sitter.service';\n    12\timport {\n    13\t  QUEUE_NAMES,\n    14\t  JOB_TYPES,\n    15\t  CodeAnalysisJobData,\n    16\t} from '../../../shared/queues/queue.constants';\n    17\timport { ParallelProcessingConfig } from '../../../config/parallel-processing.config';\n    18\t\n    19\texport interface CodeAnalysisResult {\n    20\t  filePath: string;\n    21\t  success: boolean;\n    22\t  symbols?: any[];\n    23\t  relationships?: any[];\n    24\t  scipData?: any;\n    25\t  treeSitterData?: any;\n    26\t  error?: string;\n    27\t  processingTime: number;\n    28\t}\n    29\t\n    30\t/**\n    31\t * Code Analysis Processor - Handles SCIP and Tree-sitter analysis\n    32\t * Follows codebase-indexer's parallel code analysis pattern\n    33\t */\n    34\t@Processor(QUEUE_NAMES.CODE_ANALYSIS_QUEUE)\n    35\texport class CodeAnalysisProcessor extends WorkerHost {\n    36\t  private readonly logger = new Logger(CodeAnalysisProcessor.name);\n    37\t  private readonly config: ParallelProcessingConfig;\n    38\t\n    39\t  constructor(\n    40\t    private configService: ConfigService,\n    41\t    private prisma: PrismaService,\n    42\t    private storageService: StorageService,\n    43\t    private workerPoolService: WorkerPoolService,\n    44\t    private distributedLockService: DistributedLockService,\n    45\t    private gitlabService: GitlabService,\n    46\t    private scipService: ScipService,\n    47\t    private treeSitterService: TreeSitterService,\n    48\t  ) {\n    49\t    super();\n    50\t    this.config = this.configService.get&lt;ParallelProcessingConfig&gt;('parallelProcessing');\n    51\t  }\n    52\t\n    53\t  @OnQueueActive()\n    54\t  onActive(job: Job) {\n    55\t    this.logger.debug(`Processing code analysis job ${job.id} for codebase ${job.data.codebaseId}`);\n    56\t  }\n    57\t\n    58\t  @OnQueueCompleted()\n    59\t  onCompleted(job: Job) {\n    60\t    this.logger.debug(`Completed code analysis job ${job.id} for codebase ${job.data.codebaseId}`);\n    61\t  }\n...\nPath: src/modules/sync/processors/progress-tracking.processor.ts\n...\n    31\t\n    32\t/**\n    33\t * Progress Tracking Processor - Handles real-time progress updates and WebSocket notifications\n    34\t * Follows codebase-indexer's progress tracking pattern\n    35\t */\n    36\t@Processor(QUEUE_NAMES.PROGRESS_TRACKING_QUEUE)\n    37\texport class ProgressTrackingProcessor extends WorkerHost {\n    38\t  private readonly logger = new Logger(ProgressTrackingProcessor.name);\n    39\t  private readonly config: ParallelProcessingConfig;\n    40\t\n    41\t  constructor(\n    42\t    private configService: ConfigService,\n    43\t    private prisma: PrismaService,\n    44\t    private webSocketService: WebSocketService,\n    45\t  ) {\n    46\t    super();\n    47\t    this.config = this.configService.get&lt;ParallelProcessingConfig&gt;('parallelProcessing');\n    48\t  }\n    49\t\n    50\t  @OnQueueActive()\n    51\t  onActive(job: Job) {\n    52\t    this.logger.debug(`Processing progress tracking job ${job.id}`);\n    53\t  }\n...\nPath: src/shared/queues/queue.constants.ts\n     1\t/**\n     2\t * Queue Names - Following codebase-indexer parallel processing architecture\n     3\t */\n     4\texport const QUEUE_NAMES = {\n     5\t  // Main orchestration queue\n     6\t  SYNC_QUEUE: 'sync',\n     7\t  CODEGRAPH_QUEUE: 'codegraph',\n     8\t  \n     9\t  // Parallel processing queues (new)\n    10\t  FILE_PROCESSING_QUEUE: 'file-processing',\n    11\t  EMBEDDING_QUEUE: 'embedding',\n    12\t  CODE_ANALYSIS_QUEUE: 'code-analysis',\n    13\t  BATCH_PROCESSING_QUEUE: 'batch-processing',\n    14\t  \n    15\t  // Specialized queues\n    16\t  PROGRESS_TRACKING_QUEUE: 'progress-tracking',\n    17\t  CLEANUP_QUEUE: 'cleanup',\n    18\t} as const;\n    19\t\n    20\t/**\n    21\t * Job Types for each queue\n    22\t */\n    23\texport const JOB_TYPES = {\n    24\t  // Sync Queue Jobs\n    25\t  SYNC_CODEBASE: 'sync-codebase',\n    26\t  \n    27\t  // Codegraph Queue Jobs\n    28\t  INDEX_CODEBASE: 'index-codebase',\n    29\t  \n    30\t  // File Processing Queue Jobs\n    31\t  PROCESS_FILE_BATCH: 'process-file-batch',\n    32\t  DOWNLOAD_FILE: 'download-file',\n    33\t  STORE_FILE_CONTENT: 'store-file-content',\n    34\t  VALIDATE_FILE: 'validate-file',\n    35\t  \n    36\t  // Embedding Queue Jobs\n    37\t  GENERATE_EMBEDDINGS_BATCH: 'generate-embeddings-batch',\n    38\t  PROCESS_CHUNKS: 'process-chunks',\n    39\t  STORE_EMBEDDINGS: 'store-embeddings',\n    40\t  \n    41\t  // Code Analysis Queue Jobs\n    42\t  ANALYZE_CODE_STRUCTURE: 'analyze-code-structure',\n    43\t  EXTRACT_SYMBOLS: 'extract-symbols',\n    44\t  BUILD_RELATIONSHIPS: 'build-relationships',\n    45\t  GENERATE_SCIP: 'generate-scip',\n    46\t  \n    47\t  // Batch Processing Queue Jobs\n    48\t  BATCH_DATABASE_OPERATIONS: 'batch-database-operations',\n    49\t  BATCH_STORAGE_OPERATIONS: 'batch-storage-operations',\n    50\t  BATCH_API_CALLS: 'batch-api-calls',\n    51\t  \n    52\t  // Progress Tracking Queue Jobs\n    53\t  UPDATE_PROGRESS: 'update-progress',\n    54\t  EMIT_WEBSOCKET_UPDATE: 'emit-websocket-update',\n    55\t  \n    56\t  // Cleanup Queue Jobs\n    57\t  CLEANUP_TEMP_FILES: 'cleanup-temp-files',\n    58\t  CLEANUP_EXPIRED_LOCKS: 'cleanup-expired-locks',\n    59\t  CLEANUP_OLD_JOBS: 'cleanup-old-jobs',\n    60\t} as const;\n    61\t\n    62\t/**\n    63\t * Queue Priorities\n    64\t */\n    65\texport const QUEUE_PRIORITIES = {\n    66\t  CRITICAL: 10,\n    67\t  HIGH: 7,\n    68\t  MEDIUM: 5,\n    69\t  LOW: 3,\n    70\t  BACKGROUND: 1,\n    71\t} as const;\n    72\t\n    73\t/**\n    74\t * Job Data Interfaces\n    75\t */\n...\n   116\t\n   117\t// Code Analysis Job Data\n   118\texport interface CodeAnalysisJobData {\n   119\t  codebaseId: string;\n   120\t  files: Array&lt;{\n   121\t    path: string;\n   122\t    language: string;\n   123\t    storageKey: string;\n   124\t  }&gt;;\n   125\t  analysisType: 'scip' | 'tree-sitter' | 'both';\n   126\t  batchIndex: number;\n   127\t  totalBatches: number;\n   128\t  syncJobId?: string;\n   129\t  options?: {\n   130\t    extractSymbols?: boolean;\n   131\t    buildRelationships?: boolean;\n   132\t    generateIndex?: boolean;\n   133\t  };\n   134\t}\n   135\t\n   136\t// Batch Processing Job Data\n   137\texport interface BatchProcessingJobData {\n   138\t  codebaseId: string;\n   139\t  operationType: string;\n   140\t  data: any[];\n   141\t  options?: {\n   142\t    batchSize?: number;\n   143\t    onConflict?: 'update' | 'skip' | 'error';\n   144\t    skipDuplicates?: boolean;\n   145\t    chunkDelay?: number;\n   146\t  };\n   147\t}\n...\nPath: src/config/redis.config.ts\n...\n     4\t\n     5\t@Injectable()\n     6\texport class RedisConfigService implements SharedBullConfigurationFactory {\n     7\t  constructor(private configService: ConfigService) {}\n     8\t\n     9\t  createSharedConfiguration(): BullRootModuleOptions {\n    10\t    return {\n    11\t      redis: {\n    12\t        host: this.configService.get&lt;string&gt;('QUEUE_REDIS_HOST', 'localhost'),\n    13\t        port: this.configService.get&lt;number&gt;('QUEUE_REDIS_PORT', 6379),\n    14\t        password: this.configService.get&lt;string&gt;('QUEUE_REDIS_PASSWORD'),\n    15\t        db: this.configService.get&lt;number&gt;('QUEUE_REDIS_DB', 1),\n    16\t        enableReadyCheck: false,\n    17\t        maxRetriesPerRequest: 3,\n    18\t      },\n    19\t      defaultJobOptions: {\n    20\t        removeOnComplete: 10,\n    21\t        removeOnFail: 5,\n    22\t        attempts: 3,\n    23\t        backoff: {\n    24\t          type: 'exponential',\n    25\t          delay: 2000,\n    26\t        },\n    27\t      },\n    28\t    };\n    29\t  }\n    30\t}\n...\nPath: src/modules/sync/sync.module.ts\n...\n    18\t\n    19\t@Module({\n    20\t  imports: [\n    21\t    ConfigModule,\n    22\t    BullModule.registerQueue({\n    23\t      name: SYNC_QUEUE,\n    24\t      defaultJobOptions: {\n    25\t        removeOnComplete: 50,\n    26\t        removeOnFail: 20,\n    27\t        attempts: 3,\n    28\t        backoff: {\n    29\t          type: 'exponential',\n    30\t          delay: 2000,\n    31\t        },\n    32\t        // Enhanced deduplication settings\n    33\t        jobId: undefined, // Will be set per job to ensure uniqueness\n    34\t        delay: 0,\n    35\t      },\n    36\t      settings: {\n    37\t        stalledInterval: 30 * 1000, // 30 seconds\n    38\t       ...&quot;,&quot;isError&quot;:false,&quot;requestId&quot;:&quot;&quot;}},&quot;6210448c-bc52-4d99-97aa-662266bbbc49;toolu_vrtx_01A77Sqtssm6PMSNA9CURrig&quot;:{&quot;requestId&quot;:&quot;6210448c-bc52-4d99-97aa-662266bbbc49&quot;,&quot;toolUseId&quot;:&quot;toolu_vrtx_01A77Sqtssm6PMSNA9CURrig&quot;,&quot;phase&quot;:5,&quot;result&quot;:{&quot;text&quot;:&quot;- [Queues | NestJS - A progressive Node.js framework](https://docs.nestjs.com/techniques/queues)\n  The forRoot() method is used to register a bullmq package configuration ... decorated with the @Processor() decorator). To listen for an event, use the ...\n\n- [Queuing jobs in NestJS using @nestjs/bullmq package - DEV ...](https://dev.to/railsstudent/queuing-jobs-in-nestjs-using-nestjsbullmq-package-55c1)\n  Dec 18, 2023 ... The codes of queuing jobs in NestJS is the same regardless nestjs/bull or nestjs/bullmq is used. Use InjectQueue decorator to inject a queue and add a job with ...\n\n- [typescript - Mock MikroORM class in jest to unit test methods ...](https://stackoverflow.com/questions/71358154/mock-mikroorm-class-in-jest-to-unit-test-methods-decorated-with-userequestconte)\n  Mar 4, 2022 ... This error is triggered if this.orm of that class where you use the decorator is not instance of MikroORM class. If you put a mock in there, ...\n\n- [How to work with CLS inside a bull queue · Papooch nestjs-cls ...](https://github.com/Papooch/nestjs-cls/discussions/178)\n  Likely becaue the UseCls decorator is generic and uses a bit of typescript magic to be type-safe according to the function on which you use it. I'll try to ...\n\n- [Cannot find module '@nestjs/core' or its corresponding type ...](https://stackoverflow.com/questions/68145372/cannot-find-module-nestjs-core-or-its-corresponding-type-declarations)\n  Jun 26, 2021 ... Sometimes you may also get error: Cannot find module '@nestjs/mapped-types' or its corresponding type declarations. In that case, do: npm ...&quot;,&quot;isError&quot;:false,&quot;requestId&quot;:&quot;&quot;}},&quot;7ee80746-6ce8-4d27-bcb2-692bb736c96a;toolu_vrtx_01W6xCPTRQb1YrFnnVPAzSag&quot;:{&quot;requestId&quot;:&quot;7ee80746-6ce8-4d27-bcb2-692bb736c96a&quot;,&quot;toolUseId&quot;:&quot;toolu_vrtx_01W6xCPTRQb1YrFnnVPAzSag&quot;,&quot;phase&quot;:5,&quot;result&quot;:{&quot;text&quot;:&quot; Documentation | NestJS - A progressive Node.js framework&quot;,&quot;isError&quot;:false,&quot;requestId&quot;:&quot;&quot;}},&quot;865a5fb9-a71d-422b-8e17-814ccaaf1475;toolu_vrtx_014qxHaEG1J6mz91z1ktnu3R&quot;:{&quot;requestId&quot;:&quot;865a5fb9-a71d-422b-8e17-814ccaaf1475&quot;,&quot;toolUseId&quot;:&quot;toolu_vrtx_014qxHaEG1J6mz91z1ktnu3R&quot;,&quot;phase&quot;:5,&quot;result&quot;:{&quot;text&quot;:&quot;The following code sections were retrieved:\nPath: docs/DEVELOPMENT.md\n...\n   249\t\n   250\t3. **Implement service logic**:\n   251\t   ```typescript\n   252\t   async create(createDto: CreateItemDto) {\n   253\t     return this.prisma.item.create({\n   254\t       data: createDto,\n   255\t     });\n   256\t   }\n   257\t   ```\n   258\t\n   259\t4. **Add tests**:\n   260\t   ```typescript\n   261\t   describe('create', () =&gt; {\n   262\t     it('should create item', async () =&gt; {\n   263\t       // Test implementation\n   264\t     });\n   265\t   });\n   266\t   ```\n   267\t\n   268\t### 4. Background Jobs\n   269\t\n   270\t1. **Define job processor**:\n   271\t   ```typescript\n   272\t   @Process('job-name')\n   273\t   async handleJob(job: Job&lt;JobData&gt;) {\n   274\t     // Job implementation\n   275\t   }\n   276\t   ```\n   277\t\n   278\t2. **Queue job**:\n   279\t   ```typescript\n   280\t   await this.queue.add('job-name', jobData);\n   281\t   ```\n   282\t\n   283\t3. **Test job processing**:\n   284\t   ```typescript\n   285\t   // Test job execution\n   286\t   ```\n...\nPath: src/modules/sync/processors/file-processing.processor.ts\n     1\timport { Processor, WorkerHost, OnQueueActive, OnQueueCompleted, OnQueueFailed } from '@nestjs/bullmq';\n     2\timport { Logger } from '@nestjs/common';\n     3\timport { Job } from 'bullmq';\n     4\timport { ConfigService } from '@nestjs/config';\n     5\timport { PrismaService } from '../../../shared/database/prisma.service';\n     6\timport { BatchOperationsService } from '../../../shared/database/batch-operations.service';\n     7\timport { StorageService } from '../../../shared/storage/storage.service';\n     8\timport { WorkerPoolService } from '../../../shared/workers/worker-pool.service';\n     9\timport { DistributedLockService } from '../../../shared/locking/distributed-lock.service';\n    10\timport { GitlabService } from '../../gitlab/gitlab.service';\n    11\timport {\n    12\t  QUEUE_NAMES,\n    13\t  JOB_TYPES,\n    14\t  FileProcessingJobData,\n...\n    36\t\n    37\t  constructor(\n    38\t    private configService: ConfigService,\n    39\t    private prisma: PrismaService,\n    40\t    private batchOperations: BatchOperationsService,\n    41\t    private storageService: StorageService,\n    42\t    private workerPoolService: WorkerPoolService,\n    43\t    private distributedLockService: DistributedLockService,\n    44\t    private gitlabService: GitlabService,\n    45\t  ) {\n    46\t    super();\n    47\t    this.config = this.configService.get&lt;ParallelProcessingConfig&gt;('parallelProcessing');\n    48\t  }\n    49\t\n    50\t  @OnQueueActive()\n    51\t  onActive(job: Job) {\n    52\t    this.logger.debug(`Processing file batch job ${job.id} for codebase ${job.data.codebaseId}`);\n    53\t  }\n    54\t\n    55\t  @OnQueueCompleted()\n    56\t  onCompleted(job: Job) {\n    57\t    this.logger.debug(`Completed file batch job ${job.id} for codebase ${job.data.codebaseId}`);\n    58\t  }\n...\n    67\t\n    68\t  /**\n    69\t   * Process file batch using worker pools (codebase-indexer approach)\n    70\t   */\n    71\t  @Processor(JOB_TYPES.PROCESS_FILE_BATCH)\n    72\t  async processFileBatch(job: Job&lt;FileProcessingJobData&gt;): Promise&lt;FileProcessingResult[]&gt; {\n    73\t    const { codebaseId, files, batchIndex, totalBatches, syncJobId, options } = job.data;\n    74\t    \n    75\t    this.logger.log(`Processing file batch ${batchIndex + 1}/${totalBatches} with ${files.length} files for codebase ${codebaseId}`);\n    76\t\n    77\t    // Acquire distributed lock for this codebase\n    78\t    const lockResource = `file-processing:${codebaseId}`;\n    79\t    const lockResult = await this.distributedLockService.acquireLock(lockResource, {\n    80\t      ttl: 300000, // 5 minutes\n    81\t      maxRetries: 3,\n    82\t    });\n...\nPath: src/modules/sync/processors/progress-tracking.processor.ts\n...\n    31\t\n    32\t/**\n    33\t * Progress Tracking Processor - Handles real-time progress updates and WebSocket notifications\n    34\t * Follows codebase-indexer's progress tracking pattern\n    35\t */\n    36\t@Processor(QUEUE_NAMES.PROGRESS_TRACKING_QUEUE)\n    37\texport class ProgressTrackingProcessor extends WorkerHost {\n    38\t  private readonly logger = new Logger(ProgressTrackingProcessor.name);\n    39\t  private readonly config: ParallelProcessingConfig;\n    40\t\n    41\t  constructor(\n    42\t    private configService: ConfigService,\n    43\t    private prisma: PrismaService,\n    44\t    private webSocketService: WebSocketService,\n    45\t  ) {\n    46\t    super();\n    47\t    this.config = this.configService.get&lt;ParallelProcessingConfig&gt;('parallelProcessing');\n    48\t  }\n    49\t\n    50\t  @OnQueueActive()\n    51\t  onActive(job: Job) {\n    52\t    this.logger.debug(`Processing progress tracking job ${job.id}`);\n    53\t  }\n...\nPath: src/modules/sync/processors/batch.processor.ts\n     1\timport { Processor, WorkerHost, OnQueueActive, OnQueueCompleted, OnQueueFailed } from '@nestjs/bullmq';\n     2\timport { Logger } from '@nestjs/common';\n     3\timport { Job } from 'bullmq';\n     4\timport { ConfigService } from '@nestjs/config';\n     5\timport { PrismaService } from '../../../shared/database/prisma.service';\n     6\timport { BatchOperationsService } from '../../../shared/database/batch-operations.service';\n     7\timport { StorageService } from '../../../shared/storage/storage.service';\n     8\timport { DistributedLockService } from '../../../shared/locking/distributed-lock.service';\n     9\timport { \n    10\t  QUEUE_NAMES, \n    11\t  JOB_TYPES, \n    12\t  BatchProcessingJobData,\n    13\t} from '../../../shared/queues/queue.constants';\n    14\timport { ParallelProcessingConfig } from '../../../config/parallel-processing.config';\n...\n    25\t\n    26\t/**\n    27\t * Batch Processor - Handles batch database and storage operations\n    28\t * Follows codebase-indexer's batch processing pattern for performance\n    29\t */\n    30\t@Processor(QUEUE_NAMES.BATCH_PROCESSING_QUEUE)\n    31\texport class BatchProcessor extends WorkerHost {\n    32\t  private readonly logger = new Logger(BatchProcessor.name);\n    33\t  private readonly config: ParallelProcessingConfig;\n    34\t\n    35\t  constructor(\n    36\t    private configService: ConfigService,\n    37\t    private prisma: PrismaService,\n    38\t    private batchOperations: BatchOperationsService,\n    39\t    private storageService: StorageService,\n    40\t    private distributedLockService: DistributedLockService,\n    41\t  ) {\n    42\t    super();\n    43\t    this.config = this.configService.get&lt;ParallelProcessingConfig&gt;('parallelProcessing');\n    44\t  }\n...\n    60\t\n    61\t  /**\n    62\t   * Process batch database operations\n    63\t   */\n    64\t  @Processor(JOB_TYPES.BATCH_DATABASE_OPERATIONS)\n    65\t  async processBatchDatabaseOperations(job: Job&lt;BatchProcessingJobData&gt;): Promise&lt;BatchOperationResult&gt; {\n    66\t    const { codebaseId, operationType, data, options } = job.data;\n    67\t    \n    68\t    this.logger.log(`Processing batch database operation: ${operationType} for codebase ${codebaseId}`);\n    69\t\n    70\t    const startTime = Date.now();\n...\n   117\t\n   118\t  /**\n   119\t   * Process batch storage operations\n   120\t   */\n   121\t  @Processor(JOB_TYPES.BATCH_STORAGE_OPERATIONS)\n   122\t  async processBatchStorageOperations(job: Job&lt;BatchProcessingJobData&gt;): Promise&lt;BatchOperationResult&gt; {\n   123\t    const { codebaseId, operationType, data, options } = job.data;\n   124\t    \n   125\t    this.logger.log(`Processing batch storage operation: ${operationType} for codebase ${codebaseId}`);\n   126\t\n   127\t    const startTime = Date.now();\n...\nPath: src/modules/sync/processors/code-analysis.processor.ts\n...\n    13\t  QUEUE_NAMES,\n    14\t  JOB_TYPES,\n    15\t  CodeAnalysisJobData,\n    16\t} from '../../../shared/queues/queue.constants';\n    17\timport { ParallelProcessingConfig } from '../../../config/parallel-processing.config';\n    18\t\n    19\texport interface CodeAnalysisResult {\n    20\t  filePath: string;\n    21\t  success: boolean;\n    22\t  symbols?: any[];\n    23\t  relationships?: any[];\n    24\t  scipData?: any;\n    25\t  treeSitterData?: any;\n    26\t  error?: string;\n    27\t  processingTime: number;\n    28\t}\n    29\t\n    30\t/**\n    31\t * Code Analysis Processor - Handles SCIP and Tree-sitter analysis\n    32\t * Follows codebase-indexer's parallel code analysis pattern\n    33\t */\n    34\t@Processor(QUEUE_NAMES.CODE_ANALYSIS_QUEUE)\n    35\texport class CodeAnalysisProcessor extends WorkerHost {\n    36\t  private readonly logger = new Logger(CodeAnalysisProcessor.name);\n    37\t  private readonly config: ParallelProcessingConfig;\n    38\t\n    39\t  constructor(\n    40\t    private configService: ConfigService,\n    41\t    private prisma: PrismaService,\n    42\t    private storageService: StorageService,\n    43\t    private workerPoolService: WorkerPoolService,\n    44\t    private distributedLockService: DistributedLockService,\n    45\t    private gitlabService: GitlabService,\n    46\t    private scipService: ScipService,\n    47\t    private treeSitterService: TreeSitterService,\n    48\t  ) {\n    49\t    super();\n    50\t    this.config = this.configService.get&lt;ParallelProcessingConfig&gt;('parallelProcessing');\n    51\t  }\n    52\t\n    53\t  @OnQueueActive()\n    54\t  onActive(job: Job) {\n    55\t    this.logger.debug(`Processing code analysis job ${job.id} for codebase ${job.data.codebaseId}`);\n    56\t  }\n    57\t\n    58\t  @OnQueueCompleted()\n    59\t  onCompleted(job: Job) {\n    60\t    this.logger.debug(`Completed code analysis job ${job.id} for codebase ${job.data.codebaseId}`);\n    61\t  }\n...\nPath: src/modules/codegraph/codegraph.processor.ts\n     1\timport { Processor, WorkerHost, OnQueueActive, OnQueueCompleted, OnQueueFailed } from '@nestjs/bullmq';\n     2\timport { Logger } from '@nestjs/common';\n     3\timport { Job } from 'bullmq';\n     4\timport { PrismaService } from '@/shared/database/prisma.service';\n     5\timport { StorageService } from '@/shared/storage/storage.service';\n     6\timport { WebSocketService } from '../websocket/websocket.service';\n     7\timport { DistributedLockService } from '../../shared/locking/distributed-lock.service';\n     8\timport { EnhancedQueueService } from '../../shared/queues/enhanced-queue.service';\n     9\timport { WorkerPoolService } from '../../shared/workers/worker-pool.service';\n    10\timport { ScipService } from './services/scip.service';\n    11\timport { TreeSitterService } from './services/tree-sitter.service';\n    12\timport { EmbeddingService } from './services/embedding.service';\n    13\timport { BadgerDbService } from './services/badger-db.service';\n    14\timport { CodegraphConfigService } from './services/codegraph-config.service';\n    15\timport { IndexTaskType, IndexTaskStatus } from '@prisma/client';\n    16\timport { CodegraphJobData } from './codegraph.service';\n    17\timport { QUEUE_NAMES, JOB_TYPES } from '../../shared/queues/queue.constants';\n    18\timport { ParallelProcessingConfig } from '../../config/parallel-processing.config';\n    19\timport { ConfigService } from '@nestjs/config';\n    20\t\n    21\t@Processor(QUEUE_NAMES.CODEGRAPH_QUEUE)\n    22\texport class CodegraphProcessor extends WorkerHost {\n    23\t  private readonly logger = new Logger(CodegraphProcessor.name);\n    24\t  private readonly config: ParallelProcessingConfig;\n...\n    58\t\n    59\t  /**\n    60\t   * Main entry point - Parallel processing following codebase-indexer pattern\n    61\t   */\n    62\t  @Processor(JOB_TYPES.INDEX_CODEBASE)\n    63\t  async handleIndexCodebase(job: Job&lt;CodegraphJobData&gt;): Promise&lt;void&gt; {\n    64\t    const { codebaseId, taskType, syncJobId, userId, options } = job.data;\n    65\t    const jobId = job.id as string;\n    66\t\n    67\t    this.logger.log(`Starting parallel processing for codebase: ${codebaseId}, type: ${taskType}`);\n    68\t\n    69\t    // Acquire distributed lock for this codebase\n    70\t    const lockResource = `codegraph-processing:${codebaseId}`;\n    71\t    const lockResult = await this.distributedLockService.acquireLock(lockResource, {\n    72\t      ttl: 1800000, // 30 minutes\n    73\t      maxRetries: 3,\n    74\t    });\n    75\t\n    76\t    if (!lockResult.acquired) {\n    77\t      throw new Error(`Failed to acquire lock for codebase ${codebaseId}: ${lockResult.error}`);\n    78\t    }\n...\n   135\t\n   136\t    // Pipeline 2: Code Analysis (SCIP + Tree-sitter)\n   137\t    promises.push(this.executeCodeAnalysisPipeline(codebaseId, files, totalBatches, syncJobId));\n   138\t\n   139\t    // Wait for all pipelines to complete\n   140\t    const results = await Promise.allSettled(promises);\n   141\t\n   142\t    // Check for failures\n   143\t    const failures = results.filter(result =&gt; result.status === 'rejected');\n   144\t    if (failures.length &gt; 0) {\n   145\t      this.logger.error(`${failures.length} pipelines failed:`, failures);\n   146\t      // Don't throw - allow partial success\n   147\t    }\n   148\t\n   149\t    // Update progress\n   150\t    await this.updateJobProgress(jobId, 100, 'All pipelines completed');\n   151\t  }\n...\n   163\t\n   164\t    try {\n   165\t      // Step 1: Submit file processing jobs\n   166\t      const batchSize = this.config?.performance?.maxFilesPerBatch || 100;\n   167\t      const fileBatches = this.createBatches(files, batchSize);\n   168\t\n   169\t      const fileProcessingPromises = fileBatches.map((batch, index) =&gt;\n   170\t        this.enhancedQueueService.submitFileProcessingJob({\n   171\t          codebaseId,\n   172\t          files: batch.map(f =&gt; ({\n   173\t            path: f.path,\n   174\t            hash: f.hash || '',\n   175\t            size: f.size || 0,\n   176\t            language: f.language,\n   177\t            storageKey: f.storageKey,\n   178\t          })),\n   179\t          batchIndex: index,\n   180\t          totalBatches,\n   181\t          syncJobId,\n   182\t          options: {\n   183\t            forceReprocess: false,\n   184\t            validateContent: true,\n   185\t            extractMetadata: true,\n   186\t          },\n   187\t        })\n   188\t      );\n...\n   234\t\n   235\t    try {\n   236\t      const batchSize = this.config?.performance?.maxFilesPerBatch || 100;\n   237\t      const fileBatches = this.createBatches(files, batchSize);\n   238\t\n   239\t      const codeAnalysisPromises = fileBatches.map((batch, index) =&gt;\n   240\t        this.enhancedQueueService.submitCodeAnalysisJob({\n   241\t          codebaseId,\n   242\t          files: batch.map(f =&gt; ({\n   243\t            path: f.path,\n   244\t            language: f.language || 'text',\n   245\t            storageKey: f.storageKey || f.path,\n   246\t          })),\n   247\t          analysisType: 'both', // SCIP + Tree-sitter\n   248\t          batchIndex: index,\n   249\t          totalBatches,\n   250\t          syncJobId,\n   251\t          options: {\n   252\t            extractSymbols: true,\n   253\t            buildRelationships: true,\n   254\t            generateIndex: true,\n   255\t          },\n   256\t        })\n   257\t      );\n...\nPath: src/modules/sync/sync.processor.ts\n...\n    13\timport { EnhancedQueueService } from '../../shared/queues/enhanced-queue.service';\n    14\timport { WorkerPoolService } from '../../shared/workers/worker-pool.service';\n    15\timport { SyncJobType, SyncJobStatus, CodebaseStatus, IndexTaskType } from '@prisma/client';\n    16\timport { SyncJobData } from './sync.service';\n    17\timport { QUEUE_NAMES, JOB_TYPES } from '../../shared/queues/queue.constants';\n    18\timport { ParallelProcessingConfig } from '../../config/parallel-processing.config';\n    19\t\n    20\t@Processor(QUEUE_NAMES.SYNC_QUEUE)\n    21\texport class SyncProcessor extends WorkerHost {\n    22\t  private readonly logger = new Logger(SyncProcessor.name);\n    23\t  private readonly config: ParallelProcessingConfig;\n    24\t\n    25\t  constructor(\n    26\t    private configService: ConfigService,\n    27\t    private prisma: PrismaService,\n    28\t    private gitlabService: GitlabService,\n    29\t    private fileScannerService: FileScannerService,\n    30\t    private fileService: FileService,\n    31\t    private storageService: StorageService,\n    32\t    private webSocketService: WebSocketService,\n    33\t    private codegraphService: CodegraphService,\n    34\t    private distributedLockService: DistributedLockService,\n    35\t    private enhancedQueueService: EnhancedQueueService,\n    36\t    private workerPoolService: WorkerPoolService,\n    37\t  ) {\n    38\t    super();\n    39\t    this.config = this.configService.get&lt;ParallelProcessingConfig&gt;('parallelProcessing');\n    40\t  }\n    41\t\n    42\t  @OnQueueActive()\n    43\t  onActive(job: Job) {\n    44\t    this.logger.debug(`Processing sync job ${job.id} for codebase ${job.data.codebaseId}`);\n    45\t  }\n...\n    56\t\n    57\t  /**\n    58\t   * Main orchestrator - Coordinates parallel pipelines (codebase-indexer approach)\n    59\t   */\n    60\t  @Processor(JOB_TYPES.SYNC_CODEBASE)\n    61\t  async handleSyncCodebase(job: Job&lt;SyncJobData&gt;): Promise&lt;void&gt; {\n    62\t    const { codebaseId, type, metadata } = job.data;\n    63\t    const syncJobId = metadata?.syncJobId;\n    64\t\n    65\t    this.logger.log(`Starting parallel sync orchestration for codebase: ${codebaseId}`);\n    66\t\n    67\t    // Acquire distributed lock for this codebase\n    68\t    const lockResource = `sync-orchestration:${codebaseId}`;\n    69\t    const lockResult = await this.distributedLockService.acquireLock(lockResource, {\n    70\t      ttl: 3600000, // 1 hour\n    71\t      maxRetries: 3,\n    72\t    });\n    73\t\n    74\t    if (!lockResult.acquired) {\n    75\t      throw new Error(`Failed to acquire sync lock for codebase ${codebaseId}: ${lockResult.error}`);\n    76\t    }\n...\n   126\t\n   127\t    try {\n   128\t      // Phase 1: Repository Sync &amp; File Discovery\n   129\t      await this.updateProgress(codebaseId, syncJobId, 'repository-sync', 10, 'Starting repository sync and file discovery');\n   130\t      await this.executeRepositorySync(codebaseId, syncJobId, metadata);\n   131\t\n   132\t      // Phase 2: Get discovered files\n   133\t      await this.updateProgress(codebaseId, syncJobId, 'file-discovery', 20, 'Discovering files in repository');\n   134\t      const files = await this.getDiscoveredFiles(codebaseId);\n   135\t\n   136\t      if (files.length === 0) {\n   137\t        this.logger.warn(`No files discovered for codebase ${codebaseId}`);\n   138\t        await this.updateProgress(codebaseId, syncJobId, 'completed', 100, 'No files found to process');\n   139\t        return;\n   140\t      }\n...\n   282\t\n   283\t    // Submit file processing jobs\n   284\t    const fileProcessingPromises = fileBatches.map((batch, index) =&gt;\n   285\t      this.enhancedQueueService.submitFileProcessingJob({\n   286\t        codebaseId,\n   287\t        files: batch.map(f =&gt; ({\n   288\t          path: f.path,\n   289\t          hash: f.hash || '',\n   290\t          size: f.size || 0,\n   291\t          language: f.language,\n   292\t          storageKey: f.storageKey,\n   293\t        })),\n   294\t        batchIndex: index,\n   295\t        totalBatches,\n   296\t        syncJobId,\n   297\t        options: {\n   298\t          forceReprocess: false,\n   299\t          validateContent: true,\n   300\t          extractMetadata: true,\n   301\t        },\n   302\t      })\n   303\t    );\n...\n   348\t\n   349\t    const codeAnalysisPromises = fileBatches.map((batch, index) =&gt;\n   350\t      this.enhancedQueueService.submitCodeAnalysisJob({\n   351\t        codebaseId,\n   352\t        files: batch.map(f =&gt; ({\n   353\t          path: f.path,\n   354\t          language: f.language || 'text',\n   355\t          storageKey: f.storageKey || f.path,\n   356\t        })),\n   357\t        analysisType: 'both', // SCIP + Tree-sitter\n   358\t        batchIndex: index,\n   359\t        totalBatches,\n   360\t        syncJobId,\n   361\t        options: {\n   362\t          extractSymbols: true,\n   363\t          buildRelationships: true,\n   364\t          generateIndex: true,\n   365\t        },\n   366\t      })\n   367\t    );\n   368\t\n   369\t    // Wait for code analysis to complete\n   370\t    await Promise.all(codeAnalysisPromises);\n   371\t\n   372\t    this.logger.log(`Code analysis pipeline completed for ${files.length} files`);\n   373\t  }\n...\n&quot;,&quot;isError&quot;:false,&quot;requestId&quot;:&quot;&quot;}},&quot;47e64d1b-777f-4d80-a951-4e4b88e702a1;toolu_vrtx_01KaNnahfvLdZem7o5azhszr&quot;:{&quot;requestId&quot;:&quot;47e64d1b-777f-4d80-a951-4e4b88e702a1&quot;,&quot;toolUseId&quot;:&quot;toolu_vrtx_01KaNnahfvLdZem7o5azhszr&quot;,&quot;phase&quot;:5,&quot;result&quot;:{&quot;text&quot;:&quot;The following code sections were retrieved:\nPath: docs/DEVELOPMENT.md\n...\n   249\t\n   250\t3. **Implement service logic**:\n   251\t   ```typescript\n   252\t   async create(createDto: CreateItemDto) {\n   253\t     return this.prisma.item.create({\n   254\t       data: createDto,\n   255\t     });\n   256\t   }\n   257\t   ```\n   258\t\n   259\t4. **Add tests**:\n   260\t   ```typescript\n   261\t   describe('create', () =&gt; {\n   262\t     it('should create item', async () =&gt; {\n   263\t       // Test implementation\n   264\t     });\n   265\t   });\n   266\t   ```\n   267\t\n   268\t### 4. Background Jobs\n   269\t\n   270\t1. **Define job processor**:\n   271\t   ```typescript\n   272\t   @Process('job-name')\n   273\t   async handleJob(job: Job&lt;JobData&gt;) {\n   274\t     // Job implementation\n   275\t   }\n   276\t   ```\n   277\t\n   278\t2. **Queue job**:\n   279\t   ```typescript\n   280\t   await this.queue.add('job-name', jobData);\n   281\t   ```\n   282\t\n   283\t3. **Test job processing**:\n   284\t   ```typescript\n   285\t   // Test job execution\n   286\t   ```\n...\nPath: src/shared/queues/enhanced-queue.module.ts\n...\n    11\t\n    12\t@Global()\n    13\t@Module({\n    14\t  imports: [\n    15\t    ConfigModule,\n    16\t    \n    17\t    // File Processing Queue\n    18\t    BullModule.registerQueueAsync({\n    19\t      name: QUEUE_NAMES.FILE_PROCESSING_QUEUE,\n    20\t      inject: [ConfigService],\n    21\t      useFactory: (configService: ConfigService) =&gt; {\n    22\t        const config = configService.get&lt;ParallelProcessingConfig&gt;('parallelProcessing');\n    23\t        const queueConfig = config?.queues?.fileProcessing;\n    24\t        \n    25\t        return {\n    26\t          ...DEFAULT_QUEUE_OPTIONS,\n    27\t          ...QUEUE_SPECIFIC_OPTIONS[QUEUE_NAMES.FILE_PROCESSING_QUEUE],\n    28\t          defaultJobOptions: {\n    29\t            ...DEFAULT_QUEUE_OPTIONS.defaultJobOptions,\n    30\t            ...QUEUE_SPECIFIC_OPTIONS[QUEUE_NAMES.FILE_PROCESSING_QUEUE].defaultJobOptions,\n    31\t            removeOnComplete: queueConfig?.removeOnComplete || 100,\n    32\t            removeOnFail: queueConfig?.removeOnFail || 50,\n    33\t          },\n    34\t        };\n    35\t      },\n    36\t    }),\n    37\t    \n    38\t    // Embedding Queue\n    39\t    BullModule.registerQueueAsync({\n    40\t      name: QUEUE_NAMES.EMBEDDING_QUEUE,\n    41\t      inject: [ConfigService],\n    42\t      useFactory: (configService: ConfigService) =&gt; {\n    43\t        const config = configService.get&lt;ParallelProcessingConfig&gt;('parallelProcessing');\n    44\t        const queueConfig = config?.queues?.embedding;\n    45\t        \n    46\t        return {\n    47\t          ...DEFAULT_QUEUE_OPTIONS,\n    48\t          ...QUEUE_SPECIFIC_OPTIONS[QUEUE_NAMES.EMBEDDING_QUEUE],\n    49\t          defaultJobOptions: {\n    50\t            ...DEFAULT_QUEUE_OPTIONS.defaultJobOptions,\n    51\t            ...QUEUE_SPECIFIC_OPTIONS[QUEUE_NAMES.EMBEDDING_QUEUE].defaultJobOptions,\n    52\t            removeOnComplete: queueConfig?.removeOnComplete || 50,\n    53\t            removeOnFail: queueConfig?.removeOnFail || 25,\n    54\t          },\n    55\t        };\n    56\t      },\n    57\t    }),\n    58\t    \n    59\t    // Code Analysis Queue\n    60\t    BullModule.registerQueueAsync({\n    61\t      name: QUEUE_NAMES.CODE_ANALYSIS_QUEUE,\n    62\t      inject: [ConfigService],\n    63\t      useFactory: (configService: ConfigService) =&gt; {\n    64\t        const config = configService.get&lt;ParallelProcessingConfig&gt;('parallelProcessing');\n    65\t        const queueConfig = config?.queues?.codeAnalysis;\n    66\t        \n    67\t        return {\n    68\t          ...DEFAULT_QUEUE_OPTIONS,\n    69\t          ...QUEUE_SPECIFIC_OPTIONS[QUEUE_NAMES.CODE_ANALYSIS_QUEUE],\n    70\t          defaultJobOptions: {\n    71\t            ...DEFAULT_QUEUE_OPTIONS.defaultJobOptions,\n    72\t            ...QUEUE_SPECIFIC_OPTIONS[QUEUE_NAMES.CODE_ANALYSIS_QUEUE].defaultJobOptions,\n    73\t            removeOnComplete: queueConfig?.removeOnComplete || 25,\n    74\t            removeOnFail: queueConfig?.removeOnFail || 10,\n    75\t          },\n    76\t        };\n    77\t      },\n    78\t    }),\n    79\t    \n    80\t    // Batch Processing Queue\n    81\t    BullModule.registerQueueAsync({\n    82\t      name: QUEUE_NAMES.BATCH_PROCESSING_QUEUE,\n    83\t      inject: [ConfigService],\n    84\t      useFactory: (configService: ConfigService) =&gt; {\n    85\t        const config = configService.get&lt;ParallelProcessingConfig&gt;('parallelProcessing');\n    86\t        const queueConfig = config?.queues?.batch;\n    87\t        \n    88\t        return {\n    89\t          ...DEFAULT_QUEUE_OPTIONS,\n    90\t          ...QUEUE_SPECIFIC_OPTIONS[QUEUE_NAMES.BATCH_PROCESSING_QUEUE],\n    91\t          defaultJobOptions: {\n    92\t            ...DEFAULT_QUEUE_OPTIONS.defaultJobOptions,\n    93\t            ...QUEUE_SPECIFIC_OPTIONS[QUEUE_NAMES.BATCH_PROCESSING_QUEUE].defaultJobOptions,\n    94\t            removeOnComplete: queueConfig?.removeOnComplete || 10,\n    95\t            removeOnFail: queueConfig?.removeOnFail || 5,\n    96\t          },\n    97\t        };\n    98\t      },\n    99\t    }),\n...\nPath: src/modules/sync/processors/batch.processor.ts\n     1\timport { Processor, WorkerHost, OnQueueActive, OnQueueCompleted, OnQueueFailed } from '@nestjs/bullmq';\n     2\timport { Logger } from '@nestjs/common';\n     3\timport { Job } from 'bullmq';\n     4\timport { ConfigService } from '@nestjs/config';\n     5\timport { PrismaService } from '../../../shared/database/prisma.service';\n     6\timport { BatchOperationsService } from '../../../shared/database/batch-operations.service';\n     7\timport { StorageService } from '../../../shared/storage/storage.service';\n     8\timport { DistributedLockService } from '../../../shared/locking/distributed-lock.service';\n     9\timport { \n    10\t  QUEUE_NAMES, \n    11\t  JOB_TYPES, \n    12\t  BatchProcessingJobData,\n    13\t} from '../../../shared/queues/queue.constants';\n    14\timport { ParallelProcessingConfig } from '../../../config/parallel-processing.config';\n...\n    25\t\n    26\t/**\n    27\t * Batch Processor - Handles batch database and storage operations\n    28\t * Follows codebase-indexer's batch processing pattern for performance\n    29\t */\n    30\t@Processor(QUEUE_NAMES.BATCH_PROCESSING_QUEUE)\n    31\texport class BatchProcessor extends WorkerHost {\n    32\t  private readonly logger = new Logger(BatchProcessor.name);\n    33\t  private readonly config: ParallelProcessingConfig;\n    34\t\n    35\t  constructor(\n    36\t    private configService: ConfigService,\n    37\t    private prisma: PrismaService,\n    38\t    private batchOperations: BatchOperationsService,\n    39\t    private storageService: StorageService,\n    40\t    private distributedLockService: DistributedLockService,\n    41\t  ) {\n    42\t    super();\n    43\t    this.config = this.configService.get&lt;ParallelProcessingConfig&gt;('parallelProcessing');\n    44\t  }\n...\nPath: src/modules/sync/processors/file-processing.processor.ts\n     1\timport { Processor, WorkerHost, OnQueueActive, OnQueueCompleted, OnQueueFailed } from '@nestjs/bullmq';\n     2\timport { Logger } from '@nestjs/common';\n     3\timport { Job } from 'bullmq';\n     4\timport { ConfigService } from '@nestjs/config';\n     5\timport { PrismaService } from '../../../shared/database/prisma.service';\n     6\timport { BatchOperationsService } from '../../../shared/database/batch-operations.service';\n     7\timport { StorageService } from '../../../shared/storage/storage.service';\n     8\timport { WorkerPoolService } from '../../../shared/workers/worker-pool.service';\n     9\timport { DistributedLockService } from '../../../shared/locking/distributed-lock.service';\n    10\timport { GitlabService } from '../../gitlab/gitlab.service';\n    11\timport {\n    12\t  QUEUE_NAMES,\n    13\t  JOB_TYPES,\n    14\t  FileProcessingJobData,\n    15\t} from '../../../shared/queues/queue.constants';\n    16\timport { ParallelProcessingConfig } from '../../../config/parallel-processing.config';\n    17\t\n    18\texport interface FileProcessingResult {\n    19\t  filePath: string;\n    20\t  success: boolean;\n    21\t  size: number;\n    22\t  storageKey?: string;\n    23\t  language?: string;\n    24\t  error?: string;\n    25\t  processingTime: number;\n    26\t}\n    27\t\n    28\t/**\n    29\t * File Processing Processor - Handles parallel file processing using worker pools\n    30\t * Follows codebase-indexer's parallel file processing pattern\n    31\t */\n    32\t@Processor(QUEUE_NAMES.FILE_PROCESSING_QUEUE)\n    33\texport class FileProcessingProcessor extends WorkerHost {\n    34\t  private readonly logger = new Logger(FileProcessingProcessor.name);\n    35\t  private readonly config: ParallelProcessingConfig;\n...\n    67\t\n    68\t  /**\n    69\t   * Process file batch using worker pools (codebase-indexer approach)\n    70\t   */\n    71\t  @Processor(JOB_TYPES.PROCESS_FILE_BATCH)\n    72\t  async processFileBatch(job: Job&lt;FileProcessingJobData&gt;): Promise&lt;FileProcessingResult[]&gt; {\n    73\t    const { codebaseId, files, batchIndex, totalBatches, syncJobId, options } = job.data;\n    74\t    \n    75\t    this.logger.log(`Processing file batch ${batchIndex + 1}/${totalBatches} with ${files.length} files for codebase ${codebaseId}`);\n    76\t\n    77\t    // Acquire distributed lock for this codebase\n    78\t    const lockResource = `file-processing:${codebaseId}`;\n    79\t    const lockResult = await this.distributedLockService.acquireLock(lockResource, {\n    80\t      ttl: 300000, // 5 minutes\n    81\t      maxRetries: 3,\n    82\t    });\n...\nPath: src/modules/codegraph/codegraph.processor.ts\n     1\timport { Processor, WorkerHost, OnQueueActive, OnQueueCompleted, OnQueueFailed } from '@nestjs/bullmq';\n     2\timport { Logger } from '@nestjs/common';\n     3\timport { Job } from 'bullmq';\n     4\timport { PrismaService } from '@/shared/database/prisma.service';\n     5\timport { StorageService } from '@/shared/storage/storage.service';\n     6\timport { WebSocketService } from '../websocket/websocket.service';\n     7\timport { DistributedLockService } from '../../shared/locking/distributed-lock.service';\n     8\timport { EnhancedQueueService } from '../../shared/queues/enhanced-queue.service';\n     9\timport { WorkerPoolService } from '../../shared/workers/worker-pool.service';\n    10\timport { ScipService } from './services/scip.service';\n    11\timport { TreeSitterService } from './services/tree-sitter.service';\n    12\timport { EmbeddingService } from './services/embedding.service';\n    13\timport { BadgerDbService } from './services/badger-db.service';\n    14\timport { CodegraphConfigService } from './services/codegraph-config.service';\n    15\timport { IndexTaskType, IndexTaskStatus } from '@prisma/client';\n    16\timport { CodegraphJobData } from './codegraph.service';\n    17\timport { QUEUE_NAMES, JOB_TYPES } from '../../shared/queues/queue.constants';\n    18\timport { ParallelProcessingConfig } from '../../config/parallel-processing.config';\n    19\timport { ConfigService } from '@nestjs/config';\n    20\t\n    21\t@Processor(QUEUE_NAMES.CODEGRAPH_QUEUE)\n    22\texport class CodegraphProcessor extends WorkerHost {\n    23\t  private readonly logger = new Logger(CodegraphProcessor.name);\n    24\t  private readonly config: ParallelProcessingConfig;\n...\n    58\t\n    59\t  /**\n    60\t   * Main entry point - Parallel processing following codebase-indexer pattern\n    61\t   */\n    62\t  @Processor(JOB_TYPES.INDEX_CODEBASE)\n    63\t  async handleIndexCodebase(job: Job&lt;CodegraphJobData&gt;): Promise&lt;void&gt; {\n    64\t    const { codebaseId, taskType, syncJobId, userId, options } = job.data;\n    65\t    const jobId = job.id as string;\n    66\t\n    67\t    this.logger.log(`Starting parallel processing for codebase: ${codebaseId}, type: ${taskType}`);\n    68\t\n    69\t    // Acquire distributed lock for this codebase\n    70\t    const lockResource = `codegraph-processing:${codebaseId}`;\n    71\t    const lockResult = await this.distributedLockService.acquireLock(lockResource, {\n    72\t      ttl: 1800000, // 30 minutes\n    73\t      maxRetries: 3,\n    74\t    });\n    75\t\n    76\t    if (!lockResult.acquired) {\n    77\t      throw new Error(`Failed to acquire lock for codebase ${codebaseId}: ${lockResult.error}`);\n    78\t    }\n...\nPath: src/modules/codegraph/codegraph.module.ts\n...\n    24\t\n    25\t@Module({\n    26\t  imports: [\n    27\t    DatabaseModule,\n    28\t    StorageModule,\n    29\t    WorkerPoolModule,\n    30\t    DistributedLockModule,\n    31\t    EnhancedQueueModule,\n    32\t    BullModule.registerQueue({\n    33\t      name: QUEUE_NAMES.CODEGRAPH_QUEUE,\n    34\t      defaultJobOptions: {\n    35\t        removeOnComplete: 50,\n    36\t        removeOnFail: 20,\n    37\t        attempts: 3,\n    38\t        backoff: {\n    39\t          type: 'exponential',\n    40\t          delay: 2000,\n    41\t        },\n    42\t      },\n    43\t      settings: {\n    44\t        stalledInterval: 30 * 1000, // 30 seconds\n    45\t        maxStalledCount: 1,\n    46\t      },\n    47\t    }),\n    48\t  ],\n    49\t  controllers: [CodegraphController],\n    50\t  providers: [\n    51\t    CodegraphService,\n    52\t    CodegraphProcessor,\n    53\t    ScipService,\n    54\t    TreeSitterService,\n    55\t    BadgerDbService,\n    56\t    EmbeddingService,\n    57\t    CodegraphConfigService,\n    58\t    ScipToolsService,\n    59\t    CodegraphMetricsService,\n    60\t    SearchService,\n    61\t    FileManagementService,\n    62\t    // New parallel processing services\n    63\t    EnhancedTextSplitterService,\n    64\t    SemanticChunkerService,\n    65\t    TokenChunkerService,\n    66\t  ],\n...\nPath: src/modules/sync/processors/cleanup.processor.ts\n     1\timport { Processor, WorkerHost, OnQueueActive, OnQueueCompleted, OnQueueFailed } from '@nestjs/bullmq';\n     2\timport { Logger } from '@nestjs/common';\n     3\timport { Job } from 'bullmq';\n     4\timport { ConfigService } from '@nestjs/config';\n     5\timport { PrismaService } from '../../../shared/database/prisma.service';\n     6\timport { StorageService } from '../../../shared/storage/storage.service';\n     7\timport { DistributedLockService } from '../../../shared/locking/distributed-lock.service';\n     8\timport { \n     9\t  QUEUE_NAMES, \n    10\t  JOB_TYPES, \n    11\t  CleanupJobData,\n    12\t} from '../../../shared/queues/queue.constants';\n    13\timport { ParallelProcessingConfig } from '../../../config/parallel-processing.config';\n...\n    23\t\n    24\t/**\n    25\t * Cleanup Processor - Handles cleanup of temporary files, expired locks, and old jobs\n    26\t * Follows codebase-indexer's cleanup pattern for maintenance\n    27\t */\n    28\t@Processor(QUEUE_NAMES.CLEANUP_QUEUE)\n    29\texport class CleanupProcessor extends WorkerHost {\n    30\t  private readonly logger = new Logger(CleanupProcessor.name);\n    31\t  private readonly config: ParallelProcessingConfig;\n    32\t\n    33\t  constructor(\n    34\t    private configService: ConfigService,\n    35\t    private prisma: PrismaService,\n    36\t    private storageService: StorageService,\n    37\t    private distributedLockService: DistributedLockService,\n    38\t  ) {\n    39\t    super();\n    40\t    this.config = this.configService.get&lt;ParallelProcessingConfig&gt;('parallelProcessing');\n    41\t  }\n    42\t\n    43\t  @OnQueueActive()\n    44\t  onActive(job: Job) {\n    45\t    this.logger.debug(`Processing cleanup job ${job.id}: ${job.data.operationType}`);\n    46\t  }\n...\nPath: src/modules/sync/sync.processor.ts\n     1\timport { Processor, WorkerHost, OnQueueActive, OnQueueCompleted, OnQueueFailed } from '@nestjs/bullmq';\n     2\timport { Logger } from '@nestjs/common';\n     3\timport { Job } from 'bullmq';\n     4\timport { ConfigService } from '@nestjs/config';\n     5\timport { PrismaService } from '@/shared/database/prisma.service';\n     6\timport { GitlabService } from '../gitlab/gitlab.service';\n     7\timport { FileScannerService } from '../file/file-scanner.service';\n     8\timport { FileService } from '../file/file.service';\n     9\timport { StorageService } from '@/shared/storage/storage.service';\n    10\timport { WebSocketService } from '../websocket/websocket.service';\n    11\timport { CodegraphService } from '../codegraph/codegraph.service';\n    12\timport { DistributedLockService } from '../../shared/locking/distributed-lock.service';\n...\nPath: src/modules/sync/processors/code-analysis.processor.ts\n     1\timport { Processor, WorkerHost, OnQueueActive, OnQueueCompleted, OnQueueFailed } from '@nestjs/bullmq';\n     2\timport { Logger } from '@nestjs/common';\n     3\timport { Job } from 'bullmq';\n     4\timport { ConfigService } from '@nestjs/config';\n     5\timport { PrismaService } from '../../../shared/database/prisma.service';\n     6\timport { StorageService } from '../../../shared/storage/storage.service';\n     7\timport { WorkerPoolService } from '../../../shared/workers/worker-pool.service';\n     8\timport { DistributedLockService } from '../../../shared/locking/distributed-lock.service';\n     9\timport { GitlabService } from '../../gitlab/gitlab.service';\n    10\timport { ScipService } from '../../codegraph/services/scip.service';\n    11\timport { TreeSitterService } from '../../codegraph/services/tree-sitter.service';\n    12\timport {\n    13\t  QUEUE_NAMES,\n    14\t  JOB_TYPES,\n    15\t  CodeAnalysisJobData,\n    16\t} from '../../../shared/queues/queue.constants';\n    17\timport { ParallelProcessingConfig } from '../../../config/parallel-processing.config';\n    18\t\n    19\texport interface CodeAnalysisResult {\n    20\t  filePath: string;\n    21\t  success: boolean;\n    22\t  symbols?: any[];\n    23\t  relationships?: any[];\n    24\t  scipData?: any;\n    25\t  treeSitterData?: any;\n    26\t  error?: string;\n    27\t  processingTime: number;\n    28\t}\n    29\t\n    30\t/**\n    31\t * Code Analysis Processor - Handles SCIP and Tree-sitter analysis\n    32\t * Follows codebase-indexer's parallel code analysis pattern\n    33\t */\n    34\t@Processor(QUEUE_NAMES.CODE_ANALYSIS_QUEUE)\n    35\texport class CodeAnalysisProcessor extends WorkerHost {\n    36\t  private readonly logger = new Logger(CodeAnalysisProcessor.name);\n    37\t  private readonly config: ParallelProcessingConfig;\n...\nPath: src/modules/sync/processors/progress-tracking.processor.ts\n...\n    31\t\n    32\t/**\n    33\t * Progress Tracking Processor - Handles real-time progress updates and WebSocket notifications\n    34\t * Follows codebase-indexer's progress tracking pattern\n    35\t */\n    36\t@Processor(QUEUE_NAMES.PROGRESS_TRACKING_QUEUE)\n    37\texport class ProgressTrackingProcessor extends WorkerHost {\n    38\t  private readonly logger = new Logger(ProgressTrackingProcessor.name);\n    39\t  private readonly config: ParallelProcessingConfig;\n    40\t\n    41\t  constructor(\n    42\t    private configService: ConfigService,\n    43\t    private prisma: PrismaService,\n    44\t    private webSocketService: WebSocketService,\n    45\t  ) {\n    46\t    super();\n    47\t    this.config = this.configService.get&lt;ParallelProcessingConfig&gt;('parallelProcessing');\n    48\t  }\n    49\t\n    50\t  @OnQueueActive()\n    51\t  onActive(job: Job) {\n    52\t    this.logger.debug(`Processing progress tracking job ${job.id}`);\n    53\t  }\n...\nPath: src/modules/sync/processors/embedding.processor.ts\n     1\timport { Processor, WorkerHost, OnQueueActive, OnQueueCompleted, OnQueueFailed } from '@nestjs/bullmq';\n     2\timport { Logger } from '@nestjs/common';\n     3\timport { Job } from 'bullmq';\n     4\timport { ConfigService } from '@nestjs/config';\n     5\timport { PrismaService } from '../../../shared/database/prisma.service';\n     6\timport { StorageService } from '../../../shared/storage/storage.service';\n     7\timport { EnhancedTextSplitterService } from '../../codegraph/services/enhanced-text-splitter.service';\n     8\timport { EmbeddingService } from '../../codegraph/services/embedding.service';\n     9\timport { \n    10\t  QUEUE_NAMES, \n    11\t  JOB_TYPES, \n    12\t  EmbeddingJobData,\n    13\t} from '../../../shared/queues/queue.constants';\n    14\timport { ParallelProcessingConfig } from '../../../config/parallel-processing.config';\n...\n&quot;,&quot;isError&quot;:false,&quot;requestId&quot;:&quot;&quot;}},&quot;9d12d1b4-715e-44d6-bdf8-258d3f577a0f;toolu_vrtx_01TjXWPK8vHGzKU9dDgQYMnf&quot;:{&quot;requestId&quot;:&quot;9d12d1b4-715e-44d6-bdf8-258d3f577a0f&quot;,&quot;toolUseId&quot;:&quot;toolu_vrtx_01TjXWPK8vHGzKU9dDgQYMnf&quot;,&quot;phase&quot;:5,&quot;result&quot;:{&quot;text&quot;:&quot;Here's the result of running `cat -n` on src/modules/sync/sync.processor.ts:\n     1\timport {Processor, WorkerHost} from '@nestjs/bullmq';\n     2\timport { Logger } from '@nestjs/common';\n     3\timport { Job } from 'bullmq';\n     4\timport { ConfigService } from '@nestjs/config';\n     5\timport { PrismaService } from '@/shared/database/prisma.service';\nTotal lines in file: 549\n&quot;,&quot;isError&quot;:false,&quot;requestId&quot;:&quot;&quot;}},&quot;14a3c2d7-2181-42eb-9612-e74826159077;toolu_vrtx_011FeDH2Vv1T6oiXUrvtL1hg&quot;:{&quot;requestId&quot;:&quot;14a3c2d7-2181-42eb-9612-e74826159077&quot;,&quot;toolUseId&quot;:&quot;toolu_vrtx_011FeDH2Vv1T6oiXUrvtL1hg&quot;,&quot;phase&quot;:5,&quot;result&quot;:{&quot;text&quot;:&quot;Successfully edited the file src/modules/sync/sync.processor.ts.\nResult for str_replace for entry with index [1]:\nReplacement successful.\nnew_str starts at line 1 and ends at line 1.\n\nReview the changes and make sure they are as expected (correct indentation, no duplicate lines, etc).\nEdit the file again if necessary.\n&quot;,&quot;isError&quot;:false,&quot;requestId&quot;:&quot;&quot;}},&quot;49e83517-cad9-456d-a044-1f300cb0833d;toolu_vrtx_013TUwwHEiSnpthFoPmZKDQF&quot;:{&quot;requestId&quot;:&quot;49e83517-cad9-456d-a044-1f300cb0833d&quot;,&quot;toolUseId&quot;:&quot;toolu_vrtx_013TUwwHEiSnpthFoPmZKDQF&quot;,&quot;phase&quot;:5,&quot;result&quot;:{&quot;text&quot;:&quot;Successfully edited the file src/modules/sync/sync.processor.ts.\nResult for str_replace for entry with index [1]:\nReplacement successful.\nnew_str starts at line 57 and ends at line 61.\n\nReview the changes and make sure they are as expected (correct indentation, no duplicate lines, etc).\nEdit the file again if necessary.\n&quot;,&quot;isError&quot;:false,&quot;requestId&quot;:&quot;&quot;}},&quot;44abde20-0715-4fd9-ae78-f6f7b9ec08de;toolu_vrtx_01VbtRfw6RjomDyE9iRoj8Qf&quot;:{&quot;requestId&quot;:&quot;44abde20-0715-4fd9-ae78-f6f7b9ec08de&quot;,&quot;toolUseId&quot;:&quot;toolu_vrtx_01VbtRfw6RjomDyE9iRoj8Qf&quot;,&quot;phase&quot;:5,&quot;result&quot;:{&quot;text&quot;:&quot;Here's the result of running `cat -n` on src/modules/sync/processors/file-processing.processor.ts:\n     1\timport { Processor, WorkerHost, OnQueueActive, OnQueueCompleted, OnQueueFailed } from '@nestjs/bullmq';\n     2\timport { Logger } from '@nestjs/common';\n     3\timport { Job } from 'bullmq';\n     4\timport { ConfigService } from '@nestjs/config';\n     5\timport { PrismaService } from '../../../shared/database/prisma.service';\nTotal lines in file: 489\n&quot;,&quot;isError&quot;:false,&quot;requestId&quot;:&quot;&quot;}},&quot;57be58ea-d4d1-4c94-877c-3e0e66dc6e00;toolu_vrtx_019mftPzLeKzCbp4BTUSrCAK&quot;:{&quot;requestId&quot;:&quot;57be58ea-d4d1-4c94-877c-3e0e66dc6e00&quot;,&quot;toolUseId&quot;:&quot;toolu_vrtx_019mftPzLeKzCbp4BTUSrCAK&quot;,&quot;phase&quot;:5,&quot;result&quot;:{&quot;text&quot;:&quot;Here's the result of running `cat -n` on src/modules/sync/processors/file-processing.processor.ts:\n    68\t  /**\n    69\t   * Process file batch using worker pools (codebase-indexer approach)\n    70\t   */\n    71\t  @Processor(JOB_TYPES.PROCESS_FILE_BATCH)\n    72\t  async processFileBatch(job: Job&lt;FileProcessingJobData&gt;): Promise&lt;FileProcessingResult[]&gt; {\n    73\t    const { codebaseId, files, batchIndex, totalBatches, syncJobId, options } = job.data;\n    74\t    \n    75\t    this.logger.log(`Processing file batch ${batchIndex + 1}/${totalBatches} with ${files.length} files for codebase ${codebaseId}`);\nTotal lines in file: 489\n&quot;,&quot;isError&quot;:false,&quot;requestId&quot;:&quot;&quot;}}},&quot;draftExchange&quot;:{&quot;request_message&quot;:&quot;&quot;,&quot;rich_text_json_repr&quot;:{&quot;type&quot;:&quot;doc&quot;,&quot;content&quot;:[{&quot;type&quot;:&quot;paragraph&quot;}]},&quot;status&quot;:&quot;draft&quot;},&quot;requestIds&quot;:[],&quot;isPinned&quot;:false,&quot;isShareable&quot;:true,&quot;extraData&quot;:{&quot;isAgentConversation&quot;:true,&quot;hasDirtyEdits&quot;:true},&quot;personaType&quot;:0,&quot;rootTaskUuid&quot;:&quot;52b052ce-c88a-41f9-806f-7bf3cd992f29&quot;}},&quot;agentExecutionMode&quot;:&quot;manual&quot;,&quot;isPanelCollapsed&quot;:true,&quot;displayedAnnouncements&quot;:[]}" />
      </map>
    </option>
  </component>
</project>